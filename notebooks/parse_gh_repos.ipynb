{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQ8L8iFnNoPl"
      },
      "outputs": [],
      "source": [
        "!pip install PyGithub\n",
        "!pip install nbformat"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get all repos\n",
        "\n",
        "Replace `GH_ACCESS_TOKEN` with your GitHub access token\n",
        "- You may want to go ahead without `GH_ACCESS_TOKEN` set. In this case, 60 requests / hour is the rate limit. \n",
        "- With `GH_ACCESS_TOKEN` set, the rate limit is 5000 requests / hour.\n",
        "- [Reference](https://docs.github.com/en/rest/overview/resources-in-the-rest-api?apiVersion=2022-11-28#rate-limits-for-requests-from-personal-accounts)"
      ],
      "metadata": {
        "id": "MkMz1EDPcyyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from github import Github\n",
        "\n",
        "def get_repos(username, access_token=None, include_fork=False):\n",
        "  g = Github(access_token)\n",
        "  user = g.get_user(username)\n",
        "\n",
        "  results = []\n",
        "  for repo in user.get_repos():\n",
        "      if repo.fork is False:\n",
        "        results.append(repo)\n",
        "      else:\n",
        "        if include_fork is True:\n",
        "          results.append(repo)\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "id": "6Uc0iy-JP2RG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "repos = get_repos(\"deep-diver\", \"GH_ACCESS_TOKEN\")"
      ],
      "metadata": {
        "id": "FAcZKZMAYwAJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "print(len(repos))\n",
        "pprint(repos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtcJ0Oq2G1dH",
        "outputId": "53ad565f-4357-4d74-8445-d9414c12e85a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "108\n",
            "[Repository(full_name=\"deep-diver/-\"),\n",
            " Repository(full_name=\"deep-diver/AlexNet\"),\n",
            " Repository(full_name=\"deep-diver/Baseball_Data_Analysis\"),\n",
            " Repository(full_name=\"deep-diver/book-tracking-react\"),\n",
            " Repository(full_name=\"deep-diver/calculator\"),\n",
            " Repository(full_name=\"deep-diver/CIFAR10-img-classification-tensorflow\"),\n",
            " Repository(full_name=\"deep-diver/CIFAR10-VGG19-Tensorflow\"),\n",
            " Repository(full_name=\"deep-diver/complete-mlops-system-workflow\"),\n",
            " Repository(full_name=\"deep-diver/conn\"),\n",
            " Repository(full_name=\"deep-diver/Continuous-Adaptation-for-Machine-Learning-System-to-Data-Changes\"),\n",
            " Repository(full_name=\"deep-diver/Continuous-Adaptation-with-VertexAI-AutoML-Pipeline\"),\n",
            " Repository(full_name=\"deep-diver/Data-Analysis-on-RedWine\"),\n",
            " Repository(full_name=\"deep-diver/Data-Analysis-on-Titanic\"),\n",
            " Repository(full_name=\"deep-diver/Data-Wrangling-on-OpenStreeMap\"),\n",
            " Repository(full_name=\"deep-diver/deep-diver\"),\n",
            " Repository(full_name=\"deep-diver/deeplearning-in-3-steps-book\"),\n",
            " Repository(full_name=\"deep-diver/deeplearningbook-korean-translation\"),\n",
            " Repository(full_name=\"deep-diver/DeepModels\"),\n",
            " Repository(full_name=\"deep-diver/deploy-stable-diffusion-tfserving\"),\n",
            " Repository(full_name=\"deep-diver/dvc-practice\"),\n",
            " Repository(full_name=\"deep-diver/dvc-practice2\"),\n",
            " Repository(full_name=\"deep-diver/dvc-test\"),\n",
            " Repository(full_name=\"deep-diver/EN-FR-MLT-tensorflow\"),\n",
            " Repository(full_name=\"deep-diver/Enron-Data-Analysis\"),\n",
            " Repository(full_name=\"deep-diver/fastai-course\"),\n",
            " Repository(full_name=\"deep-diver/fastai-course-korean\"),\n",
            " Repository(full_name=\"deep-diver/fastai-course-kr\"),\n",
            " Repository(full_name=\"deep-diver/fastai-course-study\"),\n",
            " Repository(full_name=\"deep-diver/fastmac\"),\n",
            " Repository(full_name=\"deep-diver/fastmac-tester\"),\n",
            " Repository(full_name=\"deep-diver/fastrelease_test\"),\n",
            " Repository(full_name=\"deep-diver/fb-group-corpus-collector\"),\n",
            " Repository(full_name=\"deep-diver/fb-group-post-fetcher\"),\n",
            " Repository(full_name=\"deep-diver/fb-group-post-scrapper\"),\n",
            " Repository(full_name=\"deep-diver/fb-group-scrapper-tester\"),\n",
            " Repository(full_name=\"deep-diver/foam-test\"),\n",
            " Repository(full_name=\"deep-diver/gde-codelabs\"),\n",
            " Repository(full_name=\"deep-diver/githubaction-demo\"),\n",
            " Repository(full_name=\"deep-diver/githubaction-demo2\"),\n",
            " Repository(full_name=\"deep-diver/githubaction-tester\"),\n",
            " Repository(full_name=\"deep-diver/gitmlops-test1\"),\n",
            " Repository(full_name=\"deep-diver/gradio-chat\"),\n",
            " Repository(full_name=\"deep-diver/hello-github-actions\"),\n",
            " Repository(full_name=\"deep-diver/hello-world\"),\n",
            " Repository(full_name=\"deep-diver/hf-hub-utils\"),\n",
            " Repository(full_name=\"deep-diver/image_search_with_natural_language\"),\n",
            " Repository(full_name=\"deep-diver/KaggleNotebook-Notes\"),\n",
            " Repository(full_name=\"deep-diver/keras-sd-serving\"),\n",
            " Repository(full_name=\"deep-diver/korean-ptsa\"),\n",
            " Repository(full_name=\"deep-diver/Linear-Regression\"),\n",
            " Repository(full_name=\"deep-diver/little-jarvis\"),\n",
            " Repository(full_name=\"deep-diver/llama-up-tmp\"),\n",
            " Repository(full_name=\"deep-diver/LLM-As-Chatbot\"),\n",
            " Repository(full_name=\"deep-diver/LLM-Collaborate\"),\n",
            " Repository(full_name=\"deep-diver/LLM-Pool\"),\n",
            " Repository(full_name=\"deep-diver/LLM-Pref-Mark-UI\"),\n",
            " Repository(full_name=\"deep-diver/LLM-Serve\"),\n",
            " Repository(full_name=\"deep-diver/LLMs-Colab\"),\n",
            " Repository(full_name=\"deep-diver/Logistic-Regression\"),\n",
            " Repository(full_name=\"deep-diver/LoRA-deployment\"),\n",
            " Repository(full_name=\"deep-diver/Machine-Learning-Yearning-Korean-Translation\"),\n",
            " Repository(full_name=\"deep-diver/ml-deployment-k8s-tfserving\"),\n",
            " Repository(full_name=\"deep-diver/ml-fn-impls\"),\n",
            " Repository(full_name=\"deep-diver/mlops-demo\"),\n",
            " Repository(full_name=\"deep-diver/mlops-hf-tf-vision-models\"),\n",
            " Repository(full_name=\"deep-diver/Model-Training-as-a-CI-CD-System\"),\n",
            " Repository(full_name=\"deep-diver/my-newsletter\"),\n",
            " Repository(full_name=\"deep-diver/neighborhood-map-react\"),\n",
            " Repository(full_name=\"deep-diver/never-leaving-vscode\"),\n",
            " Repository(full_name=\"deep-diver/newsletter-test\"),\n",
            " Repository(full_name=\"deep-diver/newsletter_awesome_blogs\"),\n",
            " Repository(full_name=\"deep-diver/object-detection-test\"),\n",
            " Repository(full_name=\"deep-diver/Object-Detection-YOLOv2-Darkflow\"),\n",
            " Repository(full_name=\"deep-diver/odaserver\"),\n",
            " Repository(full_name=\"deep-diver/paper-code-match\"),\n",
            " Repository(full_name=\"deep-diver/personal_newsletter_curation\"),\n",
            " Repository(full_name=\"deep-diver/PingPong\"),\n",
            " Repository(full_name=\"deep-diver/pocket-ml-reference-korean\"),\n",
            " Repository(full_name=\"deep-diver/portfolio_template\"),\n",
            " Repository(full_name=\"deep-diver/practical-time-series-analysis-korean\"),\n",
            " Repository(full_name=\"deep-diver/Practical-TimeSeries-Analysis-Korean\"),\n",
            " Repository(full_name=\"deep-diver/PyGithubTest\"),\n",
            " Repository(full_name=\"deep-diver/Python-Machine-Learning-Book-Practice\"),\n",
            " Repository(full_name=\"deep-diver/ready\"),\n",
            " Repository(full_name=\"deep-diver/Responsive-Portfolio\"),\n",
            " Repository(full_name=\"deep-diver/rnn_simple\"),\n",
            " Repository(full_name=\"deep-diver/Sampling-Distribution-on-Poker-Cards-\"),\n",
            " Repository(full_name=\"deep-diver/SD-TFTRT\"),\n",
            " Repository(full_name=\"deep-diver/segformer-tf-transformers\"),\n",
            " Repository(full_name=\"deep-diver/semantic-segmentation-ml-pipeline\"),\n",
            " Repository(full_name=\"deep-diver/smartwork-with-python\"),\n",
            " Repository(full_name=\"deep-diver/Soccer-Ball-Detection-YOLOv2\"),\n",
            " Repository(full_name=\"deep-diver/test-course\"),\n",
            " Repository(full_name=\"deep-diver/test-course1\"),\n",
            " Repository(full_name=\"deep-diver/test-course22\"),\n",
            " Repository(full_name=\"deep-diver/test-platform\"),\n",
            " Repository(full_name=\"deep-diver/test-repo\"),\n",
            " Repository(full_name=\"deep-diver/test1\"),\n",
            " Repository(full_name=\"deep-diver/testtttt\"),\n",
            " Repository(full_name=\"deep-diver/test_img_clf\"),\n",
            " Repository(full_name=\"deep-diver/textual-inversion-pipeline\"),\n",
            " Repository(full_name=\"deep-diver/tfs-grpc-loadtest\"),\n",
            " Repository(full_name=\"deep-diver/tfx-gpu-docker\"),\n",
            " Repository(full_name=\"deep-diver/TFX-WandB\"),\n",
            " Repository(full_name=\"deep-diver/tfx_template\"),\n",
            " Repository(full_name=\"deep-diver/VGG\"),\n",
            " Repository(full_name=\"deep-diver/voila-notebooks\"),\n",
            " Repository(full_name=\"deep-diver/YOLO-Impl-Tensorflow\")]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract source codes and save in CSV\n",
        "\n",
        "The following code snippet works in the following manner:\n",
        "\n",
        "1. Get list of files (*.py and *.ipynb) in the target repositories (`target_repos`)\n",
        "2. Grasp the content of each file and decode it with `base64`\n",
        "  - for `*.py`, plain text will be extracted\n",
        "  - for `*.ipynb`, the contents of all the code cells will be extracted and merged as a single string\n",
        "3. Create a `pd.DataFrame` of `[\"reponame\", \"filepath\", \"content\"]` column to store repository, filepath, and the extracted content\n",
        "4. Iterate 1 ~ 3 steps for all target repositories, and append DataFrame to `df` which contains all records"
      ],
      "metadata": {
        "id": "wcADppJxc2Dk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_repos = [\n",
        "  \"Continuous-Adaptation-for-Machine-Learning-System-to-Data-Changes\",\n",
        "  \"semantic-segmentation-ml-pipeline\",\n",
        "  \"mlops-hf-tf-vision-models\",\n",
        "]"
      ],
      "metadata": {
        "id": "6yU6SwsBpjKZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import pandas as pd\n",
        "from nbformat import reads, NO_CONVERT\n",
        "\n",
        "df = pd.DataFrame(columns=[\"reponame\", \"filepath\", \"content\"])\n",
        "\n",
        "from github import GithubException\n",
        "\n",
        "def get_py_files(repo, file_list, path=\".\"):\n",
        "  contents = repo.get_contents(\"\")\n",
        "  while contents:\n",
        "    file_content = contents.pop(0)\n",
        "    if file_content.type == \"dir\":\n",
        "      contents.extend(repo.get_contents(file_content.path))\n",
        "    else:\n",
        "      if file_content.name[-2:] == \"py\":\n",
        "        file_list.append(file_content)\n",
        "      elif file_content.name[-5:] == \"ipynb\":\n",
        "        file_list.append(file_content)\n",
        "\n",
        "for repo in repos:\n",
        "  if repo.name in target_repos:\n",
        "    file_list = []\n",
        "    get_py_files(repo, file_list)\n",
        "\n",
        "    if len(file_list) != 0:\n",
        "      for file in file_list:\n",
        "          if file.name.endswith(\"py\"):\n",
        "            content = file.content\n",
        "            content_str = base64.b64decode(content).decode('utf-8')\n",
        "\n",
        "            if content != '':\n",
        "              df = pd.concat(\n",
        "                  [\n",
        "                      df, \n",
        "                      pd.DataFrame.from_dict([{\n",
        "                        \"reponame\": repo.name,\n",
        "                        \"filepath\": file.path,\n",
        "                        \"content\": content_str\n",
        "                      }])\n",
        "                  ])"
      ],
      "metadata": {
        "id": "TBFOSlXARJpd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "dHE9SnbTmgjh",
        "outputId": "7649cdf4-ffeb-46d3-e1bf-cfdcf7475e6f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             reponame  \\\n",
              "0   Continuous-Adaptation-for-Machine-Learning-Sys...   \n",
              "0   Continuous-Adaptation-for-Machine-Learning-Sys...   \n",
              "0   Continuous-Adaptation-for-Machine-Learning-Sys...   \n",
              "0   Continuous-Adaptation-for-Machine-Learning-Sys...   \n",
              "0   Continuous-Adaptation-for-Machine-Learning-Sys...   \n",
              "..                                                ...   \n",
              "0                   semantic-segmentation-ml-pipeline   \n",
              "0                   semantic-segmentation-ml-pipeline   \n",
              "0                   semantic-segmentation-ml-pipeline   \n",
              "0                   semantic-segmentation-ml-pipeline   \n",
              "0                   semantic-segmentation-ml-pipeline   \n",
              "\n",
              "                                             filepath  \\\n",
              "0           custom_components/batch_pred_evaluator.py   \n",
              "0        custom_components/batch_prediction_vertex.py   \n",
              "0                  custom_components/file_list_gen.py   \n",
              "0                custom_components/span_preparator.py   \n",
              "0      custom_components/training_pipeline_trigger.py   \n",
              "..                                                ...   \n",
              "0   training_pipeline/pipeline/components/HFPusher...   \n",
              "0   training_pipeline/pipeline/components/HFPusher...   \n",
              "0   training_pipeline/pipeline/components/HFPusher...   \n",
              "0   training_pipeline/pipeline/components/HFPusher...   \n",
              "0   training_pipeline/pipeline/components/HFPusher...   \n",
              "\n",
              "                                              content  \n",
              "0   \"\"\"\\nThis component evaluates the performance ...  \n",
              "0   \"\"\"\\nThis component launches a Batch Predictio...  \n",
              "0   \"\"\"\\nGenerate a txt file formatted required by...  \n",
              "0   \"\"\"\\nThis component is responsible for separat...  \n",
              "0   \"\"\"\\nComponent responsible for triggering a tr...  \n",
              "..                                                ...  \n",
              "0   # Copyright 2022 The TensorFlow Authors. All R...  \n",
              "0   # Copyright 2022 The TensorFlow Authors. All R...  \n",
              "0   # Copyright 2022 The TensorFlow Authors. All R...  \n",
              "0   from huggingface_hub import ModelCard, ModelCa...  \n",
              "0   # Copyright 2022 The TensorFlow Authors. All R...  \n",
              "\n",
              "[102 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-27675b93-b31f-4ef0-8bd3-cd445de1d13a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reponame</th>\n",
              "      <th>filepath</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Continuous-Adaptation-for-Machine-Learning-Sys...</td>\n",
              "      <td>custom_components/batch_pred_evaluator.py</td>\n",
              "      <td>\"\"\"\\nThis component evaluates the performance ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Continuous-Adaptation-for-Machine-Learning-Sys...</td>\n",
              "      <td>custom_components/batch_prediction_vertex.py</td>\n",
              "      <td>\"\"\"\\nThis component launches a Batch Predictio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Continuous-Adaptation-for-Machine-Learning-Sys...</td>\n",
              "      <td>custom_components/file_list_gen.py</td>\n",
              "      <td>\"\"\"\\nGenerate a txt file formatted required by...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Continuous-Adaptation-for-Machine-Learning-Sys...</td>\n",
              "      <td>custom_components/span_preparator.py</td>\n",
              "      <td>\"\"\"\\nThis component is responsible for separat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Continuous-Adaptation-for-Machine-Learning-Sys...</td>\n",
              "      <td>custom_components/training_pipeline_trigger.py</td>\n",
              "      <td>\"\"\"\\nComponent responsible for triggering a tr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>semantic-segmentation-ml-pipeline</td>\n",
              "      <td>training_pipeline/pipeline/components/HFPusher...</td>\n",
              "      <td># Copyright 2022 The TensorFlow Authors. All R...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>semantic-segmentation-ml-pipeline</td>\n",
              "      <td>training_pipeline/pipeline/components/HFPusher...</td>\n",
              "      <td># Copyright 2022 The TensorFlow Authors. All R...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>semantic-segmentation-ml-pipeline</td>\n",
              "      <td>training_pipeline/pipeline/components/HFPusher...</td>\n",
              "      <td># Copyright 2022 The TensorFlow Authors. All R...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>semantic-segmentation-ml-pipeline</td>\n",
              "      <td>training_pipeline/pipeline/components/HFPusher...</td>\n",
              "      <td>from huggingface_hub import ModelCard, ModelCa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>semantic-segmentation-ml-pipeline</td>\n",
              "      <td>training_pipeline/pipeline/components/HFPusher...</td>\n",
              "      <td># Copyright 2022 The TensorFlow Authors. All R...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>102 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-27675b93-b31f-4ef0-8bd3-cd445de1d13a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-27675b93-b31f-4ef0-8bd3-cd445de1d13a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-27675b93-b31f-4ef0-8bd3-cd445de1d13a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save the resuling `DataFrame` to CSV"
      ],
      "metadata": {
        "id": "Xxiv2fIBJY_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"chansung.csv\")"
      ],
      "metadata": {
        "id": "P3e0OF3OGbgI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Include Ipynb content (optional)"
      ],
      "metadata": {
        "id": "_nUlx-picmhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = None\n",
        "\n",
        "for file in file_list:\n",
        "  if file.name[-5:] == \"ipynb\":\n",
        "    a = file\n",
        "    break"
      ],
      "metadata": {
        "id": "KshJynqTbLGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "from nbformat import reads, NO_CONVERT\n",
        "\n",
        "content = a.content\n",
        "content = base64.b64decode(content).decode('utf-8')\n",
        "\n",
        "notebook = reads(content, NO_CONVERT)\n",
        "\n",
        "cells = notebook['cells']\n",
        "code_cells = [c for c in cells if c['cell_type'] == 'code']\n",
        "for cell in code_cells:\n",
        "    print(cell['source'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNm8pVS6cFqL",
        "outputId": "f5d45c09-a64c-4096-d927-9ac4b7195a4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!gcloud init\n",
            "from google.colab import auth\n",
            "\n",
            "auth.authenticate_user()\n",
            "TARGET_ROOT_DIR = \"cifar10\"\n",
            "TARGET_TRAIN_DIR = TARGET_ROOT_DIR + \"/span-1/train\"\n",
            "TARGET_TEST_DIR = TARGET_ROOT_DIR + \"/span-1/test\"\n",
            "\n",
            "!mkdir -p {TARGET_TRAIN_DIR}\n",
            "!mkdir -p {TARGET_TEST_DIR}\n",
            "import tensorflow_datasets as tfds\n",
            "\n",
            "# Generate TFRecords with TFDS\n",
            "builder = tfds.builder(\"cifar10\")\n",
            "builder.download_and_prepare()\n",
            "!cp {builder.data_dir}/cifar10-train.tfrecord-00000-of-00001 {TARGET_TRAIN_DIR}/cifar10-train.tfrecord\n",
            "!cp {builder.data_dir}/cifar10-test.tfrecord-00000-of-00001 {TARGET_TEST_DIR}/cifar10-test.tfrecord\n",
            "!ls -R {TARGET_ROOT_DIR}\n",
            "#@title GCS\n",
            "#@markdown You should change these values as per your preferences. The copy operation can take ~5 minutes. \n",
            "BUCKET_PATH = \"gs://cifar10-csp-public2\" #@param {type:\"string\"}\n",
            "REGION = \"us-central1\" #@param {type:\"string\"}\n",
            "\n",
            "!gsutil mb -l {REGION} {BUCKET_PATH}\n",
            "!gsutil -m cp -r {TARGET_ROOT_DIR}/* {BUCKET_PATH}\n",
            "!gsutil ls -R {BUCKET_PATH}/\n",
            "!pip install tfx==1.2.0\n",
            "from tfx import v1 as tfx\n",
            "from tfx.components.example_gen import utils\n",
            "from tfx.proto import example_gen_pb2\n",
            "\n",
            "_DATA_PATH = \"gs://cifar10-csp-public\"\n",
            "\n",
            "splits = [\n",
            "    example_gen_pb2.Input.Split(name=\"train\", pattern=\"span-{SPAN}/train/*\"),\n",
            "    example_gen_pb2.Input.Split(name=\"val\", pattern=\"span-{SPAN}/test/*\"),\n",
            "]\n",
            "\n",
            "_, span, version = utils.calculate_splits_fingerprint_span_and_version(\n",
            "    _DATA_PATH, splits\n",
            ")\n",
            "span, version\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import pandas as pd\n",
        "from nbformat import reads, NO_CONVERT\n",
        "\n",
        "df = pd.DataFrame(columns=[\"reponame\", \"filepath\", \"content\"])\n",
        "\n",
        "from github import GithubException\n",
        "\n",
        "def get_py_files(repo, file_list, path=\".\"):\n",
        "  contents = repo.get_contents(\"\")\n",
        "  while contents:\n",
        "    file_content = contents.pop(0)\n",
        "    if file_content.type == \"dir\":\n",
        "      contents.extend(repo.get_contents(file_content.path))\n",
        "    else:\n",
        "      if file_content.name[-2:] == \"py\":\n",
        "        file_list.append(file_content)\n",
        "      elif file_content.name[-5:] == \"ipynb\":\n",
        "        file_list.append(file_content)\n",
        "\n",
        "for repo in repos:\n",
        "  if repo.name in target_repos:\n",
        "    file_list = []\n",
        "    get_py_files(repo, file_list)\n",
        "\n",
        "    if len(file_list) != 0:\n",
        "      for file in file_list:\n",
        "          if file.name.endswith(\"py\"):\n",
        "            content = file.content\n",
        "            content_str = base64.b64decode(content).decode('utf-8')\n",
        "\n",
        "            if content != '':\n",
        "              df = pd.concat(\n",
        "                  [\n",
        "                      df, \n",
        "                      pd.DataFrame.from_dict([{\n",
        "                        \"reponame\": repo.name,\n",
        "                        \"filepath\": file.path,\n",
        "                        \"content\": content_str\n",
        "                      }])\n",
        "                  ])\n",
        "          elif file.name.endswith(\"ipynb\"):\n",
        "            content = file.content\n",
        "            content_str = base64.b64decode(content).decode('utf-8')\n",
        "                        \n",
        "            code_cell_str = \"\"\n",
        "            notebook = reads(content_str, NO_CONVERT)\n",
        "\n",
        "            code_cells = [\n",
        "              c for c in notebook['cells'] \n",
        "              if c['cell_type'] == 'code'\n",
        "            ]\n",
        "            \n",
        "            for cell in code_cells:\n",
        "              code_cell_str += cell['source']\n",
        "\n",
        "            df = pd.concat(\n",
        "                [\n",
        "                    df, \n",
        "                    pd.DataFrame.from_dict([{\n",
        "                      \"reponame\": repo.name,\n",
        "                      \"filepath\": file.path,\n",
        "                      \"content\": code_cell_str\n",
        "                    }])\n",
        "                ])"
      ],
      "metadata": {
        "id": "5-KNpkch4Usc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}