,reponame,filepath,content
0,Continuous-Adaptation-for-Machine-Learning-System-to-Data-Changes,custom_components/batch_pred_evaluator.py,"""""""
This component evaluates the performance of a currently deployed model, and 
the evaluation is based on the result of batch prediction on Vertex AI from the previous component.
At the end, this component will output true or false to indicate if retraining is needed.
Reference: https://bit.ly/vertex-batch
""""""

from tfx.dsl.component.experimental.annotations import Parameter, OutputArtifact
from tfx.dsl.component.experimental.decorators import component
from tfx.types.experimental.simple_artifacts import Dataset

from absl import logging
import os
import json


@component
def PerformanceEvaluator(
    gcs_destination: Parameter[str],
    local_directory: Parameter[str],
    threshold: Parameter[float],
    trigger_pipeline: OutputArtifact[Dataset],
):
    """"""
     gcs_destination: GCS location where the files containing
                      the result of batch prediction is
     local_directory: Temporary directory to hold files copied
                      from the gcs_destination
           threshold: threshold to decide if retraining is needed or not
                      it is based on the measured accuracy
    trigger_pipeline: an output artifact which hold true or false
                      to indicate if retraining is needed or not
    """"""

    full_gcs_results_dir = f""{gcs_destination}/{local_directory}""

    # Create missing directories.
    os.makedirs(local_directory, exist_ok=True)

    # Get the Cloud Storage paths for each result.
    os.system(f""gsutil -m cp -r {full_gcs_results_dir} {local_directory}"")

    # Get most recently modified directory.
    latest_directory = max(
        [os.path.join(local_directory, d) for d in os.listdir(local_directory)],
        key=os.path.getmtime,
    )

    # Get downloaded results in directory.
    results_files = []
    for dirpath, subdirs, files in os.walk(latest_directory):
        for file in files:
            if file.startswith(""prediction.results""):
                results_files.append(os.path.join(dirpath, file))

    # Consolidate all the results into a list.
    results = []
    for results_file in results_files:
        # Download each result.
        with open(results_file, ""r"") as file:
            results.extend([json.loads(line) for line in file.readlines()])

    # Calculate performance.
    num_correct = 0

    for result in results:
        label = os.path.basename(result[""instance""]).split(""_"")[0]
        prediction = result[""prediction""][""label""]

        if label == prediction:
            num_correct = num_correct + 1

    accuracy = num_correct / len(results)
    logging.info(f""Accuracy: {accuracy*100}%"")

    # Store the boolean result.
    trigger_pipeline.set_string_custom_property(""result"", str(accuracy >= threshold))
"
0,Continuous-Adaptation-for-Machine-Learning-System-to-Data-Changes,custom_components/batch_prediction_vertex.py,"""""""
This component launches a Batch Prediction job on Vertex AI.
Know more about Vertex AI Batch Predictions jobs, go here:
https://cloud.google.com/vertex-ai/docs/predictions/batch-predictions.
""""""

from google.cloud import storage

from tfx.dsl.component.experimental.annotations import Parameter, InputArtifact
from tfx.dsl.component.experimental.decorators import component
from tfx.types.standard_artifacts import String
import google.cloud.aiplatform as vertex_ai

from absl import logging


@component
def BatchPredictionGen(
    gcs_source: InputArtifact[String],
    project: Parameter[str],
    location: Parameter[str],
    model_resource_name: Parameter[str],
    job_display_name: Parameter[str],
    gcs_destination: Parameter[str],
    instances_format: Parameter[str] = ""file-list"",
    machine_type: Parameter[str] = ""n1-standard-2"",
    accelerator_count: Parameter[int] = 0,
    accelerator_type: Parameter[str] = None,
    starting_replica_count: Parameter[int] = 1,
    max_replica_count: Parameter[int] = 1,
):
    """"""
    gcs_source: A location inside GCS to be used by the Batch Prediction job to get its inputs.
    Rest of the parameters are explained here: https://git.io/JiUyU.
    """"""
    storage_client = storage.Client()

    # Read GCS Source (gcs_source contains the full path of GCS object).
    # 1-1. get bucketname from gcs_source
    gcs_source_uri = gcs_source.uri.split(""//"")[1:][0].split(""/"")
    bucketname = gcs_source_uri[0]
    bucket = storage_client.get_bucket(bucketname)
    logging.info(f""bucketname: {bucketname}"")

    # 1-2. get object path without the bucket name.
    objectpath = ""/"".join(gcs_source_uri[1:])

    # 1-3. read the object to get value set by OutputArtifact from FileListGen.
    blob = bucket.blob(objectpath)
    logging.info(f""objectpath: {objectpath}"")

    gcs_source = f""gs://{blob.download_as_text()}""

    # Get Model.
    vertex_ai.init(project=project, location=location)
    model = vertex_ai.Model.list(
        filter=f""display_name={model_resource_name}"", order_by=""update_time""
    )[-1]

    # Launch a Batch Prediction job.
    logging.info(""Starting batch prediction job."")
    logging.info(f""GCS path where file list is: {gcs_source}"")
    batch_prediction_job = model.batch_predict(
        job_display_name=job_display_name,
        instances_format=instances_format,
        gcs_source=gcs_source,
        gcs_destination_prefix=gcs_destination,
        machine_type=machine_type,
        accelerator_count=accelerator_count,
        accelerator_type=accelerator_type,
        starting_replica_count=starting_replica_count,
        max_replica_count=max_replica_count,
        sync=True,
    )

    logging.info(batch_prediction_job.display_name)
    logging.info(batch_prediction_job.resource_name)
    logging.info(batch_prediction_job.state)
"
0,Continuous-Adaptation-for-Machine-Learning-System-to-Data-Changes,custom_components/file_list_gen.py,"""""""
Generate a txt file formatted required by Vertex AI's Batch Prediction
There are few options, and this component generate ""file list"" formatted txt.
(https://cloud.google.com/vertex-ai/docs/predictions/batch-predictions)
""""""

import tensorflow as tf
from absl import logging

from tfx.dsl.component.experimental.decorators import component
from tfx.dsl.component.experimental.annotations import Parameter, OutputArtifact
from tfx.types.standard_artifacts import String


@component
def FileListGen(
    outpath: OutputArtifact[String],
    gcs_src_bucket: Parameter[str],
    gcs_src_prefix: Parameter[str] = """",
    output_filename: Parameter[str] = ""test-images.txt"",
):
    """"""
    : param outpath: OutputArtifact to hold where output_filename will be located
             This will be used in the downstream component, BatchPredictionGen
    : param gcs_src_bucket: GCS bucket name where the list of raw data is
    : param gcs_src_prefix: prefix to be added to gcs_src_bucket
    : param output_filename: output filename whose content is a list of file paths of raw data
    """"""
    logging.info(""FileListGen started"")

    # 1. get the list of data
    gcs_src_prefix = (
        f""{gcs_src_prefix}/"" if len(gcs_src_prefix) != 0 else gcs_src_prefix
    )
    img_paths = tf.io.gfile.glob(f""gs://{gcs_src_bucket}/{gcs_src_prefix}*.jpg"")
    logging.info(""Successfully retrieve the file(jpg) list from GCS path"")

    # 2. write the list of data in the expected format in Vertex AI Batch Prediction to a local file
    with open(output_filename, ""w"", encoding=""utf-8"") as f:
        f.writelines(""%s\n"" % img_path for img_path in img_paths)
    logging.info(
        f""Successfully created the file list file({output_filename}) in local storage""
    )

    # 3. upload the local file to GCS location
    gcs_dst = f""{gcs_src_bucket}/{gcs_src_prefix}{output_filename}""
    tf.io.gfile.copy(output_filename, f""gs://{gcs_dst}"", overwrite=True)
    logging.info(f""Successfully uploaded the file list ({gcs_dst})"")

    # 4. store the GCS location where the local file is
    outpath.value = gcs_dst
"
0,Continuous-Adaptation-for-Machine-Learning-System-to-Data-Changes,custom_components/span_preparator.py,"""""""
This component is responsible for separating provided samples into training and
validation splits. It then converts them to TFRecords and stores those inside
a GCS location. Finally, it returns the latest span id calculated from the current
samples in `gcs_source_bucket`.
""""""

from tfx.dsl.component.experimental.decorators import component
from tfx.dsl.component.experimental.annotations import Parameter
from tfx.dsl.component.experimental.annotations import OutputArtifact, InputArtifact
from tfx.types.experimental.simple_artifacts import Dataset
from absl import logging

from datetime import datetime
import tensorflow as tf
import random
import os


# Label-mapping.
LABEL_DICT = {
    ""airplane"": 0,
    ""automobile"": 1,
    ""bird"": 2,
    ""cat"": 3,
    ""deer"": 4,
    ""dog"": 5,
    ""frog"": 6,
    ""horse"": 7,
    ""ship"": 8,
    ""truck"": 9,
}


# Images are byte-strings.
def _bytestring_feature(list_of_bytestrings):
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=list_of_bytestrings))


# Classes would be integers.
def _int_feature(list_of_ints):
    return tf.train.Feature(int64_list=tf.train.Int64List(value=list_of_ints))


# Function that prepares a record for the tfrecord file
# a record contains the image and its label.
def to_tfrecord(img_bytes, label):
    feature = {
        ""image"": _bytestring_feature([img_bytes]),
        ""label"": _int_feature([label]),
    }
    return tf.train.Example(features=tf.train.Features(feature=feature))


def write_tfrecords(filepaths, dest_gcs, tfrecord_filename, new_span, is_train):
    # For this project, we are serializing the images in one TFRecord only.
    # For more realistic purposes, this should be sharded.
    folder = ""train"" if is_train else ""test""

    with tf.io.TFRecordWriter(tfrecord_filename) as writer:
        for path in filepaths:
            image_string = tf.io.read_file(path).numpy()
            class_name = path.split(""/"")[-1].split(""_"")[0]
            label = LABEL_DICT[class_name]

            example = to_tfrecord(image_string, label)
            writer.write(example.SerializeToString())

    # Copy over the zipped TFRecord file to the GCS Bucket and
    # remove the temporary files.
    logging.info(f""gsutil cp {tfrecord_filename} {dest_gcs}/span-{new_span}/{folder}/"")
    os.system(f""gsutil cp {tfrecord_filename} {dest_gcs}/span-{new_span}/{folder}/"")
    os.remove(tfrecord_filename)


@component
def SpanPreparator(
    is_retrain: InputArtifact[Dataset],
    gcs_source_bucket: Parameter[str],
    gcs_destination_bucket: Parameter[str],
    latest_span_id: OutputArtifact[Dataset],
    gcs_source_prefix: Parameter[str] = """",
):
    """"""
    :param is_retrain: Boolean to indicate if we are retraining.
    :param gcs_source_bucket: GCS location where the entry samples are residing.
    :param gcs_destination_bucket: GCS location where the converted TFRecords will be serialized.
    :param latest_span_id: Data span.
    :param gcs_source_prefix: Location prefix.
    """"""
    if is_retrain.get_string_custom_property(""result"") == ""False"":
        # Get the latest span and determine the new span.
        last_span_str = tf.io.gfile.glob(f""{gcs_destination_bucket}/span-*"")[-1]
        last_span = int(last_span_str.split(""-"")[-1])
        new_span = last_span + 1

        timestamp = datetime.utcnow().strftime(""%y%m%d-%H%M%S"")

        # Get images from the provided GCS source.
        image_paths = tf.io.gfile.glob(f""gs://{gcs_source_bucket}/*.jpg"")
        logging.info(image_paths)
        random.shuffle(image_paths)

        # Create train and validation splits.
        val_split = 0.2
        split_index = int(len(image_paths) * (1 - val_split))
        training_paths = image_paths[:split_index]
        validation_paths = image_paths[split_index:]

        # Write as TFRecords.
        write_tfrecords(
            training_paths,
            gcs_destination_bucket,
            tfrecord_filename=f""new_training_data_{timestamp}.tfrecord"",
            new_span=new_span,
            is_train=True,
        )
        write_tfrecords(
            validation_paths,
            gcs_destination_bucket,
            tfrecord_filename=f""new_validation_data_{timestamp}.tfrecord"",
            new_span=new_span,
            is_train=False,
        )

        logging.info(""Removing images from batch prediction bucket."")
        os.system(
            f""gsutil mv gs://{gcs_source_bucket}/{gcs_source_prefix} gs://{gcs_source_bucket}/{gcs_source_prefix}_old""
        )
        latest_span_id.set_string_custom_property(""latest_span"", str(new_span))
"
0,Continuous-Adaptation-for-Machine-Learning-System-to-Data-Changes,custom_components/training_pipeline_trigger.py,"""""""
Component responsible for triggering a training job given a pipeline specification.
""""""

import json

from google.cloud import storage

from kfp.v2.google.client import AIPlatformClient
from tfx.dsl.component.experimental.annotations import Parameter, InputArtifact
from tfx.dsl.component.experimental.decorators import component
from tfx.types.experimental.simple_artifacts import Dataset

from absl import logging


@component
def PipelineTrigger(
    is_retrain: InputArtifact[Dataset],
    latest_span_id: InputArtifact[Dataset],
    pipeline_spec_path: Parameter[str],
    project_id: Parameter[str],
    region: Parameter[str],
):
    """"""
    :param is_retrain: Boolean to indicate if we are retraining.
    :param latest_span_id: Latest span id to craft training data for the model.
    :param pipeline_spec_path: Training pipeline specification path.
    :param project_id: GCP project id.
    :param region: GCP region.
    """"""
    if is_retrain.get_string_custom_property(""result"") == ""False"":
        # Check if the pipeline spec exists.
        storage_client = storage.Client()

        path_parts = pipeline_spec_path.replace(""gs://"", """").split(""/"")
        bucket_name = path_parts[0]
        blob_name = ""/"".join(path_parts[1:])

        bucket = storage_client.bucket(bucket_name)
        blob = storage.Blob(bucket=bucket, name=blob_name)

        if not blob.exists(storage_client):
            raise ValueError(f""{pipeline_spec_path} does not exist."")

        # Initialize Vertex AI API client and submit for pipeline execution.
        api_client = AIPlatformClient(project_id=project_id, region=region)

        # Fetch the latest span.
        latest_span = latest_span_id.get_string_custom_property(""latest_span"")

        # Create a training job from pipeline spec.
        response = api_client.create_run_from_job_spec(
            pipeline_spec_path,
            enable_caching=False,
            parameter_values={
                ""input-config"": json.dumps(
                    {
                        ""splits"": [
                            {
                                ""name"": ""train"",
                                ""pattern"": f""span-[{int(latest_span)-1}{latest_span}]/train/*.tfrecord"",
                            },
                            {
                                ""name"": ""val"",
                                ""pattern"": f""span-[{int(latest_span)-1}{latest_span}]/test/*.tfrecord"",
                            },
                        ]
                    }
                ),
                ""output-config"": json.dumps({}),
            },
        )
        logging.info(response)
"
0,Continuous-Adaptation-for-Machine-Learning-System-to-Data-Changes,notebooks/01_Dataset_Prep.ipynb,"!gcloud initfrom google.colab import auth

auth.authenticate_user()TARGET_ROOT_DIR = ""cifar10""
TARGET_TRAIN_DIR = TARGET_ROOT_DIR + ""/span-1/train""
TARGET_TEST_DIR = TARGET_ROOT_DIR + ""/span-1/test""

!mkdir -p {TARGET_TRAIN_DIR}
!mkdir -p {TARGET_TEST_DIR}import tensorflow_datasets as tfds

# Generate TFRecords with TFDS
builder = tfds.builder(""cifar10"")
builder.download_and_prepare()!cp {builder.data_dir}/cifar10-train.tfrecord-00000-of-00001 {TARGET_TRAIN_DIR}/cifar10-train.tfrecord
!cp {builder.data_dir}/cifar10-test.tfrecord-00000-of-00001 {TARGET_TEST_DIR}/cifar10-test.tfrecord!ls -R {TARGET_ROOT_DIR}#@title GCS
#@markdown You should change these values as per your preferences. The copy operation can take ~5 minutes. 
BUCKET_PATH = ""gs://cifar10-csp-public2"" #@param {type:""string""}
REGION = ""us-central1"" #@param {type:""string""}

!gsutil mb -l {REGION} {BUCKET_PATH}
!gsutil -m cp -r {TARGET_ROOT_DIR}/* {BUCKET_PATH}!gsutil ls -R {BUCKET_PATH}/!pip install tfx==1.2.0from tfx import v1 as tfx
from tfx.components.example_gen import utilsfrom tfx.proto import example_gen_pb2

_DATA_PATH = ""gs://cifar10-csp-public""

splits = [
    example_gen_pb2.Input.Split(name=""train"", pattern=""span-{SPAN}/train/*""),
    example_gen_pb2.Input.Split(name=""val"", pattern=""span-{SPAN}/test/*""),
]

_, span, version = utils.calculate_splits_fingerprint_span_and_version(
    _DATA_PATH, splits
)span, version"
0,Continuous-Adaptation-for-Machine-Learning-System-to-Data-Changes,notebooks/02_TFX_Training_Pipeline.ipynb,"# Use the latest version of pip.
%%capture
!pip install --upgrade tfx==1.2.0 kfp==1.6.1
!pip install -q --upgrade google-cloud-aiplatform!gcloud initfrom google.colab import auth
auth.authenticate_user()import tensorflow as tf

print(""TensorFlow version: {}"".format(tf.__version__))
from tfx import v1 as tfx

print(""TFX version: {}"".format(tfx.__version__))
import kfp

print(""KFP version: {}"".format(kfp.__version__))

from google.cloud import aiplatform as vertex_ai
import osGOOGLE_CLOUD_PROJECT = ""gcp-ml-172005""  # @param {type:""string""}
GOOGLE_CLOUD_REGION = ""us-central1""  # @param {type:""string""}
GCS_BUCKET_NAME = ""cifar10-experimental-csp2""  # @param {type:""string""}
DATA_ROOT = ""gs://cifar10-csp-public2""  # @param {type:""string""}

if not (GOOGLE_CLOUD_PROJECT and GOOGLE_CLOUD_REGION and GCS_BUCKET_NAME):
    from absl import logging

    logging.error(""Please set all required parameters."")PIPELINE_NAME = ""continuous-adaptation-for-data-changes""

# Path to various pipeline artifact.
PIPELINE_ROOT = ""gs://{}/pipeline_root/{}"".format(GCS_BUCKET_NAME, PIPELINE_NAME)

# Paths for users' Python module.
MODULE_ROOT = ""gs://{}/pipeline_module/{}"".format(GCS_BUCKET_NAME, PIPELINE_NAME)

# This is the path where your model will be pushed for serving.
SERVING_MODEL_DIR = ""gs://{}/serving_model/{}"".format(GCS_BUCKET_NAME, PIPELINE_NAME)

print(""PIPELINE_ROOT: {}"".format(PIPELINE_ROOT))_trainer_module_file = 'trainer.py'%%writefile {_trainer_module_file}

from typing import List
from absl import logging
from tensorflow import keras
from tfx import v1 as tfx
import tensorflow as tf

_IMAGE_FEATURES = {
    ""image"": tf.io.FixedLenFeature([], tf.string),
    ""label"": tf.io.FixedLenFeature([], tf.int64),
}

_CONCRETE_INPUT = ""numpy_inputs""
_TRAIN_BATCH_SIZE = 64
_EVAL_BATCH_SIZE = 64
_INPUT_SHAPE = (32, 32, 3)
_EPOCHS = 2


def _parse_fn(example):
    example = tf.io.parse_single_example(example, _IMAGE_FEATURES)
    image = tf.image.decode_jpeg(example[""image""], channels=3)
    class_label = tf.cast(example[""label""], tf.int32)
    return image, class_label


def _input_fn(file_pattern: List[str], batch_size: int) -> tf.data.Dataset:
    print(f""Reading data from: {file_pattern}"")
    tfrecord_filenames = tf.io.gfile.glob(file_pattern[0] + "".gz"")
    print(tfrecord_filenames)
    dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=""GZIP"")
    dataset = dataset.map(_parse_fn).batch(batch_size)
    return dataset.repeat()


def _make_keras_model() -> tf.keras.Model:
    """"""Creates a ResNet50-based model for classifying flowers data.

  Returns:
  A Keras Model.
  """"""
    inputs = keras.Input(shape=_INPUT_SHAPE)
    base_model = keras.applications.ResNet50(
        include_top=False, input_shape=_INPUT_SHAPE, pooling=""avg""
    )
    base_model.trainable = False
    x = tf.keras.applications.resnet.preprocess_input(inputs)
    x = base_model(
        x, training=False
    )  # Ensures BatchNorm runs in inference model in this model
    outputs = keras.layers.Dense(10, activation=""softmax"")(x)
    model = keras.Model(inputs, outputs)

    model.compile(
        optimizer=keras.optimizers.Adam(),
        loss=tf.keras.losses.SparseCategoricalCrossentropy(),
        metrics=[keras.metrics.SparseCategoricalAccuracy()],
    )

    model.summary(print_fn=logging.info)
    return model


def _preprocess(bytes_input):
    decoded = tf.io.decode_jpeg(bytes_input, channels=3)
    resized = tf.image.resize(decoded, size=(32, 32))
    return resized


@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])
def preprocess_fn(bytes_inputs):
    decoded_images = tf.map_fn(
        _preprocess, bytes_inputs, dtype=tf.float32, back_prop=False
    )
    return {_CONCRETE_INPUT: decoded_images}


def _model_exporter(model: tf.keras.Model):
    m_call = tf.function(model.call).get_concrete_function(
        [tf.TensorSpec(shape=[None, 32, 32, 3], dtype=tf.float32, name=_CONCRETE_INPUT)]
    )

    @tf.function(input_signature=[tf.TensorSpec([None], tf.string)])
    def serving_fn(bytes_inputs):
        # This function comes from the Computer Vision book from O'Reilly.
        labels = tf.constant(
            [
                ""airplane"",
                ""automobile"",
                ""bird"",
                ""cat"",
                ""deer"",
                ""dog"",
                ""frog"",
                ""horse"",
                ""ship"",
                ""truck"",
            ],
            dtype=tf.string,
        )
        images = preprocess_fn(bytes_inputs)

        probs = m_call(**images)
        indices = tf.argmax(probs, axis=1)
        pred_source = tf.gather(params=labels, indices=indices)
        pred_confidence = tf.reduce_max(probs, axis=1)
        return {""label"": pred_source, ""confidence"": pred_confidence}

    return serving_fn


def run_fn(fn_args: tfx.components.FnArgs):
    print(fn_args)

    train_dataset = _input_fn(fn_args.train_files, batch_size=_TRAIN_BATCH_SIZE)
    eval_dataset = _input_fn(fn_args.eval_files, batch_size=_EVAL_BATCH_SIZE)

    model = _make_keras_model()
    model.fit(
        train_dataset,
        steps_per_epoch=fn_args.train_steps,
        validation_data=eval_dataset,
        validation_steps=fn_args.eval_steps,
        epochs=_EPOCHS,
    )

    _, acc = model.evaluate(eval_dataset, steps=fn_args.eval_steps)
    logging.info(f""Validation accuracy: {round(acc * 100, 2)}%"")
    # The result of the training should be saved in `fn_args.serving_model_dir`
    # directory.
    tf.saved_model.save(
        model,
        fn_args.serving_model_dir,
        signatures={""serving_default"": _model_exporter(model)},
    )!gsutil cp {_trainer_module_file} {MODULE_ROOT}/
!gsutil ls -lh {MODULE_ROOT}/os.path.join(MODULE_ROOT, _trainer_module_file)_vertex_uploader_module_file = ""vertex_uploader.py""
_vertex_deployer_module_file = ""vertex_deployer.py""%%writefile {_vertex_uploader_module_file}

import os
import tensorflow as tf

from tfx.dsl.component.experimental.decorators import component
from tfx.dsl.component.experimental.annotations import Parameter
from tfx.types.standard_artifacts import String
from google.cloud import aiplatform as vertex_ai
from tfx import v1 as tfx
from absl import logging


@component
def VertexUploader(
    project: Parameter[str],
    region: Parameter[str],
    model_display_name: Parameter[str],
    pushed_model_location: Parameter[str],
    serving_image_uri: Parameter[str],
    uploaded_model: tfx.dsl.components.OutputArtifact[String],
):

    vertex_ai.init(project=project, location=region)

    pushed_model_dir = os.path.join(
        pushed_model_location, tf.io.gfile.listdir(pushed_model_location)[-1]
    )

    logging.info(f""Model registry location: {pushed_model_dir}"")

    vertex_model = vertex_ai.Model.upload(
        display_name=model_display_name,
        artifact_uri=pushed_model_dir,
        serving_container_image_uri=serving_image_uri,
        parameters_schema_uri=None,
        instance_schema_uri=None,
        explanation_metadata=None,
        explanation_parameters=None,
    )

    uploaded_model.set_string_custom_property(
        ""model_resource_name"", str(vertex_model.resource_name)
    )
    logging.info(f""Model resource: {str(vertex_model.resource_name)}"")%%writefile {_vertex_deployer_module_file}

from tfx.dsl.component.experimental.decorators import component
from tfx.dsl.component.experimental.annotations import Parameter
from tfx.types.standard_artifacts import String
from google.cloud import aiplatform as vertex_ai
from tfx import v1 as tfx
from absl import logging


@component
def VertexDeployer(
    project: Parameter[str],
    region: Parameter[str],
    model_display_name: Parameter[str],
    deployed_model_display_name: Parameter[str],
):

    logging.info(f""Endpoint display: {deployed_model_display_name}"")
    vertex_ai.init(project=project, location=region)

    endpoints = vertex_ai.Endpoint.list(
        filter=f""display_name={deployed_model_display_name}"", order_by=""update_time""
    )

    if len(endpoints) > 0:
        logging.info(f""Endpoint {deployed_model_display_name} already exists."")
        endpoint = endpoints[-1]
    else:
        endpoint = vertex_ai.Endpoint.create(deployed_model_display_name)

    model = vertex_ai.Model.list(
        filter=f""display_name={model_display_name}"", order_by=""update_time""
    )[-1]

    endpoint = vertex_ai.Endpoint.list(
        filter=f""display_name={deployed_model_display_name}"", order_by=""update_time""
    )[-1]

    deployed_model = endpoint.deploy(
        model=model,
        # Syntax from here: https://git.io/JBQDP
        traffic_split={""0"": 100},
        machine_type=""n1-standard-4"",
        min_replica_count=1,
        max_replica_count=1,
    )

    logging.info(f""Model deployed to: {deployed_model}"")!mkdir -p ./custom_components
!touch ./custom_components/__init__.py
!cp -r {_vertex_uploader_module_file} {_vertex_deployer_module_file} custom_components!ls -lh custom_componentsDATASET_DISPLAY_NAME = ""cifar10""
VERSION = ""tfx-1-2-0""
TFX_IMAGE_URI = f""gcr.io/{GOOGLE_CLOUD_PROJECT}/{DATASET_DISPLAY_NAME}:{VERSION}""
print(f""URI of the custom image: {TFX_IMAGE_URI}"")%%writefile Dockerfile

FROM gcr.io/tfx-oss-public/tfx:1.2.0
RUN mkdir -p custom_components
COPY custom_components/* ./custom_components/
RUN pip install --upgrade google-cloud-aiplatform!gcloud builds submit --tag $TFX_IMAGE_URI . --timeout=15m --machine-type=e2-highcpu-8# Specify training worker configurations. To minimize costs we can even specify two
# different configurations: a beefier machine for the Endpoint model and slightly less
# powerful machine for the mobile model.
TRAINING_JOB_SPEC = {
    ""project"": GOOGLE_CLOUD_PROJECT,
    ""worker_pool_specs"": [
        {
            ""machine_spec"": {
                ""machine_type"": ""n1-standard-4"",
                ""accelerator_type"": ""NVIDIA_TESLA_K80"",
                ""accelerator_count"": 1,
            },
            ""replica_count"": 1,
            ""container_spec"": {
                ""image_uri"": ""gcr.io/tfx-oss-public/tfx:{}"".format(tfx.__version__),
            },
        }
    ],
}SERVING_JOB_SPEC = {
    ""endpoint_name"": PIPELINE_NAME.replace(""-"", ""_""),  # '-' is not allowed.
    ""project_id"": GOOGLE_CLOUD_PROJECT,
    ""min_replica_count"": 1,
    ""max_replica_count"": 1,
    ""machine_type"": ""n1-standard-2"",
}from datetime import datetime

TIMESTAMP = datetime.now().strftime(""%Y%m%d%H%M%S"")import tfxfrom tfx.orchestration import data_types

from tfx import v1 as tfx
from tfx.proto import example_gen_pb2, range_config_pb2
from tfx.components.example_gen import utils

from custom_components.vertex_uploader import VertexUploader
from custom_components.vertex_deployer import VertexDeployer


def _create_pipeline(
    input_config: data_types.RuntimeParameter,
    output_config: data_types.RuntimeParameter,
    pipeline_name: str,
    pipeline_root: str,
    data_root: str,
    serving_model_dir: str,
    trainer_module: str,
    project_id: str,
    region: str,
) -> tfx.dsl.Pipeline:
    """"""Creates a three component flowers pipeline with TFX.""""""
    example_gen = tfx.components.ImportExampleGen(
        input_base=data_root, input_config=input_config, output_config=output_config
    )

    # Trainer
    trainer = tfx.extensions.google_cloud_ai_platform.Trainer(
        module_file=trainer_module,
        examples=example_gen.outputs[""examples""],
        train_args=tfx.proto.TrainArgs(splits=[""train""], num_steps=50000 // 64),
        eval_args=tfx.proto.EvalArgs(splits=[""val""], num_steps=10000 // 64),
        custom_config={
            tfx.extensions.google_cloud_ai_platform.ENABLE_VERTEX_KEY: True,
            tfx.extensions.google_cloud_ai_platform.VERTEX_REGION_KEY: region,
            tfx.extensions.google_cloud_ai_platform.TRAINING_ARGS_KEY: TRAINING_JOB_SPEC,
            ""use_gpu"": True,
        },
    ).with_id(""trainer"")

    # Pushes the model to a filesystem destination.
    pushed_model_location = os.path.join(serving_model_dir, ""resnet50"")
    resnet_pusher = tfx.components.Pusher(
        model=trainer.outputs[""model""],
        push_destination=tfx.proto.PushDestination(
            filesystem=tfx.proto.PushDestination.Filesystem(
                base_directory=pushed_model_location
            )
        ),
    ).with_id(""resnet_pusher"")

    # Vertex AI upload.
    model_display_name = ""resnet_cifar_latest""
    uploader = VertexUploader(
        project=project_id,
        region=region,
        model_display_name=model_display_name,
        pushed_model_location=pushed_model_location,
        serving_image_uri=""us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-5:latest"",
    ).with_id(""vertex_uploader"")
    uploader.add_upstream_node(resnet_pusher)

    # Create an endpoint.
    deployer = VertexDeployer(
        project=project_id,
        region=region,
        model_display_name=model_display_name,
        deployed_model_display_name=model_display_name + ""_"" + TIMESTAMP,
    ).with_id(""vertex_deployer"")
    deployer.add_upstream_node(uploader)

    components = [
        example_gen,
        trainer,
        resnet_pusher,
        uploader,
        deployer,
    ]

    return tfx.dsl.Pipeline(
        pipeline_name=pipeline_name,
        pipeline_root=pipeline_root,
        components=components,
        enable_cache=True,
    )import os

PIPELINE_DEFINITION_FILE = PIPELINE_NAME + ""_pipeline.json""

# Important: We need to pass the custom Docker image URI to the
# `KubeflowV2DagRunnerConfig` to take effect.
runner = tfx.orchestration.experimental.KubeflowV2DagRunner(
    config=tfx.orchestration.experimental.KubeflowV2DagRunnerConfig(
        default_image=TFX_IMAGE_URI
    ),
    output_filename=PIPELINE_DEFINITION_FILE,
)

_ = runner.run(
    _create_pipeline(
        input_config=tfx.dsl.experimental.RuntimeParameter(
            name=""input-config"",
            default='{""input_config"": {""splits"": [{""name"":""train"", ""pattern"":""span-1/train/tfrecord""}, {""name"":""val"", ""pattern"":""span-1/test/tfrecord""}]}}',
            ptype=str,
        ),
        output_config=tfx.dsl.experimental.RuntimeParameter(
            name=""output-config"", default=""{}"", ptype=str,
        ),
        pipeline_name=PIPELINE_NAME,
        pipeline_root=PIPELINE_ROOT,
        data_root=DATA_ROOT,
        serving_model_dir=SERVING_MODEL_DIR,
        trainer_module=os.path.join(MODULE_ROOT, _trainer_module_file),
        project_id=GOOGLE_CLOUD_PROJECT,
        region=GOOGLE_CLOUD_REGION,
    )
)!gsutil cp {PIPELINE_DEFINITION_FILE} {PIPELINE_ROOT}/
!gsutil ls -lh {PIPELINE_ROOT}/from kfp.v2.google import client

pipelines_client = client.AIPlatformClient(
    project_id=GOOGLE_CLOUD_PROJECT, region=GOOGLE_CLOUD_REGION,
)import json
from tfx.orchestration import data_types

_ = pipelines_client.create_run_from_job_spec(
    PIPELINE_DEFINITION_FILE,
    enable_caching=False,
    parameter_values={
        ""input-config"": json.dumps(
            {
                ""splits"": [
                    {""name"": ""train"", ""pattern"": ""span-[12]/train/*.tfrecord""},
                    {""name"": ""val"", ""pattern"": ""span-[12]/test/*.tfrecord""},
                ]
            }
        ),
        ""output-config"": json.dumps({}),
    },
)"
0,Continuous-Adaptation-for-Machine-Learning-System-to-Data-Changes,notebooks/03_Batch_Prediction_Pipeline.ipynb,"%%capture
!pip install fastdot
!pip install tfx==1.2.0
!pip install kfp==1.6.1
!pip install -q --upgrade google-cloud-aiplatform
!pip install -q --upgrade google-cloud-storage!gcloud initfrom google.colab import auth

auth.authenticate_user()# @title
from fastdot.core import *

tfx_components = [
    ""FileListGen"",
    ""BatchPredictionGen"",
    ""PerformanceEvaluator"",
    ""SpanPreparator"",
    ""PipelineTrigger"",
]
block = ""TFX Component Workflow""

g = graph_items(seq_cluster(tfx_components, block))
g_file_list_gen_module_file = ""file_list_gen.py""%%writefile {_file_list_gen_module_file}

import tfx
from tfx.dsl.component.experimental.decorators import component
from tfx.dsl.component.experimental.annotations import Parameter
from tfx.dsl.component.experimental.annotations import OutputArtifact
from tfx.types.standard_artifacts import String
from google.cloud import storage
from absl import logging


@component
def FileListGen(
    outpath: OutputArtifact[String],
    project: Parameter[str],
    gcs_source_bucket: Parameter[str],
    gcs_source_prefix: Parameter[str] = """",
    output_filename: Parameter[str] = ""test-images.txt"",
):
    logging.info(""FileListGen started"")

    client = storage.Client(project=project)
    bucket = client.get_bucket(gcs_source_bucket)
    blobs = bucket.list_blobs(prefix=gcs_source_prefix)
    logging.info(""Successfully retrieve the file(jpg) list from GCS path"")

    f = open(output_filename, ""w"")
    for blob in blobs:
        if blob.name.split(""."")[-1] == ""jpg"":
            prefix = """"
            if gcs_source_prefix != """":
                prefix = f""/{gcs_source_prefix}""
            line = f""gs://{gcs_source_bucket}{prefix}/{blob.name}\n""
            f.write(line)
    f.close()
    logging.info(
        f""Successfully created the file list file({output_filename}) in local storage""
    )

    prefix = """"
    if gcs_source_prefix != """":
        prefix = f""{gcs_source_prefix}/""
    blob = bucket.blob(f""{prefix}{output_filename}"")
    blob.upload_from_filename(output_filename)
    logging.info(f""Successfully uploaded the file list ({prefix}{output_filename})"")

    outpath.value = gcs_source_bucket + ""/"" + prefix + output_filename_batch_pred_module_file = 'batch_prediction_vertex.py'%%writefile {_batch_pred_module_file}

from google.cloud import storage

from tfx.dsl.component.experimental.annotations import Parameter, InputArtifact
from tfx.dsl.component.experimental.decorators import component
from tfx.types import artifact_utils
from tfx.types.standard_artifacts import String
import google.cloud.aiplatform as vertex_ai

from typing import Union, Sequence
from absl import logging


@component
def BatchPredictionGen(
    gcs_source: InputArtifact[String],
    project: Parameter[str],
    location: Parameter[str],
    model_resource_name: Parameter[str],
    job_display_name: Parameter[str],
    gcs_destination: Parameter[str],
    instances_format: Parameter[str] = ""file-list"",
    machine_type: Parameter[str] = ""n1-standard-2"",
    accelerator_count: Parameter[int] = 0,
    accelerator_type: Parameter[str] = None,
    starting_replica_count: Parameter[int] = 1,
    max_replica_count: Parameter[int] = 1,
):
    storage_client = storage.Client()

    # Read GCS Source (gcs_source contains the full path of GCS object)
    # 1-1. get bucketname from gcs_source
    gcs_source_uri = gcs_source.uri.split(""//"")[1:][0].split(""/"")
    bucketname = gcs_source_uri[0]
    bucket = storage_client.get_bucket(bucketname)
    logging.info(f""bucketname: {bucketname}"")

    # 1-2. get object path without the bucketname
    objectpath = ""/"".join(gcs_source_uri[1:])

    # 1-3. read the object to get value set by OutputArtifact from FileListGen
    blob = bucket.blob(objectpath)
    logging.info(f""objectpath: {objectpath}"")

    gcs_source = f""gs://{blob.download_as_text()}""

    # Get Model
    vertex_ai.init(project=project, location=location)

    model = vertex_ai.Model.list(
        filter=f""display_name={model_resource_name}"", order_by=""update_time""
    )[-1]

    # Batch Predictions
    logging.info(""Starting batch prediction job."")
    logging.info(f""GCS path where file list is: {gcs_source}"")

    batch_prediction_job = model.batch_predict(
        job_display_name=job_display_name,
        instances_format=instances_format,
        gcs_source=gcs_source,
        gcs_destination_prefix=gcs_destination,
        machine_type=machine_type,
        accelerator_count=accelerator_count,
        accelerator_type=accelerator_type,
        starting_replica_count=starting_replica_count,
        max_replica_count=max_replica_count,
        sync=True,
    )

    logging.info(batch_prediction_job.display_name)
    logging.info(batch_prediction_job.resource_name)
    logging.info(batch_prediction_job.state)_evaluator_module_file = 'batch_pred_evaluator.py'%%writefile {_evaluator_module_file}

# Reference: https://bit.ly/vertex-batch

from tfx.dsl.component.experimental.annotations import Parameter
from tfx.dsl.component.experimental.annotations import OutputArtifact
from tfx.dsl.component.experimental.decorators import component
from tfx.types.experimental.simple_artifacts import Dataset

from absl import logging
import os
import json

@component
def PerformanceEvaluator(
    gcs_destination: Parameter[str],
    local_directory: Parameter[str],
    threshold: Parameter[float],
    trigger_pipeline: OutputArtifact[Dataset],
):
    full_gcs_results_dir = f""{gcs_destination}/{local_directory}""

    # Create missing directories.
    os.makedirs(local_directory, exist_ok=True)

    # Get the Cloud Storage paths for each result.
    os.system(f""gsutil -m cp -r {full_gcs_results_dir} {local_directory}"")

    # Get most recently modified directory.
    latest_directory = max(
        [os.path.join(local_directory, d) for d in os.listdir(local_directory)],
        key=os.path.getmtime,
    )

    # Get downloaded results in directory.
    results_files = []
    for dirpath, subdirs, files in os.walk(latest_directory):
        for file in files:
            if file.startswith(""prediction.results""):
                results_files.append(os.path.join(dirpath, file))

    # Consolidate all the results into a list.
    results = []
    for results_file in results_files:
        # Download each result.
        with open(results_file, ""r"") as file:
            results.extend([json.loads(line) for line in file.readlines()])

    # Calculate performance.
    num_correct = 0

    for result in results:
        label = os.path.basename(result[""instance""]).split(""_"")[0]
        prediction = result[""prediction""][""label""]

        if label == prediction:
            num_correct = num_correct + 1

    accuracy = num_correct / len(results)
    logging.info(f""Accuracy: {accuracy*100}%"")
    trigger_pipeline.set_string_custom_property(""result"", str(accuracy >= threshold))_span_preparator_module_file = 'span_preparator.py'%%writefile {_span_preparator_module_file}

import tfx
from tfx.dsl.component.experimental.decorators import component
from tfx.dsl.component.experimental.annotations import Parameter
from tfx.dsl.component.experimental.annotations import OutputArtifact, InputArtifact
from tfx.types.experimental.simple_artifacts import Dataset
from google.cloud import storage
from absl import logging

from datetime import datetime
import tensorflow as tf
import random
import gzip
import os

# Label-mapping.
LABEL_DICT = {
    ""airplane"": 0,
    ""automobile"": 1,
    ""bird"": 2,
    ""cat"": 3,
    ""deer"": 4,
    ""dog"": 5,
    ""frog"": 6,
    ""horse"": 7,
    ""ship"": 8,
    ""truck"": 9,
}

# Images are byte-strings.
def _bytestring_feature(list_of_bytestrings):
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=list_of_bytestrings))


# Classes would be integers.
def _int_feature(list_of_ints):
    return tf.train.Feature(int64_list=tf.train.Int64List(value=list_of_ints))


# Function that prepares a record for the tfrecord file
# a record contains the image and its label.
def to_tfrecord(img_bytes, label):
    feature = {
        ""image"": _bytestring_feature([img_bytes]),
        ""label"": _int_feature([label]),
    }
    return tf.train.Example(features=tf.train.Features(feature=feature))


def write_tfrecords(filepaths, dest_gcs, tfrecord_filename, new_span, is_train):
    # For this project, we are serializing the images in one TFRecord only.
    # For more realistic purposes, this should be sharded.
    folder = ""train"" if is_train else ""test""

    with tf.io.TFRecordWriter(tfrecord_filename) as writer:
        for path in filepaths:
            image_string = tf.io.read_file(path).numpy()
            class_name = path.split(""/"")[-1].split(""_"")[0]
            label = LABEL_DICT[class_name]

            example = to_tfrecord(image_string, label)
            writer.write(example.SerializeToString())

    # Copy over the zipped TFRecord file to the GCS Bucket and
    # remove the temporary files.
    logging.info(f""gsutil cp {tfrecord_filename} {dest_gcs}/span-{new_span}/{folder}/"")
    os.system(f""gsutil cp {tfrecord_filename} {dest_gcs}/span-{new_span}/{folder}/"")
    os.remove(tfrecord_filename)


@component
def SpanPreparator(
    is_retrain: InputArtifact[Dataset],
    gcs_source_bucket: Parameter[str],
    gcs_destination_bucket: Parameter[str],
    latest_span_id: OutputArtifact[Dataset],
    gcs_source_prefix: Parameter[str] = """",
):
    if is_retrain.get_string_custom_property(""result"") == ""False"":
        last_span_str = tf.io.gfile.glob(f""{gcs_destination_bucket}/span-*"")[-1]
        last_span = int(last_span_str.split(""-"")[-1])
        new_span = last_span + 1

        timestamp = datetime.utcnow().strftime(""%y%m%d-%H%M%S"")

        image_paths = tf.io.gfile.glob(f""gs://{gcs_source_bucket}/*.jpg"")
        logging.info(image_paths)
        random.shuffle(image_paths)

        val_split = 0.2

        split_index = int(len(image_paths) * (1 - val_split))
        training_paths = image_paths[:split_index]
        validation_paths = image_paths[split_index:]

        write_tfrecords(
            training_paths,
            gcs_destination_bucket,
            tfrecord_filename=f""new_training_data_{timestamp}.tfrecord"",
            new_span=new_span,
            is_train=True,
        )
        write_tfrecords(
            validation_paths,
            gcs_destination_bucket,
            tfrecord_filename=f""new_validation_data_{timestamp}.tfrecord"",
            new_span=new_span,
            is_train=False,
        )

        logging.info(""Removing images from batch prediction bucket."")
        os.system(
            f""gsutil mv gs://{gcs_source_bucket}/{gcs_source_prefix} gs://{gcs_source_bucket}/{gcs_source_prefix}_old""
        )
        # os.system(f""gsutil rm -rf gs://{gcs_source_bucket}/*"")

        latest_span_id.set_string_custom_property(""latest_span"", str(new_span))_pipeline_trigger_module_file = 'training_pipeline_trigger.py'%%writefile {_pipeline_trigger_module_file}

import json

from google.cloud import storage

from kfp.v2.google.client import AIPlatformClient
from tfx.dsl.component.experimental.annotations import Parameter, InputArtifact
from tfx.dsl.component.experimental.decorators import component
from tfx.types.experimental.simple_artifacts import Dataset

from absl import logging

@component
def PipelineTrigger(
	is_retrain: InputArtifact[Dataset],
  latest_span_id: InputArtifact[Dataset],
	pipeline_spec_path: Parameter[str],
	project_id: Parameter[str],
	region: Parameter[str],
):
	if is_retrain.get_string_custom_property('result') == 'False':
		# Check if the pipeline spec exists.
		storage_client = storage.Client()

		path_parts = pipeline_spec_path.replace(""gs://"", """").split(""/"")
		bucket_name = path_parts[0]
		blob_name = ""/"".join(path_parts[1:])

		bucket = storage_client.bucket(bucket_name)
		blob = storage.Blob(bucket=bucket, name=blob_name)

		if not blob.exists(storage_client):
				raise ValueError(f""{pipeline_spec_path} does not exist."")

		# Initialize Vertex AI API client and submit for pipeline execution.
		api_client = AIPlatformClient(project_id=project_id, region=region)

		# Fetch the latest span.
		latest_span = latest_span_id.get_string_custom_property('latest_span')

		# Create a training job from pipeline spec.
		response = api_client.create_run_from_job_spec(pipeline_spec_path, 
																						enable_caching=False,
																						parameter_values={
																							'input-config': json.dumps({
																								'splits': [
																									{'name': 'train', 'pattern': f'span-[{int(latest_span)-1}{latest_span}]/train/*.tfrecord'},
																									{'name': 'val', 'pattern': f'span-[{int(latest_span)-1}{latest_span}]/test/*.tfrecord'}
																								]
																							}),
																							'output-config': json.dumps({})
																						})

		logging.info(response)# This bucket will be responsible for storing the pipeline related artifacts.
GOOGLE_CLOUD_PROJECT = ""gcp-ml-172005""  # @param {type:""string""}
GOOGLE_CLOUD_REGION = ""us-central1""

GCS_BUCKET_NAME = ""cifar10-experimental-csp2""  # @param {type:""string""}

MODEL_RESOURCE_NAME = ""resnet_cifar_latest""  # @param {type: ""string""}

TEST_FILENAME = ""test-images.txt""  # @param {type:""string""}
TEST_GCS_BUCKET = ""batch-prediction-collection-3""  # @param {type:""string""}
TEST_GCS_PREFIX = """"  # @param {type: ""string""}

TRAINING_PIPELINE_SPEC = ""gs://cifar10-experimental-csp2/pipeline_root/continuous-adaptation-for-data-changes/continuous-adaptation-for-data-changes_pipeline.json""  # @param {type: ""string""}
TRAINING_DATA_PATH = ""gs://cifar10-csp-public2""  # @param {type: ""string""}

if not (GOOGLE_CLOUD_PROJECT and GOOGLE_CLOUD_REGION and GCS_BUCKET_NAME):
    from absl import logging

    logging.error(""Please set all required parameters."")PIPELINE_NAME = 'continuous-adaptation-for-data-changes-batch'

# Path to various pipeline artifact.
PIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(
    GCS_BUCKET_NAME, PIPELINE_NAME)

print('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))!mkdir -p ./custom_components
!touch ./custom_components/__init__.py
!cp -r {_file_list_gen_module_file} {_batch_pred_module_file} {_evaluator_module_file} {_span_preparator_module_file} {_pipeline_trigger_module_file} custom_components!ls -lh custom_componentsDISPLAY_NAME = ""batch-predictions-pipeline""
VERSION = ""tfx-1-2-0-34""
TFX_IMAGE_URI = f""gcr.io/{GOOGLE_CLOUD_PROJECT}/{DISPLAY_NAME}:{VERSION}""
print(f""URI of the custom image: {TFX_IMAGE_URI}"")%%writefile Dockerfile

FROM gcr.io/tfx-oss-public/tfx:1.2.0
RUN mkdir -p custom_components
COPY custom_components/* ./custom_components/
RUN pip install --upgrade google-cloud-aiplatform google-cloud-storage kfp==1.6.1!gcloud builds submit --tag $TFX_IMAGE_URI . --timeout=15m --machine-type=e2-highcpu-8from datetime import datetime

TIMESTAMP = datetime.now().strftime(""%Y%m%d%H%M%S"")from tfx.orchestration import data_types

from tfx import v1 as tfx

from tfx.orchestration.pipeline import Pipeline
from custom_components.file_list_gen import FileListGen
from custom_components.batch_prediction_vertex import BatchPredictionGen
from custom_components.batch_pred_evaluator import PerformanceEvaluator
from custom_components.span_preparator import SpanPreparator
from custom_components.training_pipeline_trigger import PipelineTrigger


def _create_pipeline(
    pipeline_name: str,
    pipeline_root: str,
    data_gcs_bucket: str,
    data_gcs_prefix: data_types.RuntimeParameter,
    batch_job_gcs: str,
    job_display_name: str,
    model_resource_name: str,
    project_id: str,
    region: str,
    threshold: float,
    data_gcs_destination: str,
    training_pipeline_spec: str,
) -> Pipeline:

    # Generate a file list for batch preditions.
    # More details on the structure of this file here:
    # https://bit.ly/3BzfHVu.
    filelist_gen = FileListGen(
        project=project_id,
        gcs_source_bucket=data_gcs_bucket,
        gcs_source_prefix=data_gcs_prefix,
    ).with_id(""filelist_gen"")

    # Submit a batch prediction job.
    batch_pred_component = BatchPredictionGen(
        project=project_id,
        location=region,
        job_display_name=job_display_name,
        model_resource_name=model_resource_name,
        gcs_source=filelist_gen.outputs[""outpath""],
        gcs_destination=f""gs://{batch_job_gcs}/results/"",
        accelerator_count=0,
        accelerator_type=None,
    ).with_id(""bulk_inferer_vertex"")
    batch_pred_component.add_upstream_node(filelist_gen)

    # Evaluate the performance of the predictions.
    # In a real-world project, this evaluation takes place
    # separately, typically with the help of domain experts.
    final_gcs_destination = f""gs://{batch_job_gcs}/results/""
    evaluator = PerformanceEvaluator(
        gcs_destination=f'gs://{final_gcs_destination.split(""/"")[2]}',
        local_directory=final_gcs_destination.split(""/"")[-2],
        threshold=threshold,
    ).with_id(""batch_prediction_evaluator"")
    evaluator.add_upstream_node(batch_pred_component)

    span_preparator = SpanPreparator(
        is_retrain=evaluator.outputs[""trigger_pipeline""],
        gcs_source_bucket=data_gcs_bucket,
        gcs_source_prefix=data_gcs_prefix,
        gcs_destination_bucket=data_gcs_destination,
    ).with_id(""span_preparator"")
    span_preparator.add_upstream_node(evaluator)

    trigger = PipelineTrigger(
        is_retrain=evaluator.outputs[""trigger_pipeline""],
        latest_span_id=span_preparator.outputs[""latest_span_id""],
        pipeline_spec_path=training_pipeline_spec,
        project_id=project_id,
        region=region,
    ).with_id(""training_pipeline_trigger"")
    trigger.add_upstream_node(span_preparator)

    components = [
        filelist_gen,
        batch_pred_component,
        evaluator,
        span_preparator,
        trigger,
    ]

    return Pipeline(
        pipeline_name=pipeline_name,
        pipeline_root=pipeline_root,
        components=components,
        enable_cache=True,
    )import os

import tfx
from tfx.orchestration import data_types
from tfx.orchestration.kubeflow.v2.kubeflow_v2_dag_runner import KubeflowV2DagRunner
from tfx.orchestration.kubeflow.v2.kubeflow_v2_dag_runner import (
    KubeflowV2DagRunnerConfig,
)

PIPELINE_DEFINITION_FILE = PIPELINE_NAME + ""_pipeline.json""
THRESHOLD = 0.9

# Important: We need to pass the custom Docker image URI to the
# `KubeflowV2DagRunnerConfig` to take effect.
runner = KubeflowV2DagRunner(
    config=KubeflowV2DagRunnerConfig(default_image=TFX_IMAGE_URI),
    output_filename=PIPELINE_DEFINITION_FILE,
)

_ = runner.run(
    _create_pipeline(
        pipeline_name=PIPELINE_NAME,
        pipeline_root=PIPELINE_ROOT,
        data_gcs_bucket=TEST_GCS_BUCKET,
        data_gcs_prefix=data_types.RuntimeParameter(
            name=""data_gcs_prefix"", default="""", ptype=str
        ),
        batch_job_gcs=GCS_BUCKET_NAME,
        job_display_name=f""{MODEL_RESOURCE_NAME}_{TIMESTAMP}"",
        project_id=GOOGLE_CLOUD_PROJECT,
        region=GOOGLE_CLOUD_REGION,
        model_resource_name=MODEL_RESOURCE_NAME,
        threshold=THRESHOLD,
        data_gcs_destination=TRAINING_DATA_PATH,
        training_pipeline_spec=TRAINING_PIPELINE_SPEC,
    )
)!gsutil cp {PIPELINE_DEFINITION_FILE} {PIPELINE_ROOT}/
!gsutil ls -lh {PIPELINE_ROOT}/from kfp.v2.google import client

pipelines_client = client.AIPlatformClient(
    project_id=GOOGLE_CLOUD_PROJECT, region=GOOGLE_CLOUD_REGION,
)

_ = pipelines_client.create_run_from_job_spec(
    PIPELINE_DEFINITION_FILE,
    enable_caching=False,
    parameter_values={""data_gcs_prefix"": ""2021-10""},
)"
0,Continuous-Adaptation-for-Machine-Learning-System-to-Data-Changes,notebooks/04_Cloud_Scheduler_Trigger.ipynb,"!pip install --upgrade -q google-cloud-scheduler!gcloud initfrom google.colab import auth

auth.authenticate_user()GOOGLE_CLOUD_PROJECT = ""gcp-ml-172005""  # @param {type:""string""}
GOOGLE_CLOUD_REGION = ""us-central1""

GCS_BUCKET_NAME = ""cifar10-experimental-csp2""  # @param {type:""string""}
PIPELINE_NAME = ""continuous-adaptation-for-data-changes-batch""  # @param {type:""string""}
PIPELINE_ROOT = ""gs://{}/pipeline_root/{}"".format(GCS_BUCKET_NAME, PIPELINE_NAME)
PIPELINE_LOCATION = f""{PIPELINE_ROOT}/{PIPELINE_NAME}_pipeline.json""

PUBSUB_TOPIC = f""trigger-{PIPELINE_NAME}""

SCHEDULER_JOB_NAME = f""scheduler-job-{PUBSUB_TOPIC}""

IMAGE_LOCATION_BUCKET = ""batch-prediction-collection-3""  # @param {type:""string""}IMAGE_LOCATION_BUCKET!gcloud pubsub topics create {PUBSUB_TOPIC}!mkdir -p cloud_function
!touch cloud_function/__init__.py_cloud_function_dep = ""cloud_function/requirements.txt""%%writefile {_cloud_function_dep}

kfp==1.6.2
google-cloud-aiplatform
google-cloud-storage!pip install google-cloud-storage_cloud_function_file = ""cloud_function/main.py""%%writefile {_cloud_function_file}

import os
import re
import json
import logging
import base64

from datetime import datetime

from kfp.v2.google.client import AIPlatformClient
from google.cloud import storage


def get_number_of_images(storage_client, bucket, latest_directory):
    blobs = storage_client.list_blobs(bucket, prefix=latest_directory)

    count = 0
    for blob in blobs:
        if blob.name.split(""."")[-1] == ""jpg"":
            count = count + 1

    return count


def is_there_enough_images(storage_client, bucket, latest_directory, threshold):
    number_of_images = get_number_of_images(storage_client, bucket, latest_directory)
    print(f""number of images = {number_of_images}"")
    return number_of_images >= threshold


def get_latest_directory(storage_client, bucket):
    blobs = storage_client.list_blobs(bucket)

    folders = list(
        set(
            [
                os.path.dirname(blob.name)
                for blob in blobs
                if bool(
                    re.match(
                        ""[1-9][0-9][0-9][0-9]-[0-1][0-9]"", os.path.dirname(blob.name)
                    )
                )
                is True
            ]
        )
    )

    folders.sort(key=lambda date: datetime.strptime(date, ""%Y-%m""))
    print(folders[0])
    return folders[0]


def trigger_pipeline(event, context):
    # Parse the environment variables.
    project = os.getenv(""PROJECT"")
    region = os.getenv(""REGION"")
    gcs_pipeline_file_location = os.getenv(""GCS_PIPELINE_FILE_LOCATION"")
    gcs_image_bucket = os.getenv(""GCS_IMAGE_BUCKET"")

    print(project)
    print(region)
    print(gcs_pipeline_file_location)
    print(gcs_image_bucket)

    threshold = 100

    # Check if the pipeline file exists in the provided GCS Bucket.
    storage_client = storage.Client()
    latest_directory = get_latest_directory(storage_client, gcs_image_bucket)

    if is_there_enough_images(
        storage_client, gcs_image_bucket, latest_directory, threshold
    ):
        path_parts = gcs_pipeline_file_location.replace(""gs://"", """").split(""/"")
        pipeline_bucket = path_parts[0]
        pipeline_blob = ""/"".join(path_parts[1:])

        pipeline_bucket = storage_client.bucket(pipeline_bucket)
        blob = storage.Blob(bucket=pipeline_bucket, name=pipeline_blob)

        if not blob.exists(storage_client):
            raise ValueError(f""{gcs_pipeline_file_location} does not exist."")

        # Initialize Vertex AI API client and submit for pipeline execution.
        api_client = AIPlatformClient(project_id=project, region=region)

        response = api_client.create_run_from_job_spec(
            job_spec_path=gcs_pipeline_file_location,
            parameter_values={""data_gcs_prefix"": latest_directory},
            enable_caching=True,
        )

        logging.info(response)!cd cloud_functionENV_VARS=f""""""\
PROJECT={GOOGLE_CLOUD_PROJECT},\
REGION={GOOGLE_CLOUD_REGION},\
GCS_PIPELINE_FILE_LOCATION={PIPELINE_LOCATION},\
GCS_IMAGE_BUCKET={IMAGE_LOCATION_BUCKET}
""""""

!echo {ENV_VARS}BUCKET = f'gs://{GCS_BUCKET_NAME}'
CLOUD_FUNCTION_NAME = f'trigger-{PIPELINE_NAME}-fn'

!gcloud functions deploy {CLOUD_FUNCTION_NAME} \
    --region={GOOGLE_CLOUD_REGION} \
    --trigger-topic={PUBSUB_TOPIC} \
    --runtime=python37 \
    --source=cloud_function\
    --entry-point=trigger_pipeline\
    --stage-bucket={BUCKET}\
    --update-env-vars={ENV_VARS}import IPython

cloud_fn_url = f""https://console.cloud.google.com/functions/details/{GOOGLE_CLOUD_REGION}/{CLOUD_FUNCTION_NAME}""
html = (
    f'See the Cloud Function details <a href=""{cloud_fn_url}"" target=""_blank"">here</a>.'
)
IPython.display.display(IPython.display.HTML(html))!gcloud scheduler jobs create pubsub $SCHEDULER_JOB_NAME --schedule ""*/3 * * * *"" --topic $PUBSUB_TOPIC --attributes name=scheduler #every hour"
0,Continuous-Adaptation-for-Machine-Learning-System-to-Data-Changes,notebooks/98_Batch_Prediction_Test.ipynb,"!pip install -q --upgrade google-cloud-aiplatform
!pip install -q --upgrade google-cloud-storage!gcloud initfrom google.colab import auth

auth.authenticate_user()GOOGLE_CLOUD_PROJECT = ""central-hangar-321813""  # @param {type:""string""}
GOOGLE_CLOUD_REGION = ""us-central1""  # @param {type:""string""}

MODEL_NAME = ""resnet_cifar_latest""  # @param {type:""string""}

TEST_FILENAME = ""test-images.txt""  # @param {type:""string""}
TEST_GCS_BUCKET = ""gs://batch-prediction-collection""  # @param {type:""string""}
TEST_LOCAL_PATH = ""Continuous-Adaptation-for-Machine-Learning-System-to-Data-Changes/notebooks/test-images""  # @param {type:""string""}!git clone https://github.com/deep-diver/Continuous-Adaptation-for-Machine-Learning-System-to-Data-Changes.gitfrom os import listdir

test_files = listdir(TEST_LOCAL_PATH)
test_filesf = open(TEST_FILENAME, ""w"")

for filename in test_files:
    f.write(f""{TEST_GCS_BUCKET}/{filename}\n"")

f.close()!cat {TEST_FILENAME}!gsutil -m cp -r {TEST_FILENAME} {TEST_GCS_BUCKET}
!gsutil -m cp -r {TEST_LOCAL_PATH}/*.jpg {TEST_GCS_BUCKET}import google.cloud.aiplatform as aiplatform
from typing import Union, Sequence


def create_batch_prediction_job_dedicated_resources_sample(
    project: str,
    location: str,
    model_resource_name: str,
    job_display_name: str,
    gcs_source: Union[str, Sequence[str]],
    gcs_destination: str,
    instances_format: str = ""file-list"",
    machine_type: str = ""n1-standard-2"",
    accelerator_count: int = 1,
    accelerator_type: str = ""NVIDIA_TESLA_K80"",
    starting_replica_count: int = 1,
    max_replica_count: int = 1,
    sync: bool = True,
):
    aiplatform.init(project=project, location=location)

    my_model = aiplatform.Model(model_resource_name)

    batch_prediction_job = my_model.batch_predict(
        job_display_name=job_display_name,
        instances_format=instances_format,
        gcs_source=gcs_source,
        gcs_destination_prefix=gcs_destination,
        machine_type=machine_type,
        accelerator_count=accelerator_count,
        accelerator_type=accelerator_type,
        starting_replica_count=starting_replica_count,
        max_replica_count=max_replica_count,
        sync=sync,
    )

    batch_prediction_job.wait()

    print(batch_prediction_job.display_name)
    print(batch_prediction_job.resource_name)
    print(batch_prediction_job.state)
    return batch_prediction_jobfrom datetime import datetime

TIMESTAMP = datetime.now().strftime(""%Y%m%d%H%M%S"")create_batch_prediction_job_dedicated_resources_sample(
    project=GOOGLE_CLOUD_PROJECT,
    location=GOOGLE_CLOUD_REGION,
    model_resource_name=""2008244793993330688"",
    job_display_name=f""{MODEL_NAME}-{TIMESTAMP}"",
    gcs_source=[f""{TEST_GCS_BUCKET}/{TEST_FILENAME}""],
    gcs_destination=f""{TEST_GCS_BUCKET}/results/"",
    accelerator_type=None,
    accelerator_count=None,
)import os
import json

RESULTS_DIRECTORY = ""results""
RESULTS_DIRECTORY_FULL = f'{TEST_GCS_BUCKET}/{RESULTS_DIRECTORY}'

# Create missing directories
os.makedirs(RESULTS_DIRECTORY, exist_ok=True)

# Get the Cloud Storage paths for each result
!gsutil -m cp -r $RESULTS_DIRECTORY_FULL $RESULTS_DIRECTORY

# Get most recently modified directory
latest_directory = max(
    [
        os.path.join(RESULTS_DIRECTORY, d)
        for d in os.listdir(RESULTS_DIRECTORY)
    ],
    key=os.path.getmtime,
)

# Get downloaded results in directory
results_files = []
for dirpath, subdirs, files in os.walk(latest_directory):
    for file in files:
        if file.startswith(""prediction.results""):
            results_files.append(os.path.join(dirpath, file))

# Consolidate all the results into a list
results = []
for results_file in results_files:
    # Download each result
    with open(results_file, ""r"") as file:
        results.extend([json.loads(line) for line in file.readlines()])resultsnum_correct = 0

for result in results:
    label = os.path.basename(result[""instance""]).split(""_"")[0]
    prediction = result[""prediction""][""label""]

    print(f""label({label})/prediction({prediction})"")
    if label == prediction:
        num_correct = num_correct + 1

print()
print(f""number of results: {len(results)}"")
print(f""number of correct: {num_correct}"")
print(f""Accuracy: {num_correct/len(results)}"")"
0,mlops-hf-tf-vision-models,advanced_part1/kubeflow_runner.py,"from absl import logging

from tfx import v1 as tfx
from tfx.orchestration.kubeflow.v2 import kubeflow_v2_dag_runner as runner

from pipeline import configs
from pipeline import kubeflow_pipeline


def run():
    runner_config = runner.KubeflowV2DagRunnerConfig(
        default_image=configs.PIPELINE_IMAGE
    )

    runner.KubeflowV2DagRunner(
        config=runner_config,
        output_filename=configs.PIPELINE_NAME + ""_pipeline.json"",
    ).run(
        kubeflow_pipeline.create_pipeline(
            pipeline_name=configs.PIPELINE_NAME,
            pipeline_root=configs.PIPELINE_ROOT,
            data_path=configs.DATA_PATH,
            schema_path=configs.SCHEMA_PATH,
            modules={
                ""training_fn"": configs.TRAINING_FN,
                ""preprocessing_fn"": configs.PREPROCESSING_FN,
            },
            eval_configs=configs.EVAL_CONFIGS,
            ai_platform_training_args=configs.GCP_AI_PLATFORM_TRAINING_ARGS,
            ai_platform_serving_args=configs.GCP_AI_PLATFORM_SERVING_ARGS,
            example_gen_beam_args=configs.EXAMPLE_GEN_BEAM_ARGS,
            transform_beam_args=configs.TRANSFORM_BEAM_ARGS,
        )
    )


if __name__ == ""__main__"":
    logging.set_verbosity(logging.INFO)
    run()
"
0,mlops-hf-tf-vision-models,advanced_part1/local_runner.py,"import os
from absl import logging

from tfx import v1 as tfx
from pipeline import configs
from pipeline import local_pipeline

OUTPUT_DIR = "".""

PIPELINE_ROOT = os.path.join(OUTPUT_DIR, ""tfx_pipeline_output"", configs.PIPELINE_NAME)
METADATA_PATH = os.path.join(
    OUTPUT_DIR, ""tfx_metadata"", configs.PIPELINE_NAME, ""metadata.db""
)
SERVING_MODEL_DIR = os.path.join(PIPELINE_ROOT, ""serving_model"")


def run():
    tfx.orchestration.LocalDagRunner().run(
        local_pipeline.create_pipeline(
            pipeline_name=configs.PIPELINE_NAME,
            pipeline_root=PIPELINE_ROOT,
            data_path=configs.DATA_PATH,
            schema_path=configs.SCHEMA_PATH,
            modules={
                ""training_fn"": configs.TRAINING_FN,
                ""preprocessing_fn"": configs.PREPROCESSING_FN,
            },
            eval_configs=configs.EVAL_CONFIGS,
            serving_model_dir=SERVING_MODEL_DIR,
            metadata_connection_config=tfx.orchestration.metadata.sqlite_metadata_connection_config(
                METADATA_PATH
            ),
        )
    )


if __name__ == ""__main__"":
    logging.set_verbosity(logging.INFO)
    run()
"
0,mlops-hf-tf-vision-models,advanced_part2/kubeflow_runner.py,"from absl import logging

from tfx import v1 as tfx
from tfx.orchestration.kubeflow.v2 import kubeflow_v2_dag_runner as runner
from tfx.proto import tuner_pb2

from pipeline import configs
from pipeline import kubeflow_pipeline


def run():
    runner_config = runner.KubeflowV2DagRunnerConfig(
        default_image=configs.PIPELINE_IMAGE
    )

    runner.KubeflowV2DagRunner(
        config=runner_config,
        output_filename=configs.PIPELINE_NAME + ""_pipeline.json"",
    ).run(
        kubeflow_pipeline.create_pipeline(
            pipeline_name=configs.PIPELINE_NAME,
            pipeline_root=configs.PIPELINE_ROOT,
            data_path=configs.DATA_PATH,
            schema_path=configs.SCHEMA_PATH,
            modules={
                ""training_fn"": configs.TRAINING_FN,
                ""preprocessing_fn"": configs.PREPROCESSING_FN,
                ""tuner_fn"": configs.TUNER_FN,
            },
            eval_configs=configs.EVAL_CONFIGS,
            ai_platform_training_args=configs.GCP_AI_PLATFORM_TRAINING_ARGS,
            ai_platform_tuner_args=configs.GCP_AI_PLATFORM_TUNER_ARGS,
            tuner_args=tuner_pb2.TuneArgs(
                num_parallel_trials=configs.NUM_PARALLEL_TRIALS
            ),
            ai_platform_serving_args=configs.GCP_AI_PLATFORM_SERVING_ARGS,
            example_gen_beam_args=configs.EXAMPLE_GEN_BEAM_ARGS,
            transform_beam_args=configs.TRANSFORM_BEAM_ARGS,
        )
    )


if __name__ == ""__main__"":
    logging.set_verbosity(logging.INFO)
    run()
"
0,mlops-hf-tf-vision-models,advanced_part2/local_runner.py,"import os
from absl import logging

from tfx import v1 as tfx
from pipeline import configs
from pipeline import local_pipeline

OUTPUT_DIR = "".""

PIPELINE_ROOT = os.path.join(OUTPUT_DIR, ""tfx_pipeline_output"", configs.PIPELINE_NAME)
METADATA_PATH = os.path.join(
    OUTPUT_DIR, ""tfx_metadata"", configs.PIPELINE_NAME, ""metadata.db""
)
SERVING_MODEL_DIR = os.path.join(PIPELINE_ROOT, ""serving_model"")


def run():
    tfx.orchestration.LocalDagRunner().run(
        local_pipeline.create_pipeline(
            pipeline_name=configs.PIPELINE_NAME,
            pipeline_root=PIPELINE_ROOT,
            data_path=configs.DATA_PATH,
            schema_path=configs.SCHEMA_PATH,
            modules={
                ""training_fn"": configs.TRAINING_FN,
                ""preprocessing_fn"": configs.PREPROCESSING_FN,
                ""tuner_fn"": configs.TUNER_FN,
            },
            hyperparameters=configs.HYPER_PARAMETERS,
            eval_configs=configs.EVAL_CONFIGS,
            serving_model_dir=SERVING_MODEL_DIR,
            metadata_connection_config=tfx.orchestration.metadata.sqlite_metadata_connection_config(
                METADATA_PATH
            ),
        )
    )


if __name__ == ""__main__"":
    logging.set_verbosity(logging.INFO)
    run()
"
0,mlops-hf-tf-vision-models,basic/kubeflow_runner.py,"from absl import logging

from tfx import v1 as tfx
from tfx.orchestration.kubeflow.v2 import kubeflow_v2_dag_runner as runner

from pipeline import configs
from pipeline import kubeflow_pipeline


def run():
    runner_config = runner.KubeflowV2DagRunnerConfig(
        default_image=configs.PIPELINE_IMAGE
    )

    runner.KubeflowV2DagRunner(
        config=runner_config,
        output_filename=configs.PIPELINE_NAME + ""_pipeline.json"",
    ).run(
        kubeflow_pipeline.create_pipeline(
            pipeline_name=configs.PIPELINE_NAME,
            pipeline_root=configs.PIPELINE_ROOT,
            data_path=configs.DATA_PATH,
            modules={
                ""training_fn"": configs.TRAINING_FN,
            },
            ai_platform_training_args=configs.GCP_AI_PLATFORM_TRAINING_ARGS,
            ai_platform_serving_args=configs.GCP_AI_PLATFORM_SERVING_ARGS,
            example_gen_beam_args=configs.EXAMPLE_GEN_BEAM_ARGS,
        )
    )


if __name__ == ""__main__"":
    logging.set_verbosity(logging.INFO)
    run()
"
0,mlops-hf-tf-vision-models,basic/local_runner.py,"import os
from absl import logging

from tfx import v1 as tfx
from pipeline import configs
from pipeline import local_pipeline

OUTPUT_DIR = "".""

PIPELINE_ROOT = os.path.join(OUTPUT_DIR, ""tfx_pipeline_output"", configs.PIPELINE_NAME)
METADATA_PATH = os.path.join(
    OUTPUT_DIR, ""tfx_metadata"", configs.PIPELINE_NAME, ""metadata.db""
)
SERVING_MODEL_DIR = os.path.join(PIPELINE_ROOT, ""serving_model"")


def run():
    tfx.orchestration.LocalDagRunner().run(
        local_pipeline.create_pipeline(
            pipeline_name=configs.PIPELINE_NAME,
            pipeline_root=PIPELINE_ROOT,
            data_path=configs.DATA_PATH,
            modules={
                ""training_fn"": configs.TRAINING_FN,
            },
            serving_model_dir=SERVING_MODEL_DIR,
            metadata_connection_config=tfx.orchestration.metadata.sqlite_metadata_connection_config(
                METADATA_PATH
            ),
        )
    )


if __name__ == ""__main__"":
    logging.set_verbosity(logging.INFO)
    run()
"
0,mlops-hf-tf-vision-models,dataset/create_tfrecords.py,"""""""
Script to generate TFRecord shards from the Sidewalks dataset as shown in this
blog post: https://huggingface.co/blog/fine-tune-segformer.

The recommended way to obtain TFRecord shards is via an Apache Beam
Pipeline with an execution runner like Dataflow. Example:
https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/05_create_dataset/jpeg_to_tfrecord.py.

Usage:
    python create_tfrecords --batch_size 16
    python create_tfrecords --resize 256 # without --resize flag, no resizing is applied

References:

    * https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/05_create_dataset/05_split_tfrecord.ipynb
    * https://www.tensorflow.org/tutorials/images/segmentation
""""""

import argparse
import math
import os
from typing import Tuple

import datasets
import numpy as np
import tensorflow as tf
import tqdm
from PIL import Image

RESOLUTION = 256


def load_beans_dataset(args):
    hf_dataset_identifier = ""beans""
    ds = datasets.load_dataset(hf_dataset_identifier)

    ds = ds.shuffle(seed=1)
    ds = ds[""train""].train_test_split(test_size=args.split, seed=args.seed)
    train_ds = ds[""train""]
    val_ds = ds[""test""]

    return train_ds, val_ds


def resize_img(
    image: tf.Tensor, label: tf.Tensor, resize: int
) -> Tuple[tf.Tensor, tf.Tensor]:
    image = tf.image.resize(image, (resize, resize))
    return image, label


def process_image(
    image: Image, label: Image, resize: int
) -> Tuple[tf.Tensor, tf.Tensor]:
    image = np.array(image)
    label = np.array(label)

    image = tf.convert_to_tensor(image)
    label = tf.convert_to_tensor(label)

    if resize:
        image, label = resize_img(image, label, resize)

    return image, label


def _int64_feature(value):
    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))


def _float_feature(value):
    return tf.train.Feature(float_list=tf.train.FloatList(value=value))


def create_tfrecord(image: Image, label: Image, resize: int):
    image, label = process_image(image, label, resize)
    image_dims = image.shape

    image = tf.reshape(image, [-1])  # flatten to 1D array
    label = tf.reshape(label, [-1])  # flatten to 1D array

    return tf.train.Example(
        features=tf.train.Features(
            feature={
                ""image"": _float_feature(image.numpy()),
                ""image_shape"": _int64_feature(
                    [image_dims[0], image_dims[1], image_dims[2]]
                ),
                ""label"": _int64_feature(label.numpy()),
            }
        )
    ).SerializeToString()


def write_tfrecords(root_dir, dataset, split, batch_size, resize):
    print(f""Preparing TFRecords for split: {split}."")

    for step in tqdm.tnrange(int(math.ceil(len(dataset) / batch_size))):
        temp_ds = dataset[step * batch_size : (step + 1) * batch_size]
        shard_size = len(temp_ds[""image""])
        filename = os.path.join(
            root_dir, ""{}-{:02d}-{}.tfrec"".format(split, step, shard_size)
        )

        with tf.io.TFRecordWriter(filename) as out_file:
            for i in range(shard_size):
                image = temp_ds[""image""][i]
                label = temp_ds[""labels""][i]
                example = create_tfrecord(image, label, resize)
                out_file.write(example)
            print(""Wrote file {} containing {} records"".format(filename, shard_size))


def main(args):
    train_ds, val_ds = load_beans_dataset(args)
    print(""Dataset loaded from HF."")

    if not os.path.exists(args.root_tfrecord_dir):
        os.makedirs(args.root_tfrecord_dir, exist_ok=True)

    print(args.resize)

    write_tfrecords(
        args.root_tfrecord_dir, train_ds, ""train"", args.batch_size, args.resize
    )
    write_tfrecords(args.root_tfrecord_dir, val_ds, ""val"", args.batch_size, args.resize)


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        ""--split"", help=""Train and test split."", default=0.2, type=float
    )
    parser.add_argument(
        ""--seed"",
        help=""Seed to be used while performing train-test splits."",
        default=2022,
        type=int,
    )
    parser.add_argument(
        ""--root_tfrecord_dir"",
        help=""Root directory where the TFRecord shards will be serialized."",
        default=""beans-tfrecords"",
        type=str,
    )
    parser.add_argument(
        ""--batch_size"",
        help=""Number of samples to process in a batch before serializing a single TFRecord shard."",
        default=32,
        type=int,
    )
    parser.add_argument(
        ""--resize"",
        help=""Width and height size the image will be resized to. No resizing will be applied when this isn't set."",
        type=int,
    )

    args = parser.parse_args()
    return args


if __name__ == ""__main__"":
    args = parse_args()
    main(args)
"
0,mlops-hf-tf-vision-models,hf_integration/kubeflow_runner.py,"from absl import logging

from tfx import v1 as tfx
from tfx.orchestration.kubeflow.v2 import kubeflow_v2_dag_runner as runner
from tfx.proto import tuner_pb2

from pipeline import configs
from pipeline import kubeflow_pipeline


def run():
    runner_config = runner.KubeflowV2DagRunnerConfig(
        default_image=configs.PIPELINE_IMAGE
    )

    runner.KubeflowV2DagRunner(
        config=runner_config,
        output_filename=configs.PIPELINE_NAME + ""_pipeline.json"",
    ).run(
        kubeflow_pipeline.create_pipeline(
            pipeline_name=configs.PIPELINE_NAME,
            pipeline_root=configs.PIPELINE_ROOT,
            data_path=configs.DATA_PATH,
            schema_path=configs.SCHEMA_PATH,
            modules={
                ""training_fn"": configs.TRAINING_FN,
                ""preprocessing_fn"": configs.PREPROCESSING_FN,
                ""tuner_fn"": configs.TUNER_FN,
            },
            eval_configs=configs.EVAL_CONFIGS,
            ai_platform_training_args=configs.GCP_AI_PLATFORM_TRAINING_ARGS,
            ai_platform_tuner_args=configs.GCP_AI_PLATFORM_TUNER_ARGS,
            tuner_args=tuner_pb2.TuneArgs(
                num_parallel_trials=configs.NUM_PARALLEL_TRIALS
            ),
            ai_platform_serving_args=configs.GCP_AI_PLATFORM_SERVING_ARGS,
            example_gen_beam_args=configs.EXAMPLE_GEN_BEAM_ARGS,
            transform_beam_args=configs.TRANSFORM_BEAM_ARGS,
            hf_pusher_args=configs.HF_PUSHER_ARGS,
        )
    )


if __name__ == ""__main__"":
    logging.set_verbosity(logging.INFO)
    run()
"
0,mlops-hf-tf-vision-models,hf_integration/local_runner.py,"import os
from absl import logging

from tfx import v1 as tfx
from pipeline import configs
from pipeline import local_pipeline

OUTPUT_DIR = "".""

PIPELINE_ROOT = os.path.join(OUTPUT_DIR, ""tfx_pipeline_output"", configs.PIPELINE_NAME)
METADATA_PATH = os.path.join(
    OUTPUT_DIR, ""tfx_metadata"", configs.PIPELINE_NAME, ""metadata.db""
)
SERVING_MODEL_DIR = os.path.join(PIPELINE_ROOT, ""serving_model"")


def run():
    tfx.orchestration.LocalDagRunner().run(
        local_pipeline.create_pipeline(
            pipeline_name=configs.PIPELINE_NAME,
            pipeline_root=PIPELINE_ROOT,
            data_path=configs.DATA_PATH,
            schema_path=configs.SCHEMA_PATH,
            modules={
                ""training_fn"": configs.TRAINING_FN,
                ""preprocessing_fn"": configs.PREPROCESSING_FN,
                ""tuner_fn"": configs.TUNER_FN,
            },
            hyperparameters=configs.HYPER_PARAMETERS,
            eval_configs=configs.EVAL_CONFIGS,
            serving_model_dir=SERVING_MODEL_DIR,
            metadata_connection_config=tfx.orchestration.metadata.sqlite_metadata_connection_config(
                METADATA_PATH
            ),
        )
    )


if __name__ == ""__main__"":
    logging.set_verbosity(logging.INFO)
    run()
"
0,mlops-hf-tf-vision-models,intermediate/kubeflow_runner.py,"from absl import logging

from tfx import v1 as tfx
from tfx.orchestration.kubeflow.v2 import kubeflow_v2_dag_runner as runner

from pipeline import configs
from pipeline import kubeflow_pipeline


def run():
    runner_config = runner.KubeflowV2DagRunnerConfig(
        default_image=configs.PIPELINE_IMAGE
    )

    runner.KubeflowV2DagRunner(
        config=runner_config,
        output_filename=configs.PIPELINE_NAME + ""_pipeline.json"",
    ).run(
        kubeflow_pipeline.create_pipeline(
            pipeline_name=configs.PIPELINE_NAME,
            pipeline_root=configs.PIPELINE_ROOT,
            data_path=configs.DATA_PATH,
            schema_path=configs.SCHEMA_PATH,
            modules={
                ""training_fn"": configs.TRAINING_FN,
                ""preprocessing_fn"": configs.PREPROCESSING_FN,
            },
            ai_platform_training_args=configs.GCP_AI_PLATFORM_TRAINING_ARGS,
            ai_platform_serving_args=configs.GCP_AI_PLATFORM_SERVING_ARGS,
            example_gen_beam_args=configs.EXAMPLE_GEN_BEAM_ARGS,
            transform_beam_args=configs.TRANSFORM_BEAM_ARGS,
        )
    )


if __name__ == ""__main__"":
    logging.set_verbosity(logging.INFO)
    run()
"
0,mlops-hf-tf-vision-models,intermediate/local_runner.py,"import os
from absl import logging

from tfx import v1 as tfx
from pipeline import configs
from pipeline import local_pipeline

OUTPUT_DIR = "".""

PIPELINE_ROOT = os.path.join(OUTPUT_DIR, ""tfx_pipeline_output"", configs.PIPELINE_NAME)
METADATA_PATH = os.path.join(
    OUTPUT_DIR, ""tfx_metadata"", configs.PIPELINE_NAME, ""metadata.db""
)
SERVING_MODEL_DIR = os.path.join(PIPELINE_ROOT, ""serving_model"")


def run():
    tfx.orchestration.LocalDagRunner().run(
        local_pipeline.create_pipeline(
            pipeline_name=configs.PIPELINE_NAME,
            pipeline_root=PIPELINE_ROOT,
            data_path=configs.DATA_PATH,
            schema_path=configs.SCHEMA_PATH,
            modules={
                ""training_fn"": configs.TRAINING_FN,
                ""preprocessing_fn"": configs.PREPROCESSING_FN,
            },
            serving_model_dir=SERVING_MODEL_DIR,
            metadata_connection_config=tfx.orchestration.metadata.sqlite_metadata_connection_config(
                METADATA_PATH
            ),
        )
    )


if __name__ == ""__main__"":
    logging.set_verbosity(logging.INFO)
    run()
"
0,mlops-hf-tf-vision-models,notebooks/advanced_part1.ipynb,"!pip -q install tfx
!pip -q install transformers
!pip -q install pillow!pip install tensorflow==2.8
!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2data_path = ""gs://beans-lowres/tfrecords""
local_data_path = ""data""

model_file = ""modules/model.py""
model_fn = ""modules.model.run_fn""

proprocessing_file = ""modules/preprocessing.py""
preprocessing_fn = ""modules.preprocessing.preprocessing_fn""

schema_file = ""schema.pbtxt""!mkdir {local_data_path}
!gsutil cp {data_path}/*-00-*.tfrec {local_data_path}/import tfx
tfx.__version__from tfx import v1 as tfx
from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext
from tfx.components import ImportExampleGen
from tfx.components import StatisticsGen
from tfx.components import SchemaGen
from tfx.components import ExampleValidator
from tfx.components import Transform
from tfx.components import Trainer
from tfx.components import Evaluator
from tfx.components import Pusher
from tfx.proto import example_gen_pb2

from tfx.types import Channel
from tfx.types.standard_artifacts import Model
from tfx.types.standard_artifacts import ModelBlessing
from tfx.dsl.components.common import resolver
from tfx.dsl.experimental.latest_blessed_model_resolver import LatestBlessedModelResolver

import tensorflow_model_analysis as tfmacontext = InteractiveContext()input_config = example_gen_pb2.Input(
    splits=[
        example_gen_pb2.Input.Split(name=""train"", pattern=""train-*.tfrec""),
        example_gen_pb2.Input.Split(name=""eval"", pattern=""val-*.tfrec""),
    ]
)
example_gen = ImportExampleGen(
    input_base=local_data_path, 
    input_config=input_config
)

context.run(example_gen)statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])
context.run(statistics_gen)context.show(statistics_gen.outputs['statistics'])%%writefile {schema_file}

feature {
  name: ""image""
  type: FLOAT

  presence {
    min_fraction: 1.0
  }

  float_domain {
    min: 0
    max: 255
  }

  shape {
    dim { size: 256 }
    dim { size: 256 }
    dim { size: 3 }
  }
}

feature {
  name: ""image_shape""
  type: INT

  presence {
    min_fraction: 1.0
  }

  shape {
    dim { size: 3 }
  }
}

feature {
  name: ""label""
  type: INT

  presence {
    min_fraction: 1.0
  }

  int_domain {
    min: 0
    max: 2
  }

  shape {
    dim { size: 1 }
  }  
}schema_gen = tfx.components.ImportSchemaGen(
    schema_file=schema_file)

context.run(schema_gen)example_validator = ExampleValidator(
  statistics=statistics_gen.outputs['statistics'],
  schema=schema_gen.outputs['schema']   
)

context.run(example_validator)context.show(example_validator.outputs['anomalies'])!mkdir modules%%writefile {proprocessing_file}

import tensorflow as tf

IMAGE_KEY = ""image""
LABEL_KEY = ""label""

MODEL_INPUT_IMAGE_KEY = ""pixel_values""
MODEL_INPUT_LABEL_KEY = ""labels""

INPUT_IMG_SIZE = 224

def preprocessing_fn(inputs):
    """"""tf.transform's callback function for preprocessing inputs.
    Args:
      inputs: map from feature keys to raw not-yet-transformed features.
    Returns:
      Map from string feature key to transformed feature operations.
    """"""
    # print(inputs)
    outputs = {}

    inputs[IMAGE_KEY] = tf.image.resize(
        inputs[IMAGE_KEY], [INPUT_IMG_SIZE, INPUT_IMG_SIZE]
    )

    inputs[IMAGE_KEY] = inputs[IMAGE_KEY] / 255.0
    inputs[IMAGE_KEY] = tf.transpose(inputs[IMAGE_KEY], [0, 3, 1, 2])

    outputs[MODEL_INPUT_IMAGE_KEY] = inputs[IMAGE_KEY]
    outputs[MODEL_INPUT_LABEL_KEY] = inputs[LABEL_KEY]

    return outputstransform = Transform(
    examples=example_gen.outputs['examples'],
    schema=schema_gen.outputs['schema'],
    preprocessing_fn=preprocessing_fn)context.run(transform)%%writefile {model_file}

from typing import List, Dict, Tuple

import absl
import tensorflow as tf
import tensorflow_transform as tft
from transformers import ViTFeatureExtractor, TFViTForImageClassification
from tfx.components.trainer.fn_args_utils import FnArgs

from tfx_bsl.tfxio import dataset_options
from tfx.components.trainer.fn_args_utils import DataAccessor

feature_extractor = ViTFeatureExtractor()

_TRAIN_LENGTH = 128
_EVAL_LENGTH = 128
_TRAIN_BATCH_SIZE = 8
_EVAL_BATCH_SIZE = 8

_EPOCHS = 1

_LABELS = ['angular_leaf_spot', 'bean_rust', 'healthy']

_CONCRETE_INPUT = ""pixel_values""

_MODEL_INPUT_LABEL_KEY = ""labels""

def INFO(text: str):
    absl.logging.info(text)

def _normalize_img(
    img, mean=feature_extractor.image_mean, std=feature_extractor.image_std
):
    img = img / 255
    mean = tf.constant(mean)
    std = tf.constant(std)
    return (img - mean) / std


def _preprocess_serving(string_input):
    decoded_input = tf.io.decode_base64(string_input)
    decoded = tf.io.decode_jpeg(decoded_input, channels=3)
    resized = tf.image.resize(decoded, size=(224, 224))
    normalized = _normalize_img(resized)
    normalized = tf.transpose(
        normalized, (2, 0, 1)
    )  # Since HF models are channel-first.
    return normalized


@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])
def _preprocess_fn(string_input):
    decoded_images = tf.map_fn(
        _preprocess_serving, string_input, dtype=tf.float32, back_prop=False
    )
    return {_CONCRETE_INPUT: decoded_images}


def _model_exporter(model: tf.keras.Model):
    m_call = tf.function(model.call).get_concrete_function(
        tf.TensorSpec(
            shape=[None, 3, 224, 224], dtype=tf.float32, name=_CONCRETE_INPUT
        )
    )

    @tf.function(input_signature=[tf.TensorSpec([None], tf.string)])
    def serving_fn(string_input):
        labels = tf.constant(list(model.config.id2label.values()), dtype=tf.string)
        images = _preprocess_fn(string_input)

        predictions = m_call(**images)
        indices = tf.argmax(predictions.logits, axis=1)
        pred_source = tf.gather(params=labels, indices=indices)
        probs = tf.nn.softmax(predictions.logits, axis=1)
        pred_confidence = tf.reduce_max(probs, axis=1)
        return {""label"": pred_source, ""confidence"": pred_confidence}

    return serving_fn

###

def _transform_features_signature(
    model: tf.keras.Model, tf_transform_output: tft.TFTransformOutput
):
    """"""
    transform_features_signature simply returns a function that transforms
    any data of the type of tf.Example which is denoted as the type of sta
    ndard_artifacts.Examples in TFX. The purpose of this function is to ap
    ply Transform Graph obtained from Transform component to the data prod
    uced by ImportExampleGen. This function will be used in the Evaluator
    component, so the raw evaluation inputs from ImportExampleGen can be a
    pporiately transformed that the model could understand.
    """"""

    # basically, what Transform component emits is a SavedModel that knows
    # how to transform data. transform_features_layer() simply returns the
    # layer from the Transform.
    model.tft_layer = tf_transform_output.transform_features_layer()

    @tf.function(
        input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string, name=""examples"")]
    )
    def serve_tf_examples_fn(serialized_tf_examples):
        """"""
        raw_feature_spec returns a set of feature maps(dict) for the input
        TFRecords based on the knowledge that Transform component has lear
        ned(learn doesn't mean training here). By using this information,
        the raw data from ImportExampleGen could be parsed with tf.io.parse
        _example utility function.
        Then, it is passed to the model.tft_layer, so the final output we
        get is the transformed data of the raw input.
        """"""
        feature_spec = tf_transform_output.raw_feature_spec()
        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)

        transformed_features = model.tft_layer(parsed_features)

        return transformed_features

    return serve_tf_examples_fn


def _tf_examples_serving_signature(model, tf_transform_output):
    """"""
    tf_examples_serving_signature simply returns a function that performs
    data transformation(preprocessing) and model prediction in a sequential
    manner. How data transformation is done is idential to the process of
    transform_features_signature function.
    """"""

    model.tft_layer = tf_transform_output.transform_features_layer()

    @tf.function(
        input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string, name=""examples"")]
    )
    def serve_tf_examples_fn(
        serialized_tf_example: tf.Tensor,
    ) -> Dict[str, tf.Tensor]:
        raw_feature_spec = tf_transform_output.raw_feature_spec()
        raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)

        transformed_features = model.tft_layer(raw_features)
        logits = model(transformed_features).logits

        return {_MODEL_INPUT_LABEL_KEY: logits}

    return serve_tf_examples_fn

###

def _input_fn(
    file_pattern: List[str],
    data_accessor: DataAccessor,
    tf_transform_output: tft.TFTransformOutput,
    is_train: bool = False,
    batch_size: int = 32,
) -> tf.data.Dataset:
    INFO(f""Reading data from: {file_pattern}"")

    dataset = data_accessor.tf_dataset_factory(
        file_pattern,
        dataset_options.TensorFlowDatasetOptions(
            batch_size=batch_size, label_key=_MODEL_INPUT_LABEL_KEY
        ),
        tf_transform_output.transformed_metadata.schema,
    )

    return dataset

def _build_model():
  id2label={str(i): c for i, c in enumerate(_LABELS)}
  label2id={c: str(i) for i, c in enumerate(_LABELS)}

  model = TFViTForImageClassification.from_pretrained(
    ""google/vit-base-patch16-224-in21k"",
    num_labels=len(_LABELS),
    label2id=label2id,
    id2label=id2label,
  )

  model.layers[0].trainable=False

  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
  model.compile(optimizer='adam', loss=loss, metrics=[""accuracy""])

  return model  

def run_fn(fn_args: FnArgs):
    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)

    train_dataset = _input_fn(
        fn_args.train_files,
        fn_args.data_accessor,
        tf_transform_output,
        is_train=True,
        batch_size=_TRAIN_BATCH_SIZE,
    )

    eval_dataset = _input_fn(
        fn_args.eval_files,
        fn_args.data_accessor,
        tf_transform_output,
        is_train=False,
        batch_size=_EVAL_BATCH_SIZE,
    )

    model = _build_model()

    model.fit(
        train_dataset,
        steps_per_epoch=_TRAIN_LENGTH // _TRAIN_BATCH_SIZE,
        validation_data=eval_dataset,
        validation_steps=_EVAL_LENGTH // _EVAL_BATCH_SIZE,
        epochs=_EPOCHS,
    )

    model.save(
        fn_args.serving_model_dir, 
        save_format=""tf"", 
        signatures={
            ""serving_default"": _model_exporter(model),
            ""transform_features"": _transform_features_signature(
                model, tf_transform_output
            ),
            ""from_examples"": _tf_examples_serving_signature(model, tf_transform_output),
        },
    )trainer = Trainer(
    run_fn=model_fn,
    transformed_examples=transform.outputs[""transformed_examples""],
    transform_graph=transform.outputs[""transform_graph""],
    schema=schema_gen.outputs[""schema""],
)context.run(trainer)model_resolver = resolver.Resolver(
    strategy_class=LatestBlessedModelResolver,
    model=Channel(type=Model),
    model_blessing=Channel(type=ModelBlessing),
).with_id(""latest_blessed_model_resolver"")context.run(model_resolver)eval_configs = tfma.EvalConfig(
    model_specs=[
        tfma.ModelSpec(
            signature_name=""from_examples"",
            preprocessing_function_names=[""transform_features""],
            label_key=""labels"",
            prediction_key=""labels"",
        )
    ],
    slicing_specs=[tfma.SlicingSpec()],
    metrics_specs=[
        tfma.MetricsSpec(
            metrics=[
                tfma.MetricConfig(
                    class_name=""SparseCategoricalAccuracy"",
                    threshold=tfma.MetricThreshold(
                        value_threshold=tfma.GenericValueThreshold(
                            lower_bound={""value"": 0.55}
                        ),
                        # Change threshold will be ignored if there is no
                        # baseline model resolved from MLMD (first run).
                        change_threshold=tfma.GenericChangeThreshold(
                            direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                            absolute={""value"": -1e-3},
                        ),
                    ),
                )
            ]
        )
    ],
)


evaluator = Evaluator(
    examples=example_gen.outputs[""examples""],
    model=trainer.outputs[""model""],
    baseline_model=model_resolver.outputs[""model""],
    eval_config=eval_configs,
)context.run(evaluator)"
0,mlops-hf-tf-vision-models,notebooks/advanced_part2.ipynb,"!pip -q install tfx
!pip install -q keras_tuner
!pip -q install transformers
!pip -q install pillow!pip install tensorflow==2.8
!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2data_path = ""gs://beans-lowres/tfrecords""
local_data_path = ""data""

model_file = ""modules/model.py""
model_fn = ""modules.model.run_fn""
tuner_fn = ""modules.model.tuner_fn""

proprocessing_file = ""modules/preprocessing.py""
preprocessing_fn = ""modules.preprocessing.preprocessing_fn""

schema_file = ""schema.pbtxt""!mkdir {local_data_path}
!gsutil cp {data_path}/*-00-*.tfrec {local_data_path}/import tfx
tfx.__version__from tfx import v1 as tfx
from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext
from tfx.components import ImportExampleGen
from tfx.components import StatisticsGen
from tfx.components import SchemaGen
from tfx.components import ExampleValidator
from tfx.components import Transform
from tfx.components import Trainer
from tfx.components import Tuner
from tfx.components import Evaluator
from tfx.components import Pusher
from tfx.proto import example_gen_pb2

from tfx.types import Channel
from tfx.types.standard_artifacts import Model
from tfx.types.standard_artifacts import ModelBlessing
from tfx.dsl.components.common import resolver
from tfx.dsl.experimental.latest_blessed_model_resolver import LatestBlessedModelResolver

import tensorflow_model_analysis as tfmacontext = InteractiveContext()input_config = example_gen_pb2.Input(
    splits=[
        example_gen_pb2.Input.Split(name=""train"", pattern=""train-*.tfrec""),
        example_gen_pb2.Input.Split(name=""eval"", pattern=""val-*.tfrec""),
    ]
)
example_gen = ImportExampleGen(
    input_base=local_data_path, 
    input_config=input_config
)

context.run(example_gen)statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])
context.run(statistics_gen)context.show(statistics_gen.outputs['statistics'])%%writefile {schema_file}

feature {
  name: ""image""
  type: FLOAT

  presence {
    min_fraction: 1.0
  }

  float_domain {
    min: 0
    max: 255
  }

  shape {
    dim { size: 256 }
    dim { size: 256 }
    dim { size: 3 }
  }
}

feature {
  name: ""image_shape""
  type: INT

  presence {
    min_fraction: 1.0
  }

  shape {
    dim { size: 3 }
  }
}

feature {
  name: ""label""
  type: INT

  presence {
    min_fraction: 1.0
  }

  int_domain {
    min: 0
    max: 2
  }

  shape {
    dim { size: 1 }
  }  
}schema_gen = tfx.components.ImportSchemaGen(
    schema_file=schema_file)

context.run(schema_gen)example_validator = ExampleValidator(
  statistics=statistics_gen.outputs['statistics'],
  schema=schema_gen.outputs['schema']   
)

context.run(example_validator)context.show(example_validator.outputs['anomalies'])!mkdir modules%%writefile {proprocessing_file}

import tensorflow as tf

IMAGE_KEY = ""image""
LABEL_KEY = ""label""

MODEL_INPUT_IMAGE_KEY = ""pixel_values""
MODEL_INPUT_LABEL_KEY = ""labels""

INPUT_IMG_SIZE = 224

def preprocessing_fn(inputs):
    """"""tf.transform's callback function for preprocessing inputs.
    Args:
      inputs: map from feature keys to raw not-yet-transformed features.
    Returns:
      Map from string feature key to transformed feature operations.
    """"""
    # print(inputs)
    outputs = {}

    inputs[IMAGE_KEY] = tf.image.resize(
        inputs[IMAGE_KEY], [INPUT_IMG_SIZE, INPUT_IMG_SIZE]
    )

    inputs[IMAGE_KEY] = inputs[IMAGE_KEY] / 255.0
    inputs[IMAGE_KEY] = tf.transpose(inputs[IMAGE_KEY], [0, 3, 1, 2])

    outputs[MODEL_INPUT_IMAGE_KEY] = inputs[IMAGE_KEY]
    outputs[MODEL_INPUT_LABEL_KEY] = inputs[LABEL_KEY]

    return outputstransform = Transform(
    examples=example_gen.outputs['examples'],
    schema=schema_gen.outputs['schema'],
    preprocessing_fn=preprocessing_fn)context.run(transform)%%writefile {model_file}

from typing import List, Dict, Tuple

import absl
import tensorflow as tf
import keras_tuner
import tensorflow_transform as tft
from transformers import ViTFeatureExtractor, TFViTForImageClassification
from tfx.components.trainer.fn_args_utils import FnArgs
from tfx.v1.components import TunerFnResult

from tfx_bsl.tfxio import dataset_options
from tfx.components.trainer.fn_args_utils import DataAccessor

feature_extractor = ViTFeatureExtractor()

_TRAIN_LENGTH = 128
_EVAL_LENGTH = 128
_TRAIN_BATCH_SIZE = 8
_EVAL_BATCH_SIZE = 8

_EPOCHS = 1

_LABELS = ['angular_leaf_spot', 'bean_rust', 'healthy']

_CONCRETE_INPUT = ""pixel_values""

_MODEL_INPUT_LABEL_KEY = ""labels""

def INFO(text: str):
    absl.logging.info(text)

def _normalize_img(
    img, mean=feature_extractor.image_mean, std=feature_extractor.image_std
):
    img = img / 255
    mean = tf.constant(mean)
    std = tf.constant(std)
    return (img - mean) / std


def _preprocess_serving(string_input):
    decoded_input = tf.io.decode_base64(string_input)
    decoded = tf.io.decode_jpeg(decoded_input, channels=3)
    resized = tf.image.resize(decoded, size=(224, 224))
    normalized = _normalize_img(resized)
    normalized = tf.transpose(
        normalized, (2, 0, 1)
    )  # Since HF models are channel-first.
    return normalized


@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])
def _preprocess_fn(string_input):
    decoded_images = tf.map_fn(
        _preprocess_serving, string_input, dtype=tf.float32, back_prop=False
    )
    return {_CONCRETE_INPUT: decoded_images}


def _model_exporter(model: tf.keras.Model):
    m_call = tf.function(model.call).get_concrete_function(
        tf.TensorSpec(
            shape=[None, 3, 224, 224], dtype=tf.float32, name=_CONCRETE_INPUT
        )
    )

    @tf.function(input_signature=[tf.TensorSpec([None], tf.string)])
    def serving_fn(string_input):
        labels = tf.constant(list(model.config.id2label.values()), dtype=tf.string)
        images = _preprocess_fn(string_input)

        predictions = m_call(**images)
        indices = tf.argmax(predictions.logits, axis=1)
        pred_source = tf.gather(params=labels, indices=indices)
        probs = tf.nn.softmax(predictions.logits, axis=1)
        pred_confidence = tf.reduce_max(probs, axis=1)
        return {""label"": pred_source, ""confidence"": pred_confidence}

    return serving_fn

###

def _transform_features_signature(
    model: tf.keras.Model, tf_transform_output: tft.TFTransformOutput
):
    """"""
    transform_features_signature simply returns a function that transforms
    any data of the type of tf.Example which is denoted as the type of sta
    ndard_artifacts.Examples in TFX. The purpose of this function is to ap
    ply Transform Graph obtained from Transform component to the data prod
    uced by ImportExampleGen. This function will be used in the Evaluator
    component, so the raw evaluation inputs from ImportExampleGen can be a
    pporiately transformed that the model could understand.
    """"""

    # basically, what Transform component emits is a SavedModel that knows
    # how to transform data. transform_features_layer() simply returns the
    # layer from the Transform.
    model.tft_layer = tf_transform_output.transform_features_layer()

    @tf.function(
        input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string, name=""examples"")]
    )
    def serve_tf_examples_fn(serialized_tf_examples):
        """"""
        raw_feature_spec returns a set of feature maps(dict) for the input
        TFRecords based on the knowledge that Transform component has lear
        ned(learn doesn't mean training here). By using this information,
        the raw data from ImportExampleGen could be parsed with tf.io.parse
        _example utility function.
        Then, it is passed to the model.tft_layer, so the final output we
        get is the transformed data of the raw input.
        """"""
        feature_spec = tf_transform_output.raw_feature_spec()
        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)

        transformed_features = model.tft_layer(parsed_features)

        return transformed_features

    return serve_tf_examples_fn


def _tf_examples_serving_signature(model, tf_transform_output):
    """"""
    tf_examples_serving_signature simply returns a function that performs
    data transformation(preprocessing) and model prediction in a sequential
    manner. How data transformation is done is idential to the process of
    transform_features_signature function.
    """"""

    model.tft_layer = tf_transform_output.transform_features_layer()

    @tf.function(
        input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string, name=""examples"")]
    )
    def serve_tf_examples_fn(
        serialized_tf_example: tf.Tensor,
    ) -> Dict[str, tf.Tensor]:
        raw_feature_spec = tf_transform_output.raw_feature_spec()
        raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)

        transformed_features = model.tft_layer(raw_features)
        logits = model(transformed_features).logits

        return {_MODEL_INPUT_LABEL_KEY: logits}

    return serve_tf_examples_fn

###

def _input_fn(
    file_pattern: List[str],
    data_accessor: DataAccessor,
    tf_transform_output: tft.TFTransformOutput,
    is_train: bool = False,
    batch_size: int = 32,
) -> tf.data.Dataset:
    INFO(f""Reading data from: {file_pattern}"")

    dataset = data_accessor.tf_dataset_factory(
        file_pattern,
        dataset_options.TensorFlowDatasetOptions(
            batch_size=batch_size, label_key=_MODEL_INPUT_LABEL_KEY
        ),
        tf_transform_output.transformed_metadata.schema,
    )

    return dataset

def _get_hyperparameters() -> keras_tuner.HyperParameters:
    hp = keras_tuner.HyperParameters()
    hp.Choice(""learning_rate"", [1e-3, 1e-2], default=1e-3)
    return hp

def _build_model(hparams: keras_tuner.HyperParameters):
  id2label={str(i): c for i, c in enumerate(_LABELS)}
  label2id={c: str(i) for i, c in enumerate(_LABELS)}

  model = TFViTForImageClassification.from_pretrained(
    ""google/vit-base-patch16-224-in21k"",
    num_labels=len(_LABELS),
    label2id=label2id,
    id2label=id2label,
  )

  model.layers[0].trainable=False

  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
  optimizer = tf.keras.optimizers.Adam(learning_rate=hparams.get(""learning_rate""))
  model.compile(optimizer=optimizer, loss=loss, metrics=[""accuracy""])

  return model  

def run_fn(fn_args: FnArgs):
    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)

    train_dataset = _input_fn(
        fn_args.train_files,
        fn_args.data_accessor,
        tf_transform_output,
        is_train=True,
        batch_size=_TRAIN_BATCH_SIZE,
    )

    eval_dataset = _input_fn(
        fn_args.eval_files,
        fn_args.data_accessor,
        tf_transform_output,
        is_train=False,
        batch_size=_EVAL_BATCH_SIZE,
    )

    hparams = keras_tuner.HyperParameters.from_config(fn_args.hyperparameters)
    INFO(f""HyperParameters for training: {hparams.get_config()}"")
    model = _build_model(hparams)

    model.fit(
        train_dataset,
        steps_per_epoch=_TRAIN_LENGTH // _TRAIN_BATCH_SIZE,
        validation_data=eval_dataset,
        validation_steps=_EVAL_LENGTH // _EVAL_BATCH_SIZE,
        epochs=_EPOCHS,
    )

    model.save(
        fn_args.serving_model_dir, 
        save_format=""tf"", 
        signatures={
            ""serving_default"": _model_exporter(model),
            ""transform_features"": _transform_features_signature(
                model, tf_transform_output
            ),
            ""from_examples"": _tf_examples_serving_signature(model, tf_transform_output),
        },
    )

def tuner_fn(fn_args: FnArgs) -> TunerFnResult:
    tuner = keras_tuner.RandomSearch(
        _build_model,
        max_trials=6,
        hyperparameters=_get_hyperparameters(),
        allow_new_entries=False,
        objective=keras_tuner.Objective(""val_accuracy"", ""max""),
        directory=fn_args.working_dir,
        project_name=""img_classification_tuning"",
    )

    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)

    train_dataset = _input_fn(
        fn_args.train_files,
        fn_args.data_accessor,
        tf_transform_output,
        is_train=True,
        batch_size=_TRAIN_BATCH_SIZE,
    )

    eval_dataset = _input_fn(
        fn_args.eval_files,
        fn_args.data_accessor,
        tf_transform_output,
        is_train=False,
        batch_size=_EVAL_BATCH_SIZE,
    )

    return TunerFnResult(
        tuner=tuner,
        fit_kwargs={
            ""x"": train_dataset,
            ""validation_data"": eval_dataset,
            ""steps_per_epoch"": _TRAIN_LENGTH // _TRAIN_BATCH_SIZE,
            ""validation_steps"": _EVAL_LENGTH // _EVAL_BATCH_SIZE,
        },
    )
tuner = Tuner(
    tuner_fn=tuner_fn,
    examples=transform.outputs[""transformed_examples""],
    schema=schema_gen.outputs[""schema""],
    transform_graph=transform.outputs[""transform_graph""],
)context.run(tuner)trainer = Trainer(
    run_fn=model_fn,
    transformed_examples=transform.outputs[""transformed_examples""],
    transform_graph=transform.outputs[""transform_graph""],
    schema=schema_gen.outputs[""schema""],
    hyperparameters=tuner.outputs[""best_hyperparameters""],
)context.run(trainer)model_resolver = resolver.Resolver(
    strategy_class=LatestBlessedModelResolver,
    model=Channel(type=Model),
    model_blessing=Channel(type=ModelBlessing),
).with_id(""latest_blessed_model_resolver"")context.run(model_resolver)eval_configs = tfma.EvalConfig(
    model_specs=[
        tfma.ModelSpec(
            signature_name=""from_examples"",
            preprocessing_function_names=[""transform_features""],
            label_key=""labels"",
            prediction_key=""labels"",
        )
    ],
    slicing_specs=[tfma.SlicingSpec()],
    metrics_specs=[
        tfma.MetricsSpec(
            metrics=[
                tfma.MetricConfig(
                    class_name=""SparseCategoricalAccuracy"",
                    threshold=tfma.MetricThreshold(
                        value_threshold=tfma.GenericValueThreshold(
                            lower_bound={""value"": 0.55}
                        ),
                        # Change threshold will be ignored if there is no
                        # baseline model resolved from MLMD (first run).
                        change_threshold=tfma.GenericChangeThreshold(
                            direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                            absolute={""value"": -1e-3},
                        ),
                    ),
                )
            ]
        )
    ],
)


evaluator = Evaluator(
    examples=example_gen.outputs[""examples""],
    model=trainer.outputs[""model""],
    baseline_model=model_resolver.outputs[""model""],
    eval_config=eval_configs,
)context.run(evaluator)!ls /tmp/tfx-interactive-2022-09-29T04_00_41.730943-z1asg51a/Evaluator/blessing/9"
0,mlops-hf-tf-vision-models,notebooks/basic.ipynb,"!pip -q install tfx
!pip -q install transformers
!pip -q install pillowdata_path = ""gs://beans-lowres/tfrecords""
local_data_path = ""data""

model_file = ""modules/model.py""
model_fn = ""modules.model.run_fn""!mkdir {local_data_path}
!gsutil cp {data_path}/*-00-*.tfrec {local_data_path}/import tfx
tfx.__version__from tfx import v1 as tfx
from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext
from tfx.components import ImportExampleGen
from tfx.components import Trainer
from tfx.components import Pusher
from tfx.components import StatisticsGen
from tfx.proto import example_gen_pb2context = InteractiveContext()input_config = example_gen_pb2.Input(
    splits=[
        example_gen_pb2.Input.Split(name=""train"", pattern=""train-*.tfrec""),
        example_gen_pb2.Input.Split(name=""eval"", pattern=""val-*.tfrec""),
    ]
)
example_gen = ImportExampleGen(
    input_base=local_data_path, 
    input_config=input_config
)context.run(example_gen)!mkdir modules%%writefile {model_file}

from typing import List, Dict, Tuple

import absl
import tensorflow as tf
from transformers import ViTFeatureExtractor, TFViTForImageClassification
from tfx.components.trainer.fn_args_utils import FnArgs

feature_extractor = ViTFeatureExtractor()

_TRAIN_LENGTH = 128
_EVAL_LENGTH = 128
_TRAIN_BATCH_SIZE = 8
_EVAL_BATCH_SIZE = 8

_EPOCHS = 1

_LABELS = ['angular_leaf_spot', 'bean_rust', 'healthy']

_CONCRETE_INPUT = ""pixel_values""

def INFO(text: str):
    absl.logging.info(text)

###

def _normalize_img(
    img, mean=feature_extractor.image_mean, std=feature_extractor.image_std
):
    img = img / 255
    mean = tf.constant(mean)
    std = tf.constant(std)
    return (img - mean) / std


def _preprocess_serving(string_input):
    decoded_input = tf.io.decode_base64(string_input)
    decoded = tf.io.decode_jpeg(decoded_input, channels=3)
    resized = tf.image.resize(decoded, size=(224, 224))
    normalized = _normalize_img(resized)
    normalized = tf.transpose(
        normalized, (2, 0, 1)
    )  # Since HF models are channel-first.
    return normalized


@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])
def _preprocess_fn(string_input):
    decoded_images = tf.map_fn(
        _preprocess_serving, string_input, dtype=tf.float32, back_prop=False
    )
    return {_CONCRETE_INPUT: decoded_images}


def _model_exporter(model: tf.keras.Model):
    m_call = tf.function(model.call).get_concrete_function(
        tf.TensorSpec(
            shape=[None, 3, 224, 224], dtype=tf.float32, name=_CONCRETE_INPUT
        )
    )

    @tf.function(input_signature=[tf.TensorSpec([None], tf.string)])
    def serving_fn(string_input):
        labels = tf.constant(list(model.config.id2label.values()), dtype=tf.string)
        images = _preprocess_fn(string_input)

        predictions = m_call(**images)
        indices = tf.argmax(predictions.logits, axis=1)
        pred_source = tf.gather(params=labels, indices=indices)
        probs = tf.nn.softmax(predictions.logits, axis=1)
        pred_confidence = tf.reduce_max(probs, axis=1)
        return {""label"": pred_source, ""confidence"": pred_confidence}

    return serving_fn

###

def _parse_tfr(proto):
    feature_description = {
        ""image"": tf.io.VarLenFeature(tf.float32),
        ""image_shape"": tf.io.VarLenFeature(tf.int64),
        ""label"": tf.io.VarLenFeature(tf.int64),
    }
    rec = tf.io.parse_single_example(proto, feature_description)
    image_shape = tf.sparse.to_dense(rec[""image_shape""])
    image = tf.reshape(tf.sparse.to_dense(rec[""image""]), image_shape)
    label = tf.sparse.to_dense(rec[""label""])
    return {""pixel_values"": image, ""labels"": label}

def _preprocess(example_batch):
    images = example_batch[""pixel_values""]
    images = tf.transpose(images, perm=[0, 1, 2, 3]) # (batch_size, height, width, num_channels)
    images = tf.image.resize(images, (224, 224))
    images = tf.transpose(images, perm=[0, 3, 1, 2])

    labels = example_batch[""labels""]
    labels = tf.transpose(labels, perm=[0, 1]) # So, that TF can evaluation the shapes.
    
    return {""pixel_values"": images, ""labels"": labels}

def _input_fn(
    file_pattern: List[str],
    batch_size: int = 32,
    is_train: bool = False,
) -> tf.data.Dataset:
    INFO(f""Reading data from: {file_pattern}"")

    dataset = tf.data.TFRecordDataset(
        tf.io.gfile.glob(file_pattern[0] + "".gz""),
        num_parallel_reads=tf.data.AUTOTUNE,
        compression_type=""GZIP"",
    ).map(_parse_tfr, num_parallel_calls=tf.data.AUTOTUNE)

    if is_train:
        dataset = dataset.shuffle(batch_size * 2)

    dataset = dataset.batch(batch_size)
    dataset = dataset.prefetch(tf.data.AUTOTUNE)
    dataset = dataset.map(_preprocess)
    return dataset

def _build_model():
  id2label={str(i): c for i, c in enumerate(_LABELS)}
  label2id={c: str(i) for i, c in enumerate(_LABELS)}

  model = TFViTForImageClassification.from_pretrained(
    ""google/vit-base-patch16-224-in21k"",
    num_labels=len(_LABELS),
    label2id=label2id,
    id2label=id2label,
  )

  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
  model.compile(optimizer='adam', loss=loss, metrics=[""accuracy""])

  return model  

def run_fn(fn_args: FnArgs):
    train_dataset = _input_fn(
        fn_args.train_files,
        is_train=True,
        batch_size=_TRAIN_BATCH_SIZE,
    )

    eval_dataset = _input_fn(
        fn_args.eval_files,
        is_train=False,
        batch_size=_EVAL_BATCH_SIZE,
    )

    model = _build_model()

    model.fit(
        train_dataset,
        steps_per_epoch=_TRAIN_LENGTH // _TRAIN_BATCH_SIZE,
        validation_data=eval_dataset,
        validation_steps=_EVAL_LENGTH // _TRAIN_BATCH_SIZE,
        epochs=_EPOCHS,
    )

    model.save(
        fn_args.serving_model_dir, save_format=""tf"", signatures=_model_exporter(model)
    )trainer = Trainer(
    run_fn=model_fn,
    examples=example_gen.outputs[""examples""],
)context.run(trainer)"
0,mlops-hf-tf-vision-models,notebooks/intermediate.ipynb,"!pip -q install tfx
!pip -q install transformers
!pip -q install pillow!pip install tensorflow==2.8
!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2data_path = ""gs://beans-lowres/tfrecords""
local_data_path = ""data""

model_file = ""modules/model.py""
model_fn = ""modules.model.run_fn""

proprocessing_file = ""modules/preprocessing.py""
preprocessing_fn = ""modules.preprocessing.preprocessing_fn""

schema_file = ""schema.pbtxt""!mkdir {local_data_path}
!gsutil cp {data_path}/*-00-*.tfrec {local_data_path}/import tfx
tfx.__version__from tfx import v1 as tfx
from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext
from tfx.components import ImportExampleGen
from tfx.components import StatisticsGen
from tfx.components import SchemaGen
from tfx.components import ExampleValidator
from tfx.components import Transform
from tfx.components import Trainer
from tfx.components import Pusher
from tfx.proto import example_gen_pb2context = InteractiveContext()input_config = example_gen_pb2.Input(
    splits=[
        example_gen_pb2.Input.Split(name=""train"", pattern=""train-*.tfrec""),
        example_gen_pb2.Input.Split(name=""eval"", pattern=""val-*.tfrec""),
    ]
)
example_gen = ImportExampleGen(
    input_base=local_data_path, 
    input_config=input_config
)

context.run(example_gen)statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])
context.run(statistics_gen)context.show(statistics_gen.outputs['statistics'])%%writefile {schema_file}

feature {
  name: ""image""
  type: FLOAT

  presence {
    min_fraction: 1.0
  }

  float_domain {
    min: 0
    max: 255
  }

  shape {
    dim { size: 256 }
    dim { size: 256 }
    dim { size: 3 }
  }
}

feature {
  name: ""image_shape""
  type: INT

  presence {
    min_fraction: 1.0
  }

  shape {
    dim { size: 3 }
  }
}

feature {
  name: ""label""
  type: INT

  presence {
    min_fraction: 1.0
  }

  int_domain {
    min: 0
    max: 2
  }

  shape {
    dim { size: 1 }
  }  
}schema_gen = tfx.components.ImportSchemaGen(
    schema_file=schema_file)

context.run(schema_gen)example_validator = ExampleValidator(
  statistics=statistics_gen.outputs['statistics'],
  schema=schema_gen.outputs['schema']   
)

context.run(example_validator)context.show(example_validator.outputs['anomalies'])!mkdir modules%%writefile {proprocessing_file}

import tensorflow as tf

IMAGE_KEY = ""image""
LABEL_KEY = ""label""

MODEL_INPUT_IMAGE_KEY = ""pixel_values""
MODEL_INPUT_LABEL_KEY = ""labels""

INPUT_IMG_SIZE = 224

def preprocessing_fn(inputs):
    """"""tf.transform's callback function for preprocessing inputs.
    Args:
      inputs: map from feature keys to raw not-yet-transformed features.
    Returns:
      Map from string feature key to transformed feature operations.
    """"""
    # print(inputs)
    outputs = {}

    inputs[IMAGE_KEY] = tf.image.resize(
        inputs[IMAGE_KEY], [INPUT_IMG_SIZE, INPUT_IMG_SIZE]
    )

    inputs[IMAGE_KEY] = inputs[IMAGE_KEY] / 255.0
    inputs[IMAGE_KEY] = tf.transpose(inputs[IMAGE_KEY], [0, 3, 1, 2])

    outputs[MODEL_INPUT_IMAGE_KEY] = inputs[IMAGE_KEY]
    outputs[MODEL_INPUT_LABEL_KEY] = inputs[LABEL_KEY]

    return outputstransform = Transform(
    examples=example_gen.outputs['examples'],
    schema=schema_gen.outputs['schema'],
    preprocessing_fn=preprocessing_fn)context.run(transform)%%writefile {model_file}

from typing import List, Dict, Tuple

import absl
import tensorflow as tf
import tensorflow_transform as tft
from transformers import ViTFeatureExtractor, TFViTForImageClassification
from tfx.components.trainer.fn_args_utils import FnArgs

from tfx_bsl.tfxio import dataset_options
from tfx.components.trainer.fn_args_utils import DataAccessor

feature_extractor = ViTFeatureExtractor()

_TRAIN_LENGTH = 128
_EVAL_LENGTH = 128
_TRAIN_BATCH_SIZE = 8
_EVAL_BATCH_SIZE = 8

_EPOCHS = 1

_LABELS = ['angular_leaf_spot', 'bean_rust', 'healthy']

_CONCRETE_INPUT = ""pixel_values""

_MODEL_INPUT_LABEL_KEY = ""labels""

def INFO(text: str):
    absl.logging.info(text)

def _normalize_img(
    img, mean=feature_extractor.image_mean, std=feature_extractor.image_std
):
    img = img / 255
    mean = tf.constant(mean)
    std = tf.constant(std)
    return (img - mean) / std


def _preprocess_serving(string_input):
    decoded_input = tf.io.decode_base64(string_input)
    decoded = tf.io.decode_jpeg(decoded_input, channels=3)
    resized = tf.image.resize(decoded, size=(224, 224))
    normalized = _normalize_img(resized)
    normalized = tf.transpose(
        normalized, (2, 0, 1)
    )  # Since HF models are channel-first.
    return normalized


@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])
def _preprocess_fn(string_input):
    decoded_images = tf.map_fn(
        _preprocess_serving, string_input, dtype=tf.float32, back_prop=False
    )
    return {_CONCRETE_INPUT: decoded_images}


def _model_exporter(model: tf.keras.Model):
    m_call = tf.function(model.call).get_concrete_function(
        tf.TensorSpec(
            shape=[None, 3, 224, 224], dtype=tf.float32, name=_CONCRETE_INPUT
        )
    )

    @tf.function(input_signature=[tf.TensorSpec([None], tf.string)])
    def serving_fn(string_input):
        labels = tf.constant(list(model.config.id2label.values()), dtype=tf.string)
        images = _preprocess_fn(string_input)

        predictions = m_call(**images)
        indices = tf.argmax(predictions.logits, axis=1)
        pred_source = tf.gather(params=labels, indices=indices)
        probs = tf.nn.softmax(predictions.logits, axis=1)
        pred_confidence = tf.reduce_max(probs, axis=1)
        return {""label"": pred_source, ""confidence"": pred_confidence}

    return serving_fn


def _input_fn(
    file_pattern: List[str],
    data_accessor: DataAccessor,
    tf_transform_output: tft.TFTransformOutput,
    is_train: bool = False,
    batch_size: int = 32,
) -> tf.data.Dataset:
    INFO(f""Reading data from: {file_pattern}"")

    dataset = data_accessor.tf_dataset_factory(
        file_pattern,
        dataset_options.TensorFlowDatasetOptions(
            batch_size=batch_size, label_key=_MODEL_INPUT_LABEL_KEY
        ),
        tf_transform_output.transformed_metadata.schema,
    )

    return dataset

def _build_model():
  id2label={str(i): c for i, c in enumerate(_LABELS)}
  label2id={c: str(i) for i, c in enumerate(_LABELS)}

  model = TFViTForImageClassification.from_pretrained(
    ""google/vit-base-patch16-224-in21k"",
    num_labels=len(_LABELS),
    label2id=label2id,
    id2label=id2label,
  )

  model.layers[0].trainable=False

  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
  model.compile(optimizer='adam', loss=loss, metrics=[""accuracy""])

  return model  

def run_fn(fn_args: FnArgs):
    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)

    train_dataset = _input_fn(
        fn_args.train_files,
        fn_args.data_accessor,
        tf_transform_output,
        is_train=True,
        batch_size=_TRAIN_BATCH_SIZE,
    )

    eval_dataset = _input_fn(
        fn_args.eval_files,
        fn_args.data_accessor,
        tf_transform_output,
        is_train=False,
        batch_size=_EVAL_BATCH_SIZE,
    )

    model = _build_model()

    model.fit(
        train_dataset,
        steps_per_epoch=_TRAIN_LENGTH // _TRAIN_BATCH_SIZE,
        validation_data=eval_dataset,
        validation_steps=_EVAL_LENGTH // _EVAL_BATCH_SIZE,
        epochs=_EPOCHS,
    )

    model.save(
        fn_args.serving_model_dir, 
        save_format=""tf"", 
        signatures=_model_exporter(model)
    )trainer = Trainer(
    run_fn=model_fn,
    transformed_examples=transform.outputs[""transformed_examples""],
    transform_graph=transform.outputs[""transform_graph""],
    schema=schema_gen.outputs[""schema""],
)context.run(trainer)"
0,mlops-hf-tf-vision-models,notebooks/parse_tfrecord.ipynb,"import tensorflow as tfGCS_PATH_FULL_RESOUTION = ""gs://beans-fullres/tfrecords""
GCS_PATH_LOW_RESOLUTION = ""gs://beans-lowres/tfrecords""BATCH_SIZE = 4
AUTO = tf.data.AUTOTUNEdef parse_tfr(proto):
    feature_description = {
        ""image"": tf.io.VarLenFeature(tf.float32),
        ""image_shape"": tf.io.VarLenFeature(tf.int64),
        ""label"": tf.io.VarLenFeature(tf.int64),
    }
    rec = tf.io.parse_single_example(proto, feature_description)
    image_shape = tf.sparse.to_dense(rec[""image_shape""])
    image = tf.reshape(tf.sparse.to_dense(rec[""image""]), image_shape)
    label = tf.sparse.to_dense(rec[""label""])
    return {""pixel_values"": image, ""label"": label}


def prepare_dataset(GCS_PATH=GCS_PATH_FULL_RESOUTION, 
                    split=""train"", batch_size=BATCH_SIZE):

    if split not in [""train"", ""val""]:
        raise ValueError(
            ""Invalid split provided. Supports splits are: `train` and `val`.""
        )

    dataset = tf.data.TFRecordDataset(
        [filename for filename in tf.io.gfile.glob(f""{GCS_PATH}/{split}-*"")],
        num_parallel_reads=AUTO,
    ).map(parse_tfr, num_parallel_calls=AUTO)

    if split == ""train"":
        dataset = dataset.shuffle(batch_size * 2)

    dataset = dataset.batch(batch_size)
    dataset = dataset.prefetch(AUTO)
    return datasettrain_dataset = prepare_dataset()
val_dataset = prepare_dataset(split=""val"")for batch in train_dataset.take(1):
    print(batch[""pixel_values""].shape, batch[""label""].shape)for batch in val_dataset.take(1):
    print(batch[""pixel_values""].shape, batch[""label""].shape)train_dataset = prepare_dataset(GCS_PATH_LOW_RESOLUTION)
val_dataset = prepare_dataset(GCS_PATH_LOW_RESOLUTION, split=""val"")for batch in train_dataset.take(1):
    print(batch[""pixel_values""].shape, batch[""label""].shape)for batch in val_dataset.take(1):
    print(batch[""pixel_values""].shape, batch[""label""].shape)"
0,mlops-hf-tf-vision-models,advanced_part1/modules/ViT.py,"import tensorflow as tf
from transformers import TFViTForImageClassification

from .common import LABELS
from .common import PRETRAIN_CHECKPOINT
from .utils import INFO


def build_model():
    id2label = {str(i): c for i, c in enumerate(LABELS)}
    label2id = {c: str(i) for i, c in enumerate(LABELS)}

    model = TFViTForImageClassification.from_pretrained(
        PRETRAIN_CHECKPOINT,
        num_labels=len(LABELS),
        label2id=label2id,
        id2label=id2label,
    )

    model.layers[0].trainable = False

    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    model.compile(optimizer=""adam"", loss=loss, metrics=[""accuracy""])

    INFO(model.summary())

    return model
"
0,mlops-hf-tf-vision-models,advanced_part1/modules/common.py,"IMAGE_TFREC_KEY = ""image""
IMAGE_SHAPE_TFREC_KEY = ""image_shape""
LABEL_TFREC_KEY = ""label""

MODEL_INPUT_IMAGE_KEY = ""pixel_values""
MODEL_INPUT_LABEL_KEY = ""labels""
IMAGE_MODEL_KEY = ""pixel_values""
LABEL_MODEL_KEY = ""labels""

CONCRETE_INPUT = ""pixel_values""

PRETRAIN_CHECKPOINT = ""google/vit-base-patch16-224-in21k""

LABELS = [""angular_leaf_spot"", ""bean_rust"", ""healthy""]
"
0,mlops-hf-tf-vision-models,advanced_part1/modules/hyperparams.py,"EPOCHS = 1
BATCH_SIZE = 32

TRAIN_BATCH_SIZE = 32
EVAL_BATCH_SIZE = 32

TRAIN_LENGTH = 1034
EVAL_LENGTH = 128

INPUT_IMG_SIZE = 224
"
0,mlops-hf-tf-vision-models,advanced_part1/modules/preprocessing.py,"import tensorflow as tf
from .common import IMAGE_TFREC_KEY, LABEL_TFREC_KEY
from .common import IMAGE_MODEL_KEY, LABEL_MODEL_KEY
from .hyperparams import INPUT_IMG_SIZE


def preprocessing_fn(inputs):
    """"""tf.transform's callback function for preprocessing inputs.
    Args:
      inputs: map from feature keys to raw not-yet-transformed features.
    Returns:
      Map from string feature key to transformed feature operations.
    """"""
    # print(inputs)
    outputs = {}

    inputs[IMAGE_TFREC_KEY] = tf.image.resize(
        inputs[IMAGE_TFREC_KEY], [INPUT_IMG_SIZE, INPUT_IMG_SIZE]
    )

    inputs[IMAGE_TFREC_KEY] = inputs[IMAGE_TFREC_KEY] / 255.0
    inputs[IMAGE_TFREC_KEY] = tf.transpose(inputs[IMAGE_TFREC_KEY], [0, 3, 1, 2])

    outputs[IMAGE_MODEL_KEY] = inputs[IMAGE_TFREC_KEY]
    outputs[LABEL_MODEL_KEY] = inputs[LABEL_TFREC_KEY]

    return outputs
"
0,mlops-hf-tf-vision-models,advanced_part1/modules/signatures.py,"from typing import Dict

import tensorflow as tf
import tensorflow_transform as tft
from transformers import ViTFeatureExtractor

from .common import PRETRAIN_CHECKPOINT
from .common import CONCRETE_INPUT
from .common import LABEL_MODEL_KEY

feature_extractor = ViTFeatureExtractor.from_pretrained(PRETRAIN_CHECKPOINT)


def _normalize_img(
    img, mean=feature_extractor.image_mean, std=feature_extractor.image_std
):
    img = img / 255
    mean = tf.constant(mean)
    std = tf.constant(std)
    return (img - mean) / std


def _preprocess_serving(string_input):
    decoded_input = tf.io.decode_base64(string_input)
    decoded = tf.io.decode_jpeg(decoded_input, channels=3)
    resized = tf.image.resize(decoded, size=(224, 224))
    normalized = _normalize_img(resized)
    normalized = tf.transpose(
        normalized, (2, 0, 1)
    )  # Since HF models are channel-first.
    return normalized


@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])
def _preprocess_fn(string_input):
    decoded_images = tf.map_fn(
        _preprocess_serving, string_input, dtype=tf.float32, back_prop=False
    )
    return {CONCRETE_INPUT: decoded_images}


def model_exporter(model: tf.keras.Model):
    m_call = tf.function(model.call).get_concrete_function(
        tf.TensorSpec(shape=[None, 3, 224, 224], dtype=tf.float32, name=CONCRETE_INPUT)
    )

    @tf.function(input_signature=[tf.TensorSpec([None], tf.string)])
    def serving_fn(string_input):
        labels = tf.constant(list(model.config.id2label.values()), dtype=tf.string)
        images = _preprocess_fn(string_input)

        predictions = m_call(**images)
        indices = tf.argmax(predictions.logits, axis=1)
        pred_source = tf.gather(params=labels, indices=indices)
        probs = tf.nn.softmax(predictions.logits, axis=1)
        pred_confidence = tf.reduce_max(probs, axis=1)
        return {""label"": pred_source, ""confidence"": pred_confidence}

    return serving_fn


def transform_features_signature(
    model: tf.keras.Model, tf_transform_output: tft.TFTransformOutput
):
    """"""
    transform_features_signature simply returns a function that transforms
    any data of the type of tf.Example which is denoted as the type of sta
    ndard_artifacts.Examples in TFX. The purpose of this function is to ap
    ply Transform Graph obtained from Transform component to the data prod
    uced by ImportExampleGen. This function will be used in the Evaluator
    component, so the raw evaluation inputs from ImportExampleGen can be a
    pporiately transformed that the model could understand.
    """"""

    # basically, what Transform component emits is a SavedModel that knows
    # how to transform data. transform_features_layer() simply returns the
    # layer from the Transform.
    model.tft_layer = tf_transform_output.transform_features_layer()

    @tf.function(
        input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string, name=""examples"")]
    )
    def serve_tf_examples_fn(serialized_tf_examples):
        """"""
        raw_feature_spec returns a set of feature maps(dict) for the input
        TFRecords based on the knowledge that Transform component has lear
        ned(learn doesn't mean training here). By using this information,
        the raw data from ImportExampleGen could be parsed with tf.io.parse
        _example utility function.
        Then, it is passed to the model.tft_layer, so the final output we
        get is the transformed data of the raw input.
        """"""
        feature_spec = tf_transform_output.raw_feature_spec()
        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)

        transformed_features = model.tft_layer(parsed_features)

        return transformed_features

    return serve_tf_examples_fn


def tf_examples_serving_signature(model, tf_transform_output):
    """"""
    tf_examples_serving_signature simply returns a function that performs
    data transformation(preprocessing) and model prediction in a sequential
    manner. How data transformation is done is idential to the process of
    transform_features_signature function.
    """"""

    model.tft_layer = tf_transform_output.transform_features_layer()

    @tf.function(
        input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string, name=""examples"")]
    )
    def serve_tf_examples_fn(
        serialized_tf_example: tf.Tensor,
    ) -> Dict[str, tf.Tensor]:
        raw_feature_spec = tf_transform_output.raw_feature_spec()
        raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)

        transformed_features = model.tft_layer(raw_features)
        logits = model(transformed_features).logits

        return {LABEL_MODEL_KEY: logits}

    return serve_tf_examples_fn
"
0,mlops-hf-tf-vision-models,advanced_part1/modules/train.py,"import tensorflow_transform as tft
from tfx.components.trainer.fn_args_utils import FnArgs

from .train_data import input_fn
from .ViT import build_model
from .signatures import (
    model_exporter,
    transform_features_signature,
    tf_examples_serving_signature,
)

from .hyperparams import TRAIN_BATCH_SIZE, EVAL_BATCH_SIZE
from .hyperparams import TRAIN_LENGTH, EVAL_LENGTH
from .hyperparams import EPOCHS


def run_fn(fn_args: FnArgs):
    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)

    train_dataset = input_fn(
        fn_args.train_files,
        fn_args.data_accessor,
        tf_transform_output,
        is_train=True,
        batch_size=TRAIN_BATCH_SIZE,
    )

    eval_dataset = input_fn(
        fn_args.eval_files,
        fn_args.data_accessor,
        tf_transform_output,
        is_train=False,
        batch_size=EVAL_BATCH_SIZE,
    )

    model = build_model()

    model.fit(
        train_dataset,
        steps_per_epoch=TRAIN_LENGTH // TRAIN_BATCH_SIZE,
        validation_data=eval_dataset,
        validation_steps=EVAL_LENGTH // TRAIN_BATCH_SIZE,
        epochs=EPOCHS,
    )

    model.save(
        fn_args.serving_model_dir,
        save_format=""tf"",
        signatures={
            ""serving_default"": model_exporter(model),
            ""transform_features"": transform_features_signature(
                model, tf_transform_output
            ),
            ""from_examples"": tf_examples_serving_signature(model, tf_transform_output),
        },
    )
"
0,mlops-hf-tf-vision-models,advanced_part1/modules/train_data.py,"from typing import List

import tensorflow as tf
import tensorflow_transform as tft

from tfx_bsl.tfxio import dataset_options
from tfx.components.trainer.fn_args_utils import DataAccessor

from .utils import INFO
from .common import LABEL_MODEL_KEY
from .hyperparams import BATCH_SIZE


def input_fn(
    file_pattern: List[str],
    data_accessor: DataAccessor,
    tf_transform_output: tft.TFTransformOutput,
    is_train: bool = False,
    batch_size: int = BATCH_SIZE,
) -> tf.data.Dataset:
    INFO(f""Reading data from: {file_pattern}"")

    dataset = data_accessor.tf_dataset_factory(
        file_pattern,
        dataset_options.TensorFlowDatasetOptions(
            batch_size=batch_size, label_key=LABEL_MODEL_KEY
        ),
        tf_transform_output.transformed_metadata.schema,
    )

    return dataset
"
0,mlops-hf-tf-vision-models,advanced_part1/modules/utils.py,"import absl


def INFO(text: str):
    absl.logging.info(text)
"
0,mlops-hf-tf-vision-models,advanced_part1/pipeline/configs.py,"import os
import tensorflow_model_analysis as tfma
import tfx.extensions.google_cloud_ai_platform.constants as vertex_const
import tfx.extensions.google_cloud_ai_platform.trainer.executor as vertex_training_const

PIPELINE_NAME = ""vit-e2e-pipeline-advanced-part1""

try:
    import google.auth  # pylint: disable=g-import-not-at-top  # pytype: disable=import-error

    try:
        _, GOOGLE_CLOUD_PROJECT = google.auth.default()
    except google.auth.exceptions.DefaultCredentialsError:
        GOOGLE_CLOUD_PROJECT = ""gcp-ml-172005""
except ImportError:
    GOOGLE_CLOUD_PROJECT = ""gcp-ml-172005""

GOOGLE_CLOUD_REGION = ""us-central1""

GCS_BUCKET_NAME = GOOGLE_CLOUD_PROJECT + ""-complete-mlops""
PIPELINE_IMAGE = f""gcr.io/{GOOGLE_CLOUD_PROJECT}/{PIPELINE_NAME}""

OUTPUT_DIR = os.path.join(""gs://"", GCS_BUCKET_NAME)
PIPELINE_ROOT = os.path.join(OUTPUT_DIR, ""tfx_pipeline_output"", PIPELINE_NAME)

DATA_PATH = ""gs://beans-lowres/tfrecords/""
SCHEMA_PATH = ""pipeline/schema.pbtxt""

TRAINING_FN = ""modules.train.run_fn""
PREPROCESSING_FN = ""modules.preprocessing.preprocessing_fn""

EXAMPLE_GEN_BEAM_ARGS = None
TRANSFORM_BEAM_ARGS = None

EVAL_CONFIGS = tfma.EvalConfig(
    model_specs=[
        tfma.ModelSpec(
            signature_name=""from_examples"",
            preprocessing_function_names=[""transform_features""],
            label_key=""labels"",
            prediction_key=""labels"",
        )
    ],
    slicing_specs=[tfma.SlicingSpec()],
    metrics_specs=[
        tfma.MetricsSpec(
            metrics=[
                tfma.MetricConfig(
                    class_name=""SparseCategoricalAccuracy"",
                    threshold=tfma.MetricThreshold(
                        value_threshold=tfma.GenericValueThreshold(
                            lower_bound={""value"": 0.55}
                        ),
                        # Change threshold will be ignored if there is no
                        # baseline model resolved from MLMD (first run).
                        change_threshold=tfma.GenericChangeThreshold(
                            direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                            absolute={""value"": -1e-3},
                        ),
                    ),
                )
            ]
        )
    ],
)

GCP_AI_PLATFORM_TRAINING_ARGS = {
    vertex_const.ENABLE_VERTEX_KEY: True,
    vertex_const.VERTEX_REGION_KEY: GOOGLE_CLOUD_REGION,
    vertex_training_const.TRAINING_ARGS_KEY: {
        ""project"": GOOGLE_CLOUD_PROJECT,
        ""worker_pool_specs"": [
            {
                ""machine_spec"": {
                    ""machine_type"": ""n1-standard-4"",
                    ""accelerator_type"": ""NVIDIA_TESLA_K80"",
                    ""accelerator_count"": 1,
                },
                ""replica_count"": 1,
                ""container_spec"": {
                    ""image_uri"": PIPELINE_IMAGE,
                },
            }
        ],
    },
    ""use_gpu"": True,
}

fullres_data = os.environ.get(""FULL_RES_DATA"", ""false"")

if fullres_data.lower() == ""true"":
    DATA_PATH = ""gs://beans-fullres/tfrecords/""

    DATAFLOW_SERVICE_ACCOUNT = ""csp-gde-dataflow@gcp-ml-172005.iam.gserviceaccount.com""
    DATAFLOW_MACHINE_TYPE = ""n1-standard-4""
    DATAFLOW_MAX_WORKERS = 4
    DATAFLOW_DISK_SIZE_GB = 100

    EXAMPLE_GEN_BEAM_ARGS = [
        ""--runner=DataflowRunner"",
        ""--project="" + GOOGLE_CLOUD_PROJECT,
        ""--region="" + GOOGLE_CLOUD_REGION,
        ""--service_account_email="" + DATAFLOW_SERVICE_ACCOUNT,
        ""--machine_type="" + DATAFLOW_MACHINE_TYPE,
        ""--experiments=use_runner_v2"",
        ""--max_num_workers="" + str(DATAFLOW_MAX_WORKERS),
        ""--disk_size_gb="" + str(DATAFLOW_DISK_SIZE_GB),
    ]

    TRANSFORM_BEAM_ARGS = [
        ""--runner=DataflowRunner"",
        ""--project="" + GOOGLE_CLOUD_PROJECT,
        ""--region="" + GOOGLE_CLOUD_REGION,
        ""--service_account_email="" + DATAFLOW_SERVICE_ACCOUNT,
        ""--machine_type="" + DATAFLOW_MACHINE_TYPE,
        ""--experiments=use_runner_v2"",
        ""--max_num_workers="" + str(DATAFLOW_MAX_WORKERS),
        ""--disk_size_gb="" + str(DATAFLOW_DISK_SIZE_GB),
        ""--worker_harness_container_image="" + PIPELINE_IMAGE,
    ]

    GCP_AI_PLATFORM_TRAINING_ARGS[vertex_training_const.TRAINING_ARGS_KEY][
        ""worker_pool_specs""
    ] = [
        {
            ""machine_spec"": {
                ""machine_type"": ""n1-standard-8"",
                ""accelerator_type"": ""NVIDIA_TESLA_V100"",
                ""accelerator_count"": 1,
            },
            ""replica_count"": 1,
            ""container_spec"": {
                ""image_uri"": PIPELINE_IMAGE,
            },
        }
    ]


GCP_AI_PLATFORM_SERVING_ARGS = {
    vertex_const.ENABLE_VERTEX_KEY: True,
    vertex_const.VERTEX_REGION_KEY: GOOGLE_CLOUD_REGION,
    vertex_const.VERTEX_CONTAINER_IMAGE_URI_KEY: ""us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest"",
    vertex_const.SERVING_ARGS_KEY: {
        ""project_id"": GOOGLE_CLOUD_PROJECT,
        ""deployed_model_display_name"": PIPELINE_NAME.replace(""-"", ""_""),
        ""endpoint_name"": ""prediction-"" + PIPELINE_NAME.replace(""-"", ""_""),
        ""traffic_split"": {""0"": 100},
        ""machine_type"": ""n1-standard-4"",
        ""min_replica_count"": 1,
        ""max_replica_count"": 1,
    },
}
"
0,mlops-hf-tf-vision-models,advanced_part1/pipeline/kubeflow_pipeline.py,"from typing import Any, Dict, List, Optional, Text

import tensorflow_model_analysis as tfma

from tfx import v1 as tfx

from ml_metadata.proto import metadata_store_pb2
from tfx.proto import example_gen_pb2

from tfx.components import ImportExampleGen
from tfx.components import StatisticsGen
from tfx.components import ExampleValidator
from tfx.components import Transform
from tfx.components import Evaluator
from tfx.extensions.google_cloud_ai_platform.trainer.component import (
    Trainer as VertexTrainer,
)
from tfx.extensions.google_cloud_ai_platform.pusher.component import (
    Pusher as VertexPusher,
)
from tfx.orchestration import pipeline
from tfx.proto import example_gen_pb2

from tfx.types import Channel
from tfx.types.standard_artifacts import Model
from tfx.types.standard_artifacts import ModelBlessing
from tfx.dsl.components.common import resolver
from tfx.dsl.experimental.latest_blessed_model_resolver import (
    LatestBlessedModelResolver,
)


def create_pipeline(
    pipeline_name: Text,
    pipeline_root: Text,
    data_path: Text,
    schema_path: Text,
    modules: Dict[Text, Text],
    eval_configs: tfma.EvalConfig,
    metadata_connection_config: Optional[metadata_store_pb2.ConnectionConfig] = None,
    ai_platform_training_args: Optional[Dict[Text, Text]] = None,
    ai_platform_serving_args: Optional[Dict[Text, Any]] = None,
    example_gen_beam_args: Optional[List] = None,
    transform_beam_args: Optional[List] = None,
) -> tfx.dsl.Pipeline:
    components = []

    input_config = example_gen_pb2.Input(
        splits=[
            example_gen_pb2.Input.Split(name=""train"", pattern=""train-*.tfrec""),
            example_gen_pb2.Input.Split(name=""eval"", pattern=""val-*.tfrec""),
        ]
    )
    example_gen = ImportExampleGen(input_base=data_path, input_config=input_config)
    if example_gen_beam_args is not None:
        example_gen.with_beam_pipeline_args(example_gen_beam_args)
    components.append(example_gen)

    statistics_gen = StatisticsGen(examples=example_gen.outputs[""examples""])
    components.append(statistics_gen)

    schema_gen = tfx.components.ImportSchemaGen(schema_file=schema_path)
    components.append(schema_gen)

    example_validator = ExampleValidator(
        statistics=statistics_gen.outputs[""statistics""],
        schema=schema_gen.outputs[""schema""],
    )
    components.append(example_validator)

    transform_args = {
        ""examples"": example_gen.outputs[""examples""],
        ""schema"": schema_gen.outputs[""schema""],
        ""preprocessing_fn"": modules[""preprocessing_fn""],
    }
    transform = Transform(**transform_args)
    if transform_beam_args is not None:
        transform.with_beam_pipeline_args(transform_beam_args)
    components.append(transform)

    trainer_args = {
        ""run_fn"": modules[""training_fn""],
        ""transformed_examples"": transform.outputs[""transformed_examples""],
        ""transform_graph"": transform.outputs[""transform_graph""],
        ""schema"": schema_gen.outputs[""schema""],
        ""custom_config"": ai_platform_training_args,
    }
    trainer = VertexTrainer(**trainer_args)
    components.append(trainer)

    model_resolver = resolver.Resolver(
        strategy_class=LatestBlessedModelResolver,
        model=Channel(type=Model),
        model_blessing=Channel(type=ModelBlessing),
    ).with_id(""latest_blessed_model_resolver"")
    components.append(model_resolver)

    evaluator = Evaluator(
        examples=example_gen.outputs[""examples""],
        model=trainer.outputs[""model""],
        baseline_model=model_resolver.outputs[""model""],
        eval_config=eval_configs,
    )
    components.append(evaluator)

    pusher_args = {
        ""model"": trainer.outputs[""model""],
        ""model_blessing"": evaluator.outputs[""blessing""],
        ""custom_config"": ai_platform_serving_args,
    }
    pusher = VertexPusher(**pusher_args)  # pylint: disable=unused-variable
    components.append(pusher)

    return pipeline.Pipeline(
        pipeline_name=pipeline_name,
        pipeline_root=pipeline_root,
        components=components,
        enable_cache=True,
        metadata_connection_config=metadata_connection_config,
    )
"
0,mlops-hf-tf-vision-models,advanced_part1/pipeline/local_pipeline.py,"from typing import Dict, Optional, Text

import tensorflow_model_analysis as tfma

from tfx import v1 as tfx

from ml_metadata.proto import metadata_store_pb2
from tfx.proto import example_gen_pb2

from tfx.components import ImportExampleGen
from tfx.components import StatisticsGen
from tfx.components import ExampleValidator
from tfx.components import Transform
from tfx.components import Trainer
from tfx.components import Evaluator
from tfx.components import Pusher
from tfx.orchestration import pipeline
from tfx.proto import example_gen_pb2

from tfx.types import Channel
from tfx.types.standard_artifacts import Model
from tfx.types.standard_artifacts import ModelBlessing
from tfx.dsl.components.common import resolver
from tfx.dsl.experimental.latest_blessed_model_resolver import (
    LatestBlessedModelResolver,
)


def create_pipeline(
    pipeline_name: Text,
    pipeline_root: Text,
    data_path: Text,
    schema_path: Text,
    modules: Dict[Text, Text],
    eval_configs: tfma.EvalConfig,
    serving_model_dir: Text,
    metadata_connection_config: Optional[metadata_store_pb2.ConnectionConfig] = None,
) -> tfx.dsl.Pipeline:
    components = []

    input_config = example_gen_pb2.Input(
        splits=[
            example_gen_pb2.Input.Split(name=""train"", pattern=""train-00-*.tfrec""),
            example_gen_pb2.Input.Split(name=""eval"", pattern=""val-00-*.tfrec""),
        ]
    )
    example_gen = ImportExampleGen(input_base=data_path, input_config=input_config)
    components.append(example_gen)

    statistics_gen = StatisticsGen(examples=example_gen.outputs[""examples""])
    components.append(statistics_gen)

    schema_gen = tfx.components.ImportSchemaGen(schema_file=schema_path)
    components.append(schema_gen)

    example_validator = ExampleValidator(
        statistics=statistics_gen.outputs[""statistics""],
        schema=schema_gen.outputs[""schema""],
    )
    components.append(example_validator)

    transform_args = {
        ""examples"": example_gen.outputs[""examples""],
        ""schema"": schema_gen.outputs[""schema""],
        ""preprocessing_fn"": modules[""preprocessing_fn""],
    }
    transform = Transform(**transform_args)
    components.append(transform)

    trainer_args = {
        ""run_fn"": modules[""training_fn""],
        ""transformed_examples"": transform.outputs[""transformed_examples""],
        ""transform_graph"": transform.outputs[""transform_graph""],
        ""schema"": schema_gen.outputs[""schema""],
    }
    trainer = Trainer(**trainer_args)
    components.append(trainer)

    model_resolver = resolver.Resolver(
        strategy_class=LatestBlessedModelResolver,
        model=Channel(type=Model),
        model_blessing=Channel(type=ModelBlessing),
    ).with_id(""latest_blessed_model_resolver"")
    components.append(model_resolver)

    evaluator = Evaluator(
        examples=example_gen.outputs[""examples""],
        model=trainer.outputs[""model""],
        baseline_model=model_resolver.outputs[""model""],
        eval_config=eval_configs,
    )
    components.append(evaluator)

    pusher_args = {
        ""model"": trainer.outputs[""model""],
        ""model_blessing"": evaluator.outputs[""blessing""],
        ""push_destination"": tfx.proto.PushDestination(
            filesystem=tfx.proto.PushDestination.Filesystem(
                base_directory=serving_model_dir
            )
        ),
    }
    pusher = Pusher(**pusher_args)  # pylint: disable=unused-variable
    components.append(pusher)

    return pipeline.Pipeline(
        pipeline_name=pipeline_name,
        pipeline_root=pipeline_root,
        components=components,
        enable_cache=False,
        metadata_connection_config=metadata_connection_config,
    )
"
0,mlops-hf-tf-vision-models,advanced_part2/modules/ViT.py,"import tensorflow as tf
import keras_tuner
from transformers import TFViTForImageClassification

from .common import LABELS
from .common import PRETRAIN_CHECKPOINT
from .utils import INFO


def build_model(hparams: keras_tuner.HyperParameters):
    id2label = {str(i): c for i, c in enumerate(LABELS)}
    label2id = {c: str(i) for i, c in enumerate(LABELS)}

    model = TFViTForImageClassification.from_pretrained(
        PRETRAIN_CHECKPOINT,
        num_labels=len(LABELS),
        label2id=label2id,
        id2label=id2label,
    )

    model.layers[0].trainable = False

    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    optimizer = tf.keras.optimizers.Adam(learning_rate=hparams.get(""learning_rate""))
    model.compile(optimizer=optimizer, loss=loss, metrics=[""accuracy""])

    INFO(model.summary())

    return model
"
0,mlops-hf-tf-vision-models,advanced_part2/modules/common.py,"IMAGE_TFREC_KEY = ""image""
IMAGE_SHAPE_TFREC_KEY = ""image_shape""
LABEL_TFREC_KEY = ""label""

MODEL_INPUT_IMAGE_KEY = ""pixel_values""
MODEL_INPUT_LABEL_KEY = ""labels""
IMAGE_MODEL_KEY = ""pixel_values""
LABEL_MODEL_KEY = ""labels""

CONCRETE_INPUT = ""pixel_values""

PRETRAIN_CHECKPOINT = ""google/vit-base-patch16-224-in21k""

LABELS = [""angular_leaf_spot"", ""bean_rust"", ""healthy""]
"
0,mlops-hf-tf-vision-models,advanced_part2/modules/hyperparams.py,"import keras_tuner

EPOCHS = 10
BATCH_SIZE = 32

TRAIN_BATCH_SIZE = 32
EVAL_BATCH_SIZE = 32

TRAIN_LENGTH = 1034
EVAL_LENGTH = 128

INPUT_IMG_SIZE = 224


def get_hyperparameters(hyperparameters) -> keras_tuner.HyperParameters:
    hp_set = keras_tuner.HyperParameters()

    for hp in hyperparameters:
        hp_set.Choice(
            hp, hyperparameters[hp][""values""], default=hyperparameters[hp][""default""]
        )

    return hp_set
"
0,mlops-hf-tf-vision-models,advanced_part2/modules/preprocessing.py,"import tensorflow as tf
from .common import IMAGE_TFREC_KEY, LABEL_TFREC_KEY
from .common import IMAGE_MODEL_KEY, LABEL_MODEL_KEY
from .hyperparams import INPUT_IMG_SIZE


def preprocessing_fn(inputs):
    """"""tf.transform's callback function for preprocessing inputs.
    Args:
      inputs: map from feature keys to raw not-yet-transformed features.
    Returns:
      Map from string feature key to transformed feature operations.
    """"""
    # print(inputs)
    outputs = {}

    inputs[IMAGE_TFREC_KEY] = tf.image.resize(
        inputs[IMAGE_TFREC_KEY], [INPUT_IMG_SIZE, INPUT_IMG_SIZE]
    )

    inputs[IMAGE_TFREC_KEY] = inputs[IMAGE_TFREC_KEY] / 255.0
    inputs[IMAGE_TFREC_KEY] = tf.transpose(inputs[IMAGE_TFREC_KEY], [0, 3, 1, 2])

    outputs[IMAGE_MODEL_KEY] = inputs[IMAGE_TFREC_KEY]
    outputs[LABEL_MODEL_KEY] = inputs[LABEL_TFREC_KEY]

    return outputs
"
0,mlops-hf-tf-vision-models,advanced_part2/modules/signatures.py,"from typing import Dict

import tensorflow as tf
import tensorflow_transform as tft
from transformers import ViTFeatureExtractor

from .common import PRETRAIN_CHECKPOINT
from .common import CONCRETE_INPUT
from .common import LABEL_MODEL_KEY

feature_extractor = ViTFeatureExtractor.from_pretrained(PRETRAIN_CHECKPOINT)


def _normalize_img(
    img, mean=feature_extractor.image_mean, std=feature_extractor.image_std
):
    img = img / 255
    mean = tf.constant(mean)
    std = tf.constant(std)
    return (img - mean) / std


def _preprocess_serving(string_input):
    decoded_input = tf.io.decode_base64(string_input)
    decoded = tf.io.decode_jpeg(decoded_input, channels=3)
    resized = tf.image.resize(decoded, size=(224, 224))
    normalized = _normalize_img(resized)
    normalized = tf.transpose(
        normalized, (2, 0, 1)
    )  # Since HF models are channel-first.
    return normalized


@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])
def _preprocess_fn(string_input):
    decoded_images = tf.map_fn(
        _preprocess_serving, string_input, dtype=tf.float32, back_prop=False
    )
    return {CONCRETE_INPUT: decoded_images}


def model_exporter(model: tf.keras.Model):
    m_call = tf.function(model.call).get_concrete_function(
        tf.TensorSpec(shape=[None, 3, 224, 224], dtype=tf.float32, name=CONCRETE_INPUT)
    )

    @tf.function(input_signature=[tf.TensorSpec([None], tf.string)])
    def serving_fn(string_input):
        labels = tf.constant(list(model.config.id2label.values()), dtype=tf.string)
        images = _preprocess_fn(string_input)

        predictions = m_call(**images)
        indices = tf.argmax(predictions.logits, axis=1)
        pred_source = tf.gather(params=labels, indices=indices)
        probs = tf.nn.softmax(predictions.logits, axis=1)
        pred_confidence = tf.reduce_max(probs, axis=1)
        return {""label"": pred_source, ""confidence"": pred_confidence}

    return serving_fn


def transform_features_signature(
    model: tf.keras.Model, tf_transform_output: tft.TFTransformOutput
):
    """"""
    transform_features_signature simply returns a function that transforms
    any data of the type of tf.Example which is denoted as the type of sta
    ndard_artifacts.Examples in TFX. The purpose of this function is to ap
    ply Transform Graph obtained from Transform component to the data prod
    uced by ImportExampleGen. This function will be used in the Evaluator
    component, so the raw evaluation inputs from ImportExampleGen can be a
    pporiately transformed that the model could understand.
    """"""

    # basically, what Transform component emits is a SavedModel that knows
    # how to transform data. transform_features_layer() simply returns the
    # layer from the Transform.
    model.tft_layer = tf_transform_output.transform_features_layer()

    @tf.function(
        input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string, name=""examples"")]
    )
    def serve_tf_examples_fn(serialized_tf_examples):
        """"""
        raw_feature_spec returns a set of feature maps(dict) for the input
        TFRecords based on the knowledge that Transform component has lear
        ned(learn doesn't mean training here). By using this information,
        the raw data from ImportExampleGen could be parsed with tf.io.parse
        _example utility function.
        Then, it is passed to the model.tft_layer, so the final output we
        get is the transformed data of the raw input.
        """"""
        feature_spec = tf_transform_output.raw_feature_spec()
        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)

        transformed_features = model.tft_layer(parsed_features)

        return transformed_features

    return serve_tf_examples_fn


def tf_examples_serving_signature(model, tf_transform_output):
    """"""
    tf_examples_serving_signature simply returns a function that performs
    data transformation(preprocessing) and model prediction in a sequential
    manner. How data transformation is done is idential to the process of
    transform_features_signature function.
    """"""

    model.tft_layer = tf_transform_output.transform_features_layer()

    @tf.function(
        input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string, name=""examples"")]
    )
    def serve_tf_examples_fn(
        serialized_tf_example: tf.Tensor,
    ) -> Dict[str, tf.Tensor]:
        raw_feature_spec = tf_transform_output.raw_feature_spec()
        raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)

        transformed_features = model.tft_layer(raw_features)
        logits = model(transformed_features).logits

        return {LABEL_MODEL_KEY: logits}

    return serve_tf_examples_fn
"
0,mlops-hf-tf-vision-models,advanced_part2/modules/train.py,"import keras_tuner
import tensorflow_transform as tft
from tfx.components.trainer.fn_args_utils import FnArgs

from .train_data import input_fn
from .ViT import build_model
from .signatures import (
    model_exporter,
    transform_features_signature,
    tf_examples_serving_signature,
)

from .hyperparams import TRAIN_BATCH_SIZE, EVAL_BATCH_SIZE
from .hyperparams import TRAIN_LENGTH, EVAL_LENGTH
from .hyperparams import EPOCHS

from .utils import INFO


def run_fn(fn_args: FnArgs):
    custom_config = fn_args.custom_config
    epochs = EPOCHS

    if custom_config is not None:
        if ""is_local"" in custom_config:
            epochs = 1

    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)

    train_dataset = input_fn(
        fn_args.train_files,
        fn_args.data_accessor,
        tf_transform_output,
        is_train=True,
        batch_size=TRAIN_BATCH_SIZE,
    )

    eval_dataset = input_fn(
        fn_args.eval_files,
        fn_args.data_accessor,
        tf_transform_output,
        is_train=False,
        batch_size=EVAL_BATCH_SIZE,
    )

    hparams = keras_tuner.HyperParameters.from_config(fn_args.hyperparameters)
    INFO(f""HyperParameters for training: {hparams.get_config()}"")
    model = build_model(hparams)

    model.fit(
        train_dataset,
        steps_per_epoch=TRAIN_LENGTH // TRAIN_BATCH_SIZE,
        validation_data=eval_dataset,
        validation_steps=EVAL_LENGTH // TRAIN_BATCH_SIZE,
        epochs=epochs,
    )

    model.save(
        fn_args.serving_model_dir,
        save_format=""tf"",
        signatures={
            ""serving_default"": model_exporter(model),
            ""transform_features"": transform_features_signature(
                model, tf_transform_output
            ),
            ""from_examples"": tf_examples_serving_signature(model, tf_transform_output),
        },
    )
"
0,mlops-hf-tf-vision-models,advanced_part2/modules/train_data.py,"from typing import List

import tensorflow as tf
import tensorflow_transform as tft

from tfx_bsl.tfxio import dataset_options
from tfx.components.trainer.fn_args_utils import DataAccessor

from .utils import INFO
from .common import LABEL_MODEL_KEY
from .hyperparams import BATCH_SIZE


def input_fn(
    file_pattern: List[str],
    data_accessor: DataAccessor,
    tf_transform_output: tft.TFTransformOutput,
    is_train: bool = False,
    batch_size: int = BATCH_SIZE,
) -> tf.data.Dataset:
    INFO(f""Reading data from: {file_pattern}"")

    dataset = data_accessor.tf_dataset_factory(
        file_pattern,
        dataset_options.TensorFlowDatasetOptions(
            batch_size=batch_size, label_key=LABEL_MODEL_KEY
        ),
        tf_transform_output.transformed_metadata.schema,
    )

    return dataset
"
0,mlops-hf-tf-vision-models,advanced_part2/modules/tuning.py,"import keras_tuner
import tensorflow_transform as tft
from tfx.components.trainer.fn_args_utils import FnArgs
from tfx.v1.components import TunerFnResult

from .train_data import input_fn
from .ViT import build_model

from .hyperparams import TRAIN_BATCH_SIZE, EVAL_BATCH_SIZE
from .hyperparams import TRAIN_LENGTH, EVAL_LENGTH
from .hyperparams import get_hyperparameters


def tuner_fn(fn_args: FnArgs) -> TunerFnResult:
    hyperparameters = fn_args.custom_config[""hyperparameters""]

    tuner = keras_tuner.RandomSearch(
        build_model,
        max_trials=6,
        hyperparameters=get_hyperparameters(hyperparameters),
        allow_new_entries=False,
        objective=keras_tuner.Objective(""val_accuracy"", ""max""),
        directory=fn_args.working_dir,
        project_name=""ViT MLOps Advanced Part2"",
    )

    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)

    train_dataset = input_fn(
        fn_args.train_files,
        fn_args.data_accessor,
        tf_transform_output,
        is_train=True,
        batch_size=TRAIN_BATCH_SIZE,
    )

    eval_dataset = input_fn(
        fn_args.eval_files,
        fn_args.data_accessor,
        tf_transform_output,
        is_train=False,
        batch_size=EVAL_BATCH_SIZE,
    )

    return TunerFnResult(
        tuner=tuner,
        fit_kwargs={
            ""x"": train_dataset,
            ""validation_data"": eval_dataset,
            ""steps_per_epoch"": TRAIN_LENGTH // TRAIN_BATCH_SIZE,
            ""validation_steps"": EVAL_LENGTH // EVAL_BATCH_SIZE,
        },
    )
"
0,mlops-hf-tf-vision-models,advanced_part2/modules/utils.py,"import absl


def INFO(text: str):
    absl.logging.info(text)
"
0,mlops-hf-tf-vision-models,advanced_part2/pipeline/configs.py,"import os
import tensorflow_model_analysis as tfma
import tfx.extensions.google_cloud_ai_platform.constants as vertex_const
import tfx.extensions.google_cloud_ai_platform.trainer.executor as vertex_training_const
import tfx.extensions.google_cloud_ai_platform.tuner.executor as vertex_tuner_const

PIPELINE_NAME = ""vit-e2e-pipeline-advanced-part2""

try:
    import google.auth  # pylint: disable=g-import-not-at-top  # pytype: disable=import-error

    try:
        _, GOOGLE_CLOUD_PROJECT = google.auth.default()
    except google.auth.exceptions.DefaultCredentialsError:
        GOOGLE_CLOUD_PROJECT = ""gcp-ml-172005""
except ImportError:
    GOOGLE_CLOUD_PROJECT = ""gcp-ml-172005""

GOOGLE_CLOUD_REGION = ""us-central1""

GCS_BUCKET_NAME = GOOGLE_CLOUD_PROJECT + ""-complete-mlops""
PIPELINE_IMAGE = f""gcr.io/{GOOGLE_CLOUD_PROJECT}/{PIPELINE_NAME}""

OUTPUT_DIR = os.path.join(""gs://"", GCS_BUCKET_NAME)
PIPELINE_ROOT = os.path.join(OUTPUT_DIR, ""tfx_pipeline_output"", PIPELINE_NAME)

DATA_PATH = ""gs://beans-lowres/tfrecords/""
SCHEMA_PATH = ""pipeline/schema.pbtxt""

TRAINING_FN = ""modules.train.run_fn""
TUNER_FN = ""modules.tuning.tuner_fn""
PREPROCESSING_FN = ""modules.preprocessing.preprocessing_fn""

EXAMPLE_GEN_BEAM_ARGS = None
TRANSFORM_BEAM_ARGS = None

TRAIN_BATCH_SIZE = 32
EVAL_BATCH_SIZE = 32
TRAIN_LENGTH = 1034
EVAL_LENGTH = 128

HYPER_PARAMETERS = {
    ""learning_rate"": {""values"": [1e-3, 1e-2, 1e-1], ""default"": 1e-3},
}

EVAL_CONFIGS = tfma.EvalConfig(
    model_specs=[
        tfma.ModelSpec(
            signature_name=""from_examples"",
            preprocessing_function_names=[""transform_features""],
            label_key=""labels"",
            prediction_key=""labels"",
        )
    ],
    slicing_specs=[tfma.SlicingSpec()],
    metrics_specs=[
        tfma.MetricsSpec(
            metrics=[
                tfma.MetricConfig(
                    class_name=""SparseCategoricalAccuracy"",
                    threshold=tfma.MetricThreshold(
                        value_threshold=tfma.GenericValueThreshold(
                            lower_bound={""value"": 0.55}
                        ),
                        # Change threshold will be ignored if there is no
                        # baseline model resolved from MLMD (first run).
                        change_threshold=tfma.GenericChangeThreshold(
                            direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                            absolute={""value"": -1e-3},
                        ),
                    ),
                )
            ]
        )
    ],
)

GCP_AI_PLATFORM_TRAINING_ARGS = {
    vertex_const.ENABLE_VERTEX_KEY: True,
    vertex_const.VERTEX_REGION_KEY: GOOGLE_CLOUD_REGION,
    vertex_training_const.TRAINING_ARGS_KEY: {
        ""project"": GOOGLE_CLOUD_PROJECT,
        ""worker_pool_specs"": [
            {
                ""machine_spec"": {
                    ""machine_type"": ""n1-standard-4"",
                    ""accelerator_type"": ""NVIDIA_TESLA_K80"",
                    ""accelerator_count"": 1,
                },
                ""replica_count"": 1,
                ""container_spec"": {
                    ""image_uri"": PIPELINE_IMAGE,
                },
            }
        ],
    },
    ""use_gpu"": True,
}

fullres_data = os.environ.get(""FULL_RES_DATA"", ""false"")

if fullres_data.lower() == ""true"":
    DATA_PATH = ""gs://beans-fullres/tfrecords/""

    DATAFLOW_SERVICE_ACCOUNT = ""csp-gde-dataflow@gcp-ml-172005.iam.gserviceaccount.com""
    DATAFLOW_MACHINE_TYPE = ""n1-standard-4""
    DATAFLOW_MAX_WORKERS = 4
    DATAFLOW_DISK_SIZE_GB = 100

    EXAMPLE_GEN_BEAM_ARGS = [
        ""--runner=DataflowRunner"",
        ""--project="" + GOOGLE_CLOUD_PROJECT,
        ""--region="" + GOOGLE_CLOUD_REGION,
        ""--service_account_email="" + DATAFLOW_SERVICE_ACCOUNT,
        ""--machine_type="" + DATAFLOW_MACHINE_TYPE,
        ""--experiments=use_runner_v2"",
        ""--max_num_workers="" + str(DATAFLOW_MAX_WORKERS),
        ""--disk_size_gb="" + str(DATAFLOW_DISK_SIZE_GB),
    ]

    TRANSFORM_BEAM_ARGS = [
        ""--runner=DataflowRunner"",
        ""--project="" + GOOGLE_CLOUD_PROJECT,
        ""--region="" + GOOGLE_CLOUD_REGION,
        ""--service_account_email="" + DATAFLOW_SERVICE_ACCOUNT,
        ""--machine_type="" + DATAFLOW_MACHINE_TYPE,
        ""--experiments=use_runner_v2"",
        ""--max_num_workers="" + str(DATAFLOW_MAX_WORKERS),
        ""--disk_size_gb="" + str(DATAFLOW_DISK_SIZE_GB),
        ""--worker_harness_container_image="" + PIPELINE_IMAGE,
    ]

    GCP_AI_PLATFORM_TRAINING_ARGS[vertex_training_const.TRAINING_ARGS_KEY][
        ""worker_pool_specs""
    ] = [
        {
            ""machine_spec"": {
                ""machine_type"": ""n1-standard-8"",
                ""accelerator_type"": ""NVIDIA_TESLA_V100"",
                ""accelerator_count"": 1,
            },
            ""replica_count"": 1,
            ""container_spec"": {
                ""image_uri"": PIPELINE_IMAGE,
            },
        }
    ]

NUM_PARALLEL_TRIALS = 3
GCP_AI_PLATFORM_TUNER_ARGS = {
    vertex_const.ENABLE_VERTEX_KEY: True,
    vertex_const.VERTEX_REGION_KEY: GOOGLE_CLOUD_REGION,
    vertex_tuner_const.TUNING_ARGS_KEY: {
        ""project"": GOOGLE_CLOUD_PROJECT,
        ""job_spec"": {
            ""worker_pool_specs"": [
                {
                    ""machine_spec"": {
                        ""machine_type"": ""n1-standard-4"",
                        ""accelerator_type"": ""NVIDIA_TESLA_K80"",
                        ""accelerator_count"": 1,
                    },
                    ""replica_count"": 1,
                    ""container_spec"": {
                        ""image_uri"": PIPELINE_IMAGE,
                    },
                }
            ],
        },
    },
    vertex_tuner_const.REMOTE_TRIALS_WORKING_DIR_KEY: os.path.join(
        PIPELINE_ROOT, ""trials""
    ),
    ""use_gpu"": True,
    ""hyperparameters"": HYPER_PARAMETERS,
}

GCP_AI_PLATFORM_SERVING_ARGS = {
    vertex_const.ENABLE_VERTEX_KEY: True,
    vertex_const.VERTEX_REGION_KEY: GOOGLE_CLOUD_REGION,
    vertex_const.VERTEX_CONTAINER_IMAGE_URI_KEY: ""us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest"",
    vertex_const.SERVING_ARGS_KEY: {
        ""project_id"": GOOGLE_CLOUD_PROJECT,
        ""deployed_model_display_name"": PIPELINE_NAME.replace(""-"", ""_""),
        ""endpoint_name"": ""prediction-"" + PIPELINE_NAME.replace(""-"", ""_""),
        ""traffic_split"": {""0"": 100},
        ""machine_type"": ""n1-standard-4"",
        ""min_replica_count"": 1,
        ""max_replica_count"": 1,
    },
}
"
0,mlops-hf-tf-vision-models,advanced_part2/pipeline/kubeflow_pipeline.py,"from typing import Any, Dict, List, Optional, Text

import tensorflow_model_analysis as tfma

from tfx import v1 as tfx

from ml_metadata.proto import metadata_store_pb2
from tfx.proto import example_gen_pb2

from tfx.components import ImportExampleGen
from tfx.components import StatisticsGen
from tfx.components import ExampleValidator
from tfx.components import Transform
from tfx.components import Evaluator
from tfx.extensions.google_cloud_ai_platform.trainer.component import (
    Trainer as VertexTrainer,
)
from tfx.extensions.google_cloud_ai_platform.pusher.component import (
    Pusher as VertexPusher,
)
from tfx.extensions.google_cloud_ai_platform.tuner.component import Tuner as VertexTuner
from tfx.orchestration import pipeline
from tfx.proto import example_gen_pb2
from tfx.proto import tuner_pb2

from tfx.types import Channel
from tfx.types.standard_artifacts import Model
from tfx.types.standard_artifacts import ModelBlessing
from tfx.dsl.components.common import resolver
from tfx.dsl.experimental.latest_blessed_model_resolver import (
    LatestBlessedModelResolver,
)


def create_pipeline(
    pipeline_name: Text,
    pipeline_root: Text,
    data_path: Text,
    schema_path: Text,
    modules: Dict[Text, Text],
    eval_configs: tfma.EvalConfig,
    metadata_connection_config: Optional[metadata_store_pb2.ConnectionConfig] = None,
    ai_platform_training_args: Optional[Dict[Text, Text]] = None,
    ai_platform_tuner_args: Optional[Dict[Text, Text]] = None,
    tuner_args: tuner_pb2.TuneArgs = None,
    ai_platform_serving_args: Optional[Dict[Text, Any]] = None,
    example_gen_beam_args: Optional[List] = None,
    transform_beam_args: Optional[List] = None,
) -> tfx.dsl.Pipeline:
    components = []

    input_config = example_gen_pb2.Input(
        splits=[
            example_gen_pb2.Input.Split(name=""train"", pattern=""train-*.tfrec""),
            example_gen_pb2.Input.Split(name=""eval"", pattern=""val-*.tfrec""),
        ]
    )
    example_gen = ImportExampleGen(input_base=data_path, input_config=input_config)
    if example_gen_beam_args is not None:
        example_gen.with_beam_pipeline_args(example_gen_beam_args)
    components.append(example_gen)

    statistics_gen = StatisticsGen(examples=example_gen.outputs[""examples""])
    components.append(statistics_gen)

    schema_gen = tfx.components.ImportSchemaGen(schema_file=schema_path)
    components.append(schema_gen)

    example_validator = ExampleValidator(
        statistics=statistics_gen.outputs[""statistics""],
        schema=schema_gen.outputs[""schema""],
    )
    components.append(example_validator)

    transform_args = {
        ""examples"": example_gen.outputs[""examples""],
        ""schema"": schema_gen.outputs[""schema""],
        ""preprocessing_fn"": modules[""preprocessing_fn""],
    }
    transform = Transform(**transform_args)
    if transform_beam_args is not None:
        transform.with_beam_pipeline_args(transform_beam_args)
    components.append(transform)

    tuner = VertexTuner(
        tuner_fn=modules[""tuner_fn""],
        examples=transform.outputs[""transformed_examples""],
        transform_graph=transform.outputs[""transform_graph""],
        tune_args=tuner_args,
        custom_config=ai_platform_tuner_args,
    )
    components.append(tuner)

    trainer_args = {
        ""run_fn"": modules[""training_fn""],
        ""transformed_examples"": transform.outputs[""transformed_examples""],
        ""transform_graph"": transform.outputs[""transform_graph""],
        ""schema"": schema_gen.outputs[""schema""],
        ""hyperparameters"": tuner.outputs[""best_hyperparameters""],
        ""custom_config"": ai_platform_training_args,
    }
    trainer = VertexTrainer(**trainer_args)
    components.append(trainer)

    model_resolver = resolver.Resolver(
        strategy_class=LatestBlessedModelResolver,
        model=Channel(type=Model),
        model_blessing=Channel(type=ModelBlessing),
    ).with_id(""latest_blessed_model_resolver"")
    components.append(model_resolver)

    evaluator = Evaluator(
        examples=example_gen.outputs[""examples""],
        model=trainer.outputs[""model""],
        baseline_model=model_resolver.outputs[""model""],
        eval_config=eval_configs,
    )
    components.append(evaluator)

    pusher_args = {
        ""model"": trainer.outputs[""model""],
        ""model_blessing"": evaluator.outputs[""blessing""],
        ""custom_config"": ai_platform_serving_args,
    }
    pusher = VertexPusher(**pusher_args)  # pylint: disable=unused-variable
    components.append(pusher)

    return pipeline.Pipeline(
        pipeline_name=pipeline_name,
        pipeline_root=pipeline_root,
        components=components,
        enable_cache=True,
        metadata_connection_config=metadata_connection_config,
    )
"
0,mlops-hf-tf-vision-models,advanced_part2/pipeline/local_pipeline.py,"from typing import Dict, Optional, Text

import tensorflow_model_analysis as tfma

from tfx import v1 as tfx

from ml_metadata.proto import metadata_store_pb2
from tfx.proto import example_gen_pb2

from tfx.components import ImportExampleGen
from tfx.components import StatisticsGen
from tfx.components import ExampleValidator
from tfx.components import Transform
from tfx.components import Tuner
from tfx.components import Trainer
from tfx.components import Evaluator
from tfx.components import Pusher
from tfx.orchestration import pipeline
from tfx.proto import example_gen_pb2

from tfx.types import Channel
from tfx.types.standard_artifacts import Model
from tfx.types.standard_artifacts import ModelBlessing
from tfx.dsl.components.common import resolver
from tfx.dsl.experimental.latest_blessed_model_resolver import (
    LatestBlessedModelResolver,
)


def create_pipeline(
    pipeline_name: Text,
    pipeline_root: Text,
    data_path: Text,
    schema_path: Text,
    modules: Dict[Text, Text],
    hyperparameters: Dict[Text, Text],
    eval_configs: tfma.EvalConfig,
    serving_model_dir: Text,
    metadata_connection_config: Optional[metadata_store_pb2.ConnectionConfig] = None,
) -> tfx.dsl.Pipeline:
    components = []

    input_config = example_gen_pb2.Input(
        splits=[
            example_gen_pb2.Input.Split(name=""train"", pattern=""train-00-*.tfrec""),
            example_gen_pb2.Input.Split(name=""eval"", pattern=""val-00-*.tfrec""),
        ]
    )
    example_gen = ImportExampleGen(input_base=data_path, input_config=input_config)
    components.append(example_gen)

    statistics_gen = StatisticsGen(examples=example_gen.outputs[""examples""])
    components.append(statistics_gen)

    schema_gen = tfx.components.ImportSchemaGen(schema_file=schema_path)
    components.append(schema_gen)

    example_validator = ExampleValidator(
        statistics=statistics_gen.outputs[""statistics""],
        schema=schema_gen.outputs[""schema""],
    )
    components.append(example_validator)

    transform_args = {
        ""examples"": example_gen.outputs[""examples""],
        ""schema"": schema_gen.outputs[""schema""],
        ""preprocessing_fn"": modules[""preprocessing_fn""],
    }
    transform = Transform(**transform_args)
    components.append(transform)

    tuner = Tuner(
        tuner_fn=modules[""tuner_fn""],
        examples=transform.outputs[""transformed_examples""],
        schema=schema_gen.outputs[""schema""],
        transform_graph=transform.outputs[""transform_graph""],
        custom_config={""hyperparameters"": hyperparameters},
    )
    components.append(tuner)

    trainer_args = {
        ""run_fn"": modules[""training_fn""],
        ""transformed_examples"": transform.outputs[""transformed_examples""],
        ""transform_graph"": transform.outputs[""transform_graph""],
        ""schema"": schema_gen.outputs[""schema""],
        ""hyperparameters"": tuner.outputs[""best_hyperparameters""],
        ""custom_config"": {""is_local"": True},
    }
    trainer = Trainer(**trainer_args)
    components.append(trainer)

    model_resolver = resolver.Resolver(
        strategy_class=LatestBlessedModelResolver,
        model=Channel(type=Model),
        model_blessing=Channel(type=ModelBlessing),
    ).with_id(""latest_blessed_model_resolver"")
    components.append(model_resolver)

    evaluator = Evaluator(
        examples=example_gen.outputs[""examples""],
        model=trainer.outputs[""model""],
        baseline_model=model_resolver.outputs[""model""],
        eval_config=eval_configs,
    )
    components.append(evaluator)

    pusher_args = {
        ""model"": trainer.outputs[""model""],
        ""model_blessing"": evaluator.outputs[""blessing""],
        ""push_destination"": tfx.proto.PushDestination(
            filesystem=tfx.proto.PushDestination.Filesystem(
                base_directory=serving_model_dir
            )
        ),
    }
    pusher = Pusher(**pusher_args)  # pylint: disable=unused-variable
    components.append(pusher)

    return pipeline.Pipeline(
        pipeline_name=pipeline_name,
        pipeline_root=pipeline_root,
        components=components,
        enable_cache=False,
        metadata_connection_config=metadata_connection_config,
    )
"
0,mlops-hf-tf-vision-models,basic/modules/ViT.py,"import tensorflow as tf
from transformers import TFViTForImageClassification

from .common import LABELS
from .common import PRETRAIN_CHECKPOINT
from .utils import INFO


def build_model():
    id2label = {str(i): c for i, c in enumerate(LABELS)}
    label2id = {c: str(i) for i, c in enumerate(LABELS)}

    model = TFViTForImageClassification.from_pretrained(
        PRETRAIN_CHECKPOINT,
        num_labels=len(LABELS),
        label2id=label2id,
        id2label=id2label,
    )

    model.layers[0].trainable = False

    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    model.compile(optimizer=""adam"", loss=loss, metrics=[""accuracy""])

    INFO(model.summary())

    return model
"
0,mlops-hf-tf-vision-models,basic/modules/common.py,"IMAGE_TFREC_KEY = ""image""
IMAGE_SHAPE_TFREC_KEY = ""image_shape""
LABEL_TFREC_KEY = ""label""

IMAGE_MODEL_KEY = ""pixel_values""
LABEL_MODEL_KEY = ""labels""

CONCRETE_INPUT = ""pixel_values""

PRETRAIN_CHECKPOINT = ""google/vit-base-patch16-224-in21k""

LABELS = [""angular_leaf_spot"", ""bean_rust"", ""healthy""]
"
0,mlops-hf-tf-vision-models,basic/modules/hyperparams.py,"EPOCHS = 1
BATCH_SIZE = 32

TRAIN_BATCH_SIZE = 32
EVAL_BATCH_SIZE = 32

TRAIN_LENGTH = 1034
EVAL_LENGTH = 128
"
0,mlops-hf-tf-vision-models,basic/modules/signatures.py,"import tensorflow as tf
from transformers import ViTFeatureExtractor

from .common import PRETRAIN_CHECKPOINT
from .common import CONCRETE_INPUT

feature_extractor = ViTFeatureExtractor.from_pretrained(PRETRAIN_CHECKPOINT)


def _normalize_img(
    img, mean=feature_extractor.image_mean, std=feature_extractor.image_std
):
    img = img / 255
    mean = tf.constant(mean)
    std = tf.constant(std)
    return (img - mean) / std


def _preprocess_serving(string_input):
    decoded_input = tf.io.decode_base64(string_input)
    decoded = tf.io.decode_jpeg(decoded_input, channels=3)
    resized = tf.image.resize(decoded, size=(224, 224))
    normalized = _normalize_img(resized)
    normalized = tf.transpose(
        normalized, (2, 0, 1)
    )  # Since HF models are channel-first.
    return normalized


@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])
def _preprocess_fn(string_input):
    decoded_images = tf.map_fn(
        _preprocess_serving, string_input, dtype=tf.float32, back_prop=False
    )
    return {CONCRETE_INPUT: decoded_images}


def model_exporter(model: tf.keras.Model):
    m_call = tf.function(model.call).get_concrete_function(
        tf.TensorSpec(shape=[None, 3, 224, 224], dtype=tf.float32, name=CONCRETE_INPUT)
    )

    @tf.function(input_signature=[tf.TensorSpec([None], tf.string)])
    def serving_fn(string_input):
        labels = tf.constant(list(model.config.id2label.values()), dtype=tf.string)
        images = _preprocess_fn(string_input)

        predictions = m_call(**images)
        indices = tf.argmax(predictions.logits, axis=1)
        pred_source = tf.gather(params=labels, indices=indices)
        probs = tf.nn.softmax(predictions.logits, axis=1)
        pred_confidence = tf.reduce_max(probs, axis=1)
        return {""label"": pred_source, ""confidence"": pred_confidence}

    return serving_fn
"
0,mlops-hf-tf-vision-models,basic/modules/train.py,"from tfx.components.trainer.fn_args_utils import FnArgs

from .train_data import input_fn
from .ViT import build_model
from .signatures import model_exporter

from .hyperparams import TRAIN_BATCH_SIZE, EVAL_BATCH_SIZE
from .hyperparams import TRAIN_LENGTH, EVAL_LENGTH
from .hyperparams import EPOCHS


def run_fn(fn_args: FnArgs):
    train_dataset = input_fn(
        fn_args.train_files,
        is_train=True,
        batch_size=TRAIN_BATCH_SIZE,
    )

    eval_dataset = input_fn(
        fn_args.eval_files,
        is_train=False,
        batch_size=EVAL_BATCH_SIZE,
    )

    model = build_model()

    model.fit(
        train_dataset,
        steps_per_epoch=TRAIN_LENGTH // TRAIN_BATCH_SIZE,
        validation_data=eval_dataset,
        validation_steps=EVAL_LENGTH // TRAIN_BATCH_SIZE,
        epochs=EPOCHS,
    )

    model.save(
        fn_args.serving_model_dir, save_format=""tf"", signatures=model_exporter(model)
    )
"
0,mlops-hf-tf-vision-models,basic/modules/train_data.py,"from typing import List

import tensorflow as tf

from .utils import INFO
from .common import IMAGE_TFREC_KEY, IMAGE_SHAPE_TFREC_KEY, LABEL_TFREC_KEY
from .common import IMAGE_MODEL_KEY, LABEL_MODEL_KEY
from .hyperparams import BATCH_SIZE


def _parse_tfr(proto):
    feature_description = {
        IMAGE_TFREC_KEY: tf.io.VarLenFeature(tf.float32),
        IMAGE_SHAPE_TFREC_KEY: tf.io.VarLenFeature(tf.int64),
        LABEL_TFREC_KEY: tf.io.VarLenFeature(tf.int64),
    }

    rec = tf.io.parse_single_example(proto, feature_description)

    image_shape = tf.sparse.to_dense(rec[IMAGE_SHAPE_TFREC_KEY])
    image = tf.reshape(tf.sparse.to_dense(rec[IMAGE_TFREC_KEY]), image_shape)

    label = tf.sparse.to_dense(rec[LABEL_TFREC_KEY])

    return {IMAGE_MODEL_KEY: image, LABEL_MODEL_KEY: label}


def _preprocess(example_batch):
    images = example_batch[IMAGE_MODEL_KEY]
    images = tf.transpose(
        images, perm=[0, 1, 2, 3]
    )  # (batch_size, height, width, num_channels)
    images = tf.image.resize(images, (224, 224))
    images = tf.transpose(images, perm=[0, 3, 1, 2])

    labels = example_batch[LABEL_MODEL_KEY]
    labels = tf.transpose(labels, perm=[0, 1])  # So, that TF can evaluation the shapes.

    return {IMAGE_MODEL_KEY: images, LABEL_MODEL_KEY: labels}


def input_fn(
    file_pattern: List[str],
    batch_size: int = BATCH_SIZE,
    is_train: bool = False,
) -> tf.data.Dataset:
    INFO(f""Reading data from: {file_pattern}"")

    dataset = tf.data.TFRecordDataset(
        tf.io.gfile.glob(file_pattern[0] + "".gz""),
        num_parallel_reads=tf.data.AUTOTUNE,
        compression_type=""GZIP"",
    ).map(_parse_tfr, num_parallel_calls=tf.data.AUTOTUNE)

    if is_train:
        dataset = dataset.shuffle(batch_size * 2)

    dataset = dataset.batch(batch_size)
    dataset = dataset.prefetch(tf.data.AUTOTUNE)
    dataset = dataset.map(_preprocess)
    return dataset
"
0,mlops-hf-tf-vision-models,basic/modules/utils.py,"import absl


def INFO(text: str):
    absl.logging.info(text)
"
0,mlops-hf-tf-vision-models,basic/pipeline/configs.py,"import os  # pylint: disable=unused-import
import tfx.extensions.google_cloud_ai_platform.constants as vertex_const
import tfx.extensions.google_cloud_ai_platform.trainer.executor as vertex_training_const

PIPELINE_NAME = ""vit-e2e-pipeline-basic""

try:
    import google.auth  # pylint: disable=g-import-not-at-top  # pytype: disable=import-error

    try:
        _, GOOGLE_CLOUD_PROJECT = google.auth.default()
    except google.auth.exceptions.DefaultCredentialsError:
        GOOGLE_CLOUD_PROJECT = ""gcp-ml-172005""
except ImportError:
    GOOGLE_CLOUD_PROJECT = ""gcp-ml-172005""

GOOGLE_CLOUD_REGION = ""us-central1""

GCS_BUCKET_NAME = GOOGLE_CLOUD_PROJECT + ""-complete-mlops""
PIPELINE_IMAGE = f""gcr.io/{GOOGLE_CLOUD_PROJECT}/{PIPELINE_NAME}""

OUTPUT_DIR = os.path.join(""gs://"", GCS_BUCKET_NAME)
PIPELINE_ROOT = os.path.join(OUTPUT_DIR, ""tfx_pipeline_output"", PIPELINE_NAME)

DATA_PATH = ""gs://beans-lowres/tfrecords/""

TRAINING_FN = ""modules.train.run_fn""

EXAMPLE_GEN_BEAM_ARGS = None

GCP_AI_PLATFORM_TRAINING_ARGS = {
    vertex_const.ENABLE_VERTEX_KEY: True,
    vertex_const.VERTEX_REGION_KEY: GOOGLE_CLOUD_REGION,
    vertex_training_const.TRAINING_ARGS_KEY: {
        ""project"": GOOGLE_CLOUD_PROJECT,
        ""worker_pool_specs"": [
            {
                ""machine_spec"": {
                    ""machine_type"": ""n1-standard-4"",
                    ""accelerator_type"": ""NVIDIA_TESLA_K80"",
                    ""accelerator_count"": 1,
                },
                ""replica_count"": 1,
                ""container_spec"": {
                    ""image_uri"": PIPELINE_IMAGE,
                },
            }
        ],
    },
    ""use_gpu"": True,
}

fullres_data = os.environ.get(""FULL_RES_DATA"", ""false"")

if fullres_data.lower() == ""true"":
    DATA_PATH = ""gs://beans-fullres/tfrecords/""

    DATAFLOW_SERVICE_ACCOUNT = ""csp-gde-dataflow@gcp-ml-172005.iam.gserviceaccount.com""
    DATAFLOW_MACHINE_TYPE = ""n1-standard-4""
    DATAFLOW_MAX_WORKERS = 4
    DATAFLOW_DISK_SIZE_GB = 100

    EXAMPLE_GEN_BEAM_ARGS = [
        ""--runner=DataflowRunner"",
        ""--project="" + GOOGLE_CLOUD_PROJECT,
        ""--region="" + GOOGLE_CLOUD_REGION,
        ""--service_account_email="" + DATAFLOW_SERVICE_ACCOUNT,
        ""--machine_type="" + DATAFLOW_MACHINE_TYPE,
        ""--experiments=use_runner_v2"",
        ""--max_num_workers="" + str(DATAFLOW_MAX_WORKERS),
        ""--disk_size_gb="" + str(DATAFLOW_DISK_SIZE_GB),
    ]

    GCP_AI_PLATFORM_TRAINING_ARGS[vertex_training_const.TRAINING_ARGS_KEY][
        ""worker_pool_specs""
    ] = [
        {
            ""machine_spec"": {
                ""machine_type"": ""n1-standard-8"",
                ""accelerator_type"": ""NVIDIA_TESLA_V100"",
                ""accelerator_count"": 1,
            },
            ""replica_count"": 1,
            ""container_spec"": {
                ""image_uri"": PIPELINE_IMAGE,
            },
        }
    ]


GCP_AI_PLATFORM_SERVING_ARGS = {
    vertex_const.ENABLE_VERTEX_KEY: True,
    vertex_const.VERTEX_REGION_KEY: GOOGLE_CLOUD_REGION,
    vertex_const.VERTEX_CONTAINER_IMAGE_URI_KEY: ""us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest"",
    vertex_const.SERVING_ARGS_KEY: {
        ""project_id"": GOOGLE_CLOUD_PROJECT,
        ""deployed_model_display_name"": PIPELINE_NAME.replace(""-"", ""_""),
        ""endpoint_name"": ""prediction-"" + PIPELINE_NAME.replace(""-"", ""_""),
        ""traffic_split"": {""0"": 100},
        ""machine_type"": ""n1-standard-4"",
        ""min_replica_count"": 1,
        ""max_replica_count"": 1,
    },
}
"
0,mlops-hf-tf-vision-models,basic/pipeline/kubeflow_pipeline.py,"from typing import Any, Dict, List, Optional, Text

from tfx import v1 as tfx

from ml_metadata.proto import metadata_store_pb2
from tfx.proto import example_gen_pb2

from tfx.components import ImportExampleGen
from tfx.extensions.google_cloud_ai_platform.trainer.component import (
    Trainer as VertexTrainer,
)
from tfx.extensions.google_cloud_ai_platform.pusher.component import (
    Pusher as VertexPusher,
)
from tfx.orchestration import pipeline
from tfx.proto import example_gen_pb2


def create_pipeline(
    pipeline_name: Text,
    pipeline_root: Text,
    data_path: Text,
    modules: Dict[Text, Text],
    metadata_connection_config: Optional[metadata_store_pb2.ConnectionConfig] = None,
    ai_platform_training_args: Optional[Dict[Text, Text]] = None,
    ai_platform_serving_args: Optional[Dict[Text, Any]] = None,
    example_gen_beam_args: Optional[List] = None,
) -> tfx.dsl.Pipeline:
    components = []

    input_config = example_gen_pb2.Input(
        splits=[
            example_gen_pb2.Input.Split(name=""train"", pattern=""train-*.tfrec""),
            example_gen_pb2.Input.Split(name=""eval"", pattern=""val-*.tfrec""),
        ]
    )
    example_gen = ImportExampleGen(input_base=data_path, input_config=input_config)
    if example_gen_beam_args is not None:
        example_gen.with_beam_pipeline_args(example_gen_beam_args)
    components.append(example_gen)

    trainer_args = {
        ""run_fn"": modules[""training_fn""],
        ""examples"": example_gen.outputs[""examples""],
        ""custom_config"": ai_platform_training_args,
    }
    trainer = VertexTrainer(**trainer_args)
    components.append(trainer)

    pusher_args = {
        ""model"": trainer.outputs[""model""],
        ""custom_config"": ai_platform_serving_args,
    }
    pusher = VertexPusher(**pusher_args)  # pylint: disable=unused-variable
    components.append(pusher)

    return pipeline.Pipeline(
        pipeline_name=pipeline_name,
        pipeline_root=pipeline_root,
        components=components,
        enable_cache=True,
        metadata_connection_config=metadata_connection_config,
    )
"
0,mlops-hf-tf-vision-models,basic/pipeline/local_pipeline.py,"from typing import Dict, Optional, Text

from tfx import v1 as tfx

from ml_metadata.proto import metadata_store_pb2
from tfx.proto import example_gen_pb2

from tfx.components import ImportExampleGen
from tfx.components import Trainer
from tfx.components import Pusher
from tfx.orchestration import pipeline
from tfx.proto import example_gen_pb2


def create_pipeline(
    pipeline_name: Text,
    pipeline_root: Text,
    data_path: Text,
    modules: Dict[Text, Text],
    serving_model_dir: Text,
    metadata_connection_config: Optional[metadata_store_pb2.ConnectionConfig] = None,
) -> tfx.dsl.Pipeline:
    components = []

    input_config = example_gen_pb2.Input(
        splits=[
            example_gen_pb2.Input.Split(name=""train"", pattern=""train-00-*.tfrec""),
            example_gen_pb2.Input.Split(name=""eval"", pattern=""val-00-*.tfrec""),
        ]
    )
    example_gen = ImportExampleGen(input_base=data_path, input_config=input_config)
    components.append(example_gen)

    trainer = Trainer(
        run_fn=modules[""training_fn""],
        examples=example_gen.outputs[""examples""],
    )
    components.append(trainer)

    pusher_args = {
        ""model"": trainer.outputs[""model""],
        ""push_destination"": tfx.proto.PushDestination(
            filesystem=tfx.proto.PushDestination.Filesystem(
                base_directory=serving_model_dir
            )
        ),
    }
    pusher = Pusher(**pusher_args)  # pylint: disable=unused-variable
    components.append(pusher)

    return pipeline.Pipeline(
        pipeline_name=pipeline_name,
        pipeline_root=pipeline_root,
        components=components,
        enable_cache=False,
        metadata_connection_config=metadata_connection_config,
    )
"
0,mlops-hf-tf-vision-models,hf_integration/modules/ViT.py,"import tensorflow as tf
import keras_tuner
from transformers import TFViTForImageClassification

from .common import LABELS
from .common import PRETRAIN_CHECKPOINT
from .utils import INFO


def build_model(hparams: keras_tuner.HyperParameters):
    id2label = {str(i): c for i, c in enumerate(LABELS)}
    label2id = {c: str(i) for i, c in enumerate(LABELS)}

    model = TFViTForImageClassification.from_pretrained(
        PRETRAIN_CHECKPOINT,
        num_labels=len(LABELS),
        label2id=label2id,
        id2label=id2label,
    )

    model.layers[0].trainable = False

    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    optimizer = tf.keras.optimizers.Adam(learning_rate=hparams.get(""learning_rate""))
    model.compile(optimizer=optimizer, loss=loss, metrics=[""accuracy""])

    INFO(model.summary())

    return model
"
0,mlops-hf-tf-vision-models,hf_integration/modules/common.py,"IMAGE_TFREC_KEY = ""image""
IMAGE_SHAPE_TFREC_KEY = ""image_shape""
LABEL_TFREC_KEY = ""label""

MODEL_INPUT_IMAGE_KEY = ""pixel_values""
MODEL_INPUT_LABEL_KEY = ""labels""
IMAGE_MODEL_KEY = ""pixel_values""
LABEL_MODEL_KEY = ""labels""

CONCRETE_INPUT = ""pixel_values""

PRETRAIN_CHECKPOINT = ""google/vit-base-patch16-224-in21k""

LABELS = [""angular_leaf_spot"", ""bean_rust"", ""healthy""]
"
0,mlops-hf-tf-vision-models,hf_integration/modules/hyperparams.py,"import keras_tuner

EPOCHS = 10
BATCH_SIZE = 32

TRAIN_BATCH_SIZE = 32
EVAL_BATCH_SIZE = 32

TRAIN_LENGTH = 1034
EVAL_LENGTH = 128

INPUT_IMG_SIZE = 224


def get_hyperparameters(hyperparameters) -> keras_tuner.HyperParameters:
    hp_set = keras_tuner.HyperParameters()

    for hp in hyperparameters:
        hp_set.Choice(
            hp, hyperparameters[hp][""values""], default=hyperparameters[hp][""default""]
        )

    return hp_set
"
0,mlops-hf-tf-vision-models,hf_integration/modules/preprocessing.py,"import tensorflow as tf
from .common import IMAGE_TFREC_KEY, LABEL_TFREC_KEY
from .common import IMAGE_MODEL_KEY, LABEL_MODEL_KEY
from .hyperparams import INPUT_IMG_SIZE


def preprocessing_fn(inputs):
    """"""tf.transform's callback function for preprocessing inputs.
    Args:
      inputs: map from feature keys to raw not-yet-transformed features.
    Returns:
      Map from string feature key to transformed feature operations.
    """"""
    # print(inputs)
    outputs = {}

    inputs[IMAGE_TFREC_KEY] = tf.image.resize(
        inputs[IMAGE_TFREC_KEY], [INPUT_IMG_SIZE, INPUT_IMG_SIZE]
    )

    inputs[IMAGE_TFREC_KEY] = inputs[IMAGE_TFREC_KEY] / 255.0
    inputs[IMAGE_TFREC_KEY] = tf.transpose(inputs[IMAGE_TFREC_KEY], [0, 3, 1, 2])

    outputs[IMAGE_MODEL_KEY] = inputs[IMAGE_TFREC_KEY]
    outputs[LABEL_MODEL_KEY] = inputs[LABEL_TFREC_KEY]

    return outputs
"
0,mlops-hf-tf-vision-models,hf_integration/modules/signatures.py,"from typing import Dict

import tensorflow as tf
import tensorflow_transform as tft
from transformers import ViTFeatureExtractor

from .common import PRETRAIN_CHECKPOINT
from .common import CONCRETE_INPUT
from .common import LABEL_MODEL_KEY

feature_extractor = ViTFeatureExtractor.from_pretrained(PRETRAIN_CHECKPOINT)


def _normalize_img(
    img, mean=feature_extractor.image_mean, std=feature_extractor.image_std
):
    img = img / 255
    mean = tf.constant(mean)
    std = tf.constant(std)
    return (img - mean) / std


def _preprocess_serving(string_input):
    decoded_input = tf.io.decode_base64(string_input)
    decoded = tf.io.decode_jpeg(decoded_input, channels=3)
    resized = tf.image.resize(decoded, size=(224, 224))
    normalized = _normalize_img(resized)
    normalized = tf.transpose(
        normalized, (2, 0, 1)
    )  # Since HF models are channel-first.
    return normalized


@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])
def _preprocess_fn(string_input):
    decoded_images = tf.map_fn(
        _preprocess_serving, string_input, dtype=tf.float32, back_prop=False
    )
    return {CONCRETE_INPUT: decoded_images}


def model_exporter(model: tf.keras.Model):
    m_call = tf.function(model.call).get_concrete_function(
        tf.TensorSpec(shape=[None, 3, 224, 224], dtype=tf.float32, name=CONCRETE_INPUT)
    )

    @tf.function(input_signature=[tf.TensorSpec([None], tf.string)])
    def serving_fn(string_input):
        labels = tf.constant(list(model.config.id2label.values()), dtype=tf.string)
        images = _preprocess_fn(string_input)

        predictions = m_call(**images)
        indices = tf.argmax(predictions.logits, axis=1)
        pred_source = tf.gather(params=labels, indices=indices)
        probs = tf.nn.softmax(predictions.logits, axis=1)
        pred_confidence = tf.reduce_max(probs, axis=1)
        return {""label"": pred_source, ""confidence"": pred_confidence}

    return serving_fn


def transform_features_signature(
    model: tf.keras.Model, tf_transform_output: tft.TFTransformOutput
):
    """"""
    transform_features_signature simply returns a function that transforms
    any data of the type of tf.Example which is denoted as the type of sta
    ndard_artifacts.Examples in TFX. The purpose of this function is to ap
    ply Transform Graph obtained from Transform component to the data prod
    uced by ImportExampleGen. This function will be used in the Evaluator
    component, so the raw evaluation inputs from ImportExampleGen can be a
    pporiately transformed that the model could understand.
    """"""

    # basically, what Transform component emits is a SavedModel that knows
    # how to transform data. transform_features_layer() simply returns the
    # layer from the Transform.
    model.tft_layer = tf_transform_output.transform_features_layer()

    @tf.function(
        input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string, name=""examples"")]
    )
    def serve_tf_examples_fn(serialized_tf_examples):
        """"""
        raw_feature_spec returns a set of feature maps(dict) for the input
        TFRecords based on the knowledge that Transform component has lear
        ned(learn doesn't mean training here). By using this information,
        the raw data from ImportExampleGen could be parsed with tf.io.parse
        _example utility function.
        Then, it is passed to the model.tft_layer, so the final output we
        get is the transformed data of the raw input.
        """"""
        feature_spec = tf_transform_output.raw_feature_spec()
        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)

        transformed_features = model.tft_layer(parsed_features)

        return transformed_features

    return serve_tf_examples_fn


def tf_examples_serving_signature(model, tf_transform_output):
    """"""
    tf_examples_serving_signature simply returns a function that performs
    data transformation(preprocessing) and model prediction in a sequential
    manner. How data transformation is done is idential to the process of
    transform_features_signature function.
    """"""

    model.tft_layer = tf_transform_output.transform_features_layer()

    @tf.function(
        input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string, name=""examples"")]
    )
    def serve_tf_examples_fn(
        serialized_tf_example: tf.Tensor,
    ) -> Dict[str, tf.Tensor]:
        raw_feature_spec = tf_transform_output.raw_feature_spec()
        raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)

        transformed_features = model.tft_layer(raw_features)
        logits = model(transformed_features).logits

        return {LABEL_MODEL_KEY: logits}

    return serve_tf_examples_fn
"
0,mlops-hf-tf-vision-models,hf_integration/modules/train.py,"import keras_tuner
import tensorflow_transform as tft
from tfx.components.trainer.fn_args_utils import FnArgs

from .train_data import input_fn
from .ViT import build_model
from .signatures import (
    model_exporter,
    transform_features_signature,
    tf_examples_serving_signature,
)

from .hyperparams import TRAIN_BATCH_SIZE, EVAL_BATCH_SIZE
from .hyperparams import TRAIN_LENGTH, EVAL_LENGTH
from .hyperparams import EPOCHS

from .utils import INFO


def run_fn(fn_args: FnArgs):
    custom_config = fn_args.custom_config
    epochs = EPOCHS

    if custom_config is not None:
        if ""is_local"" in custom_config:
            epochs = 1

    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)

    train_dataset = input_fn(
        fn_args.train_files,
        fn_args.data_accessor,
        tf_transform_output,
        is_train=True,
        batch_size=TRAIN_BATCH_SIZE,
    )

    eval_dataset = input_fn(
        fn_args.eval_files,
        fn_args.data_accessor,
        tf_transform_output,
        is_train=False,
        batch_size=EVAL_BATCH_SIZE,
    )

    hparams = keras_tuner.HyperParameters.from_config(fn_args.hyperparameters)
    INFO(f""HyperParameters for training: {hparams.get_config()}"")
    model = build_model(hparams)

    model.fit(
        train_dataset,
        steps_per_epoch=TRAIN_LENGTH // TRAIN_BATCH_SIZE,
        validation_data=eval_dataset,
        validation_steps=EVAL_LENGTH // TRAIN_BATCH_SIZE,
        epochs=epochs,
    )

    model.save(
        fn_args.serving_model_dir,
        save_format=""tf"",
        signatures={
            ""serving_default"": model_exporter(model),
            ""transform_features"": transform_features_signature(
                model, tf_transform_output
            ),
            ""from_examples"": tf_examples_serving_signature(model, tf_transform_output),
        },
    )
"
0,mlops-hf-tf-vision-models,hf_integration/modules/train_data.py,"from typing import List

import tensorflow as tf
import tensorflow_transform as tft

from tfx_bsl.tfxio import dataset_options
from tfx.components.trainer.fn_args_utils import DataAccessor

from .utils import INFO
from .common import LABEL_MODEL_KEY
from .hyperparams import BATCH_SIZE


def input_fn(
    file_pattern: List[str],
    data_accessor: DataAccessor,
    tf_transform_output: tft.TFTransformOutput,
    is_train: bool = False,
    batch_size: int = BATCH_SIZE,
) -> tf.data.Dataset:
    INFO(f""Reading data from: {file_pattern}"")

    dataset = data_accessor.tf_dataset_factory(
        file_pattern,
        dataset_options.TensorFlowDatasetOptions(
            batch_size=batch_size, label_key=LABEL_MODEL_KEY
        ),
        tf_transform_output.transformed_metadata.schema,
    )

    return dataset
"
0,mlops-hf-tf-vision-models,hf_integration/modules/tuning.py,"import keras_tuner
import tensorflow_transform as tft
from tfx.components.trainer.fn_args_utils import FnArgs
from tfx.v1.components import TunerFnResult

from .train_data import input_fn
from .ViT import build_model

from .hyperparams import TRAIN_BATCH_SIZE, EVAL_BATCH_SIZE
from .hyperparams import TRAIN_LENGTH, EVAL_LENGTH
from .hyperparams import get_hyperparameters


def tuner_fn(fn_args: FnArgs) -> TunerFnResult:
    hyperparameters = fn_args.custom_config[""hyperparameters""]

    tuner = keras_tuner.RandomSearch(
        build_model,
        max_trials=6,
        hyperparameters=get_hyperparameters(hyperparameters),
        allow_new_entries=False,
        objective=keras_tuner.Objective(""val_accuracy"", ""max""),
        directory=fn_args.working_dir,
        project_name=""ViT MLOps Advanced Part2"",
    )

    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)

    train_dataset = input_fn(
        fn_args.train_files,
        fn_args.data_accessor,
        tf_transform_output,
        is_train=True,
        batch_size=TRAIN_BATCH_SIZE,
    )

    eval_dataset = input_fn(
        fn_args.eval_files,
        fn_args.data_accessor,
        tf_transform_output,
        is_train=False,
        batch_size=EVAL_BATCH_SIZE,
    )

    return TunerFnResult(
        tuner=tuner,
        fit_kwargs={
            ""x"": train_dataset,
            ""validation_data"": eval_dataset,
            ""steps_per_epoch"": TRAIN_LENGTH // TRAIN_BATCH_SIZE,
            ""validation_steps"": EVAL_LENGTH // EVAL_BATCH_SIZE,
        },
    )
"
0,mlops-hf-tf-vision-models,hf_integration/modules/utils.py,"import absl


def INFO(text: str):
    absl.logging.info(text)
"
0,mlops-hf-tf-vision-models,hf_integration/pipeline/configs.py,"import os
import tensorflow_model_analysis as tfma
import tfx.extensions.google_cloud_ai_platform.constants as vertex_const
import tfx.extensions.google_cloud_ai_platform.trainer.executor as vertex_training_const
import tfx.extensions.google_cloud_ai_platform.tuner.executor as vertex_tuner_const

PIPELINE_NAME = ""vit-e2e-pipeline-hf-integration""

try:
    import google.auth  # pylint: disable=g-import-not-at-top  # pytype: disable=import-error

    try:
        _, GOOGLE_CLOUD_PROJECT = google.auth.default()
    except google.auth.exceptions.DefaultCredentialsError:
        GOOGLE_CLOUD_PROJECT = ""gcp-ml-172005""
except ImportError:
    GOOGLE_CLOUD_PROJECT = ""gcp-ml-172005""

GOOGLE_CLOUD_REGION = ""us-central1""

GCS_BUCKET_NAME = GOOGLE_CLOUD_PROJECT + ""-complete-mlops""
PIPELINE_IMAGE = f""gcr.io/{GOOGLE_CLOUD_PROJECT}/{PIPELINE_NAME}""

OUTPUT_DIR = os.path.join(""gs://"", GCS_BUCKET_NAME)
PIPELINE_ROOT = os.path.join(OUTPUT_DIR, ""tfx_pipeline_output"", PIPELINE_NAME)

DATA_PATH = ""gs://beans-lowres/tfrecords/""
SCHEMA_PATH = ""pipeline/schema.pbtxt""

TRAINING_FN = ""modules.train.run_fn""
TUNER_FN = ""modules.tuning.tuner_fn""
PREPROCESSING_FN = ""modules.preprocessing.preprocessing_fn""

EXAMPLE_GEN_BEAM_ARGS = None
TRANSFORM_BEAM_ARGS = None

TRAIN_BATCH_SIZE = 32
EVAL_BATCH_SIZE = 32
TRAIN_LENGTH = 1034
EVAL_LENGTH = 128

HYPER_PARAMETERS = {
    ""learning_rate"": {""values"": [1e-4, 1e-3, 1e-2, 1e-1], ""default"": 1e-4},
}

EVAL_CONFIGS = tfma.EvalConfig(
    model_specs=[
        tfma.ModelSpec(
            signature_name=""from_examples"",
            preprocessing_function_names=[""transform_features""],
            label_key=""labels"",
            prediction_key=""labels"",
        )
    ],
    slicing_specs=[tfma.SlicingSpec()],
    metrics_specs=[
        tfma.MetricsSpec(
            metrics=[
                tfma.MetricConfig(
                    class_name=""SparseCategoricalAccuracy"",
                    threshold=tfma.MetricThreshold(
                        value_threshold=tfma.GenericValueThreshold(
                            lower_bound={""value"": 0.55}
                        ),
                        # Change threshold will be ignored if there is no
                        # baseline model resolved from MLMD (first run).
                        change_threshold=tfma.GenericChangeThreshold(
                            direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                            absolute={""value"": -1e-3},
                        ),
                    ),
                )
            ]
        )
    ],
)

GCP_AI_PLATFORM_TRAINING_ARGS = {
    vertex_const.ENABLE_VERTEX_KEY: True,
    vertex_const.VERTEX_REGION_KEY: GOOGLE_CLOUD_REGION,
    vertex_training_const.TRAINING_ARGS_KEY: {
        ""project"": GOOGLE_CLOUD_PROJECT,
        ""worker_pool_specs"": [
            {
                ""machine_spec"": {
                    ""machine_type"": ""n1-standard-4"",
                    ""accelerator_type"": ""NVIDIA_TESLA_K80"",
                    ""accelerator_count"": 1,
                },
                ""replica_count"": 1,
                ""container_spec"": {
                    ""image_uri"": PIPELINE_IMAGE,
                },
            }
        ],
    },
    ""use_gpu"": True,
}

fullres_data = os.environ.get(""FULL_RES_DATA"", ""false"")

if fullres_data.lower() == ""true"":
    DATA_PATH = ""gs://beans-fullres/tfrecords/""

    DATAFLOW_SERVICE_ACCOUNT = ""csp-gde-dataflow@gcp-ml-172005.iam.gserviceaccount.com""
    DATAFLOW_MACHINE_TYPE = ""n1-standard-4""
    DATAFLOW_MAX_WORKERS = 4
    DATAFLOW_DISK_SIZE_GB = 100

    EXAMPLE_GEN_BEAM_ARGS = [
        ""--runner=DataflowRunner"",
        ""--project="" + GOOGLE_CLOUD_PROJECT,
        ""--region="" + GOOGLE_CLOUD_REGION,
        ""--service_account_email="" + DATAFLOW_SERVICE_ACCOUNT,
        ""--machine_type="" + DATAFLOW_MACHINE_TYPE,
        ""--experiments=use_runner_v2"",
        ""--max_num_workers="" + str(DATAFLOW_MAX_WORKERS),
        ""--disk_size_gb="" + str(DATAFLOW_DISK_SIZE_GB),
    ]

    TRANSFORM_BEAM_ARGS = [
        ""--runner=DataflowRunner"",
        ""--project="" + GOOGLE_CLOUD_PROJECT,
        ""--region="" + GOOGLE_CLOUD_REGION,
        ""--service_account_email="" + DATAFLOW_SERVICE_ACCOUNT,
        ""--machine_type="" + DATAFLOW_MACHINE_TYPE,
        ""--experiments=use_runner_v2"",
        ""--max_num_workers="" + str(DATAFLOW_MAX_WORKERS),
        ""--disk_size_gb="" + str(DATAFLOW_DISK_SIZE_GB),
        ""--worker_harness_container_image="" + PIPELINE_IMAGE,
    ]

    GCP_AI_PLATFORM_TRAINING_ARGS[vertex_training_const.TRAINING_ARGS_KEY][
        ""worker_pool_specs""
    ] = [
        {
            ""machine_spec"": {
                ""machine_type"": ""n1-standard-8"",
                ""accelerator_type"": ""NVIDIA_TESLA_V100"",
                ""accelerator_count"": 1,
            },
            ""replica_count"": 1,
            ""container_spec"": {
                ""image_uri"": PIPELINE_IMAGE,
            },
        }
    ]

NUM_PARALLEL_TRIALS = 3
GCP_AI_PLATFORM_TUNER_ARGS = {
    vertex_const.ENABLE_VERTEX_KEY: True,
    vertex_const.VERTEX_REGION_KEY: GOOGLE_CLOUD_REGION,
    vertex_tuner_const.TUNING_ARGS_KEY: {
        ""project"": GOOGLE_CLOUD_PROJECT,
        ""job_spec"": {
            ""worker_pool_specs"": [
                {
                    ""machine_spec"": {
                        ""machine_type"": ""n1-standard-4"",
                        ""accelerator_type"": ""NVIDIA_TESLA_K80"",
                        ""accelerator_count"": 1,
                    },
                    ""replica_count"": 1,
                    ""container_spec"": {
                        ""image_uri"": PIPELINE_IMAGE,
                    },
                }
            ],
        },
    },
    vertex_tuner_const.REMOTE_TRIALS_WORKING_DIR_KEY: os.path.join(
        PIPELINE_ROOT, ""trials""
    ),
    ""use_gpu"": True,
    ""hyperparameters"": HYPER_PARAMETERS,
}

GCP_AI_PLATFORM_SERVING_ARGS = {
    vertex_const.ENABLE_VERTEX_KEY: True,
    vertex_const.VERTEX_REGION_KEY: GOOGLE_CLOUD_REGION,
    vertex_const.VERTEX_CONTAINER_IMAGE_URI_KEY: ""us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest"",
    vertex_const.SERVING_ARGS_KEY: {
        ""project_id"": GOOGLE_CLOUD_PROJECT,
        ""deployed_model_display_name"": PIPELINE_NAME.replace(""-"", ""_""),
        ""endpoint_name"": ""prediction-"" + PIPELINE_NAME.replace(""-"", ""_""),
        ""traffic_split"": {""0"": 100},
        ""machine_type"": ""n1-standard-4"",
        ""min_replica_count"": 1,
        ""max_replica_count"": 1,
    },
}

HF_PUSHER_ARGS = {
    ""username"": ""chansung"",
    ""access_token"": ""$HF_ACCESS_TOKEN"",
    ""repo_name"": PIPELINE_NAME,
    ""space_config"": {
        ""app_path"": ""app.gradio"",
    }
}
"
0,mlops-hf-tf-vision-models,hf_integration/pipeline/kubeflow_pipeline.py,"from typing import Any, Dict, List, Optional, Text

import tensorflow_model_analysis as tfma

from tfx import v1 as tfx

from ml_metadata.proto import metadata_store_pb2
from tfx.proto import example_gen_pb2

from tfx.components import ImportExampleGen
from tfx.components import StatisticsGen
from tfx.components import ExampleValidator
from tfx.components import Transform
from tfx.components import Evaluator
from tfx.extensions.google_cloud_ai_platform.trainer.component import (
    Trainer as VertexTrainer,
)
from tfx.extensions.google_cloud_ai_platform.pusher.component import (
    Pusher as VertexPusher,
)
from tfx.extensions.google_cloud_ai_platform.tuner.component import Tuner as VertexTuner
from tfx.orchestration import pipeline
from tfx.proto import example_gen_pb2
from tfx.proto import tuner_pb2

from tfx.types import Channel
from tfx.types.standard_artifacts import Model
from tfx.types.standard_artifacts import ModelBlessing
from tfx.dsl.components.common import resolver
from tfx.dsl.experimental.latest_blessed_model_resolver import (
    LatestBlessedModelResolver,
)

from pipeline.components.HFPusher.component import HFPusher
# from pipeline.components.HFPusher.component import HFSpaceConfig

def create_pipeline(
    pipeline_name: Text,
    pipeline_root: Text,
    data_path: Text,
    schema_path: Text,
    modules: Dict[Text, Text],
    eval_configs: tfma.EvalConfig,
    metadata_connection_config: Optional[metadata_store_pb2.ConnectionConfig] = None,
    ai_platform_training_args: Optional[Dict[Text, Text]] = None,
    ai_platform_tuner_args: Optional[Dict[Text, Text]] = None,
    tuner_args: tuner_pb2.TuneArgs = None,
    ai_platform_serving_args: Optional[Dict[Text, Any]] = None,
    example_gen_beam_args: Optional[List] = None,
    transform_beam_args: Optional[List] = None,
    hf_pusher_args: Optional[Dict[Text, Any]] = None,
) -> tfx.dsl.Pipeline:
    components = []

    input_config = example_gen_pb2.Input(
        splits=[
            example_gen_pb2.Input.Split(name=""train"", pattern=""train-*.tfrec""),
            example_gen_pb2.Input.Split(name=""eval"", pattern=""val-*.tfrec""),
        ]
    )
    example_gen = ImportExampleGen(input_base=data_path, input_config=input_config)
    if example_gen_beam_args is not None:
        example_gen.with_beam_pipeline_args(example_gen_beam_args)
    components.append(example_gen)

    statistics_gen = StatisticsGen(examples=example_gen.outputs[""examples""])
    components.append(statistics_gen)

    schema_gen = tfx.components.ImportSchemaGen(schema_file=schema_path)
    components.append(schema_gen)

    example_validator = ExampleValidator(
        statistics=statistics_gen.outputs[""statistics""],
        schema=schema_gen.outputs[""schema""],
    )
    components.append(example_validator)

    transform_args = {
        ""examples"": example_gen.outputs[""examples""],
        ""schema"": schema_gen.outputs[""schema""],
        ""preprocessing_fn"": modules[""preprocessing_fn""],
    }
    transform = Transform(**transform_args)
    if transform_beam_args is not None:
        transform.with_beam_pipeline_args(transform_beam_args)
    components.append(transform)

    tuner = VertexTuner(
        tuner_fn=modules[""tuner_fn""],
        examples=transform.outputs[""transformed_examples""],
        transform_graph=transform.outputs[""transform_graph""],
        tune_args=tuner_args,
        custom_config=ai_platform_tuner_args,
    )
    components.append(tuner)

    trainer_args = {
        ""run_fn"": modules[""training_fn""],
        ""transformed_examples"": transform.outputs[""transformed_examples""],
        ""transform_graph"": transform.outputs[""transform_graph""],
        ""schema"": schema_gen.outputs[""schema""],
        ""hyperparameters"": tuner.outputs[""best_hyperparameters""],
        ""custom_config"": ai_platform_training_args,
    }
    trainer = VertexTrainer(**trainer_args)
    components.append(trainer)

    model_resolver = resolver.Resolver(
        strategy_class=LatestBlessedModelResolver,
        model=Channel(type=Model),
        model_blessing=Channel(type=ModelBlessing),
    ).with_id(""latest_blessed_model_resolver"")
    components.append(model_resolver)

    evaluator = Evaluator(
        examples=example_gen.outputs[""examples""],
        model=trainer.outputs[""model""],
        baseline_model=model_resolver.outputs[""model""],
        eval_config=eval_configs,
    )
    components.append(evaluator)

    pusher_args = {
        ""model"": trainer.outputs[""model""],
        ""model_blessing"": evaluator.outputs[""blessing""],
        ""custom_config"": ai_platform_serving_args,
    }
    pusher = VertexPusher(**pusher_args)  # pylint: disable=unused-variable
    components.append(pusher)

    hf_pusher_args['model'] = trainer.outputs[""model""]
    hf_pusher_args['model_blessing'] = evaluator.outputs[""blessing""]
    hf_pusher = HFPusher(**hf_pusher_args)
    components.append(hf_pusher)

    return pipeline.Pipeline(
        pipeline_name=pipeline_name,
        pipeline_root=pipeline_root,
        components=components,
        enable_cache=True,
        metadata_connection_config=metadata_connection_config,
    )
"
0,mlops-hf-tf-vision-models,hf_integration/pipeline/local_pipeline.py,"from typing import Dict, Optional, Text

import tensorflow_model_analysis as tfma

from tfx import v1 as tfx

from ml_metadata.proto import metadata_store_pb2
from tfx.proto import example_gen_pb2

from tfx.components import ImportExampleGen
from tfx.components import StatisticsGen
from tfx.components import ExampleValidator
from tfx.components import Transform
from tfx.components import Tuner
from tfx.components import Trainer
from tfx.components import Evaluator
from tfx.components import Pusher
from tfx.orchestration import pipeline
from tfx.proto import example_gen_pb2

from tfx.types import Channel
from tfx.types.standard_artifacts import Model
from tfx.types.standard_artifacts import ModelBlessing
from tfx.dsl.components.common import resolver
from tfx.dsl.experimental.latest_blessed_model_resolver import (
    LatestBlessedModelResolver,
)

from pipeline.components.HFPusher.component import HFPusher
# from pipeline.components.HFPusher.component import HFSpaceConfig


def create_pipeline(
    pipeline_name: Text,
    pipeline_root: Text,
    data_path: Text,
    schema_path: Text,
    modules: Dict[Text, Text],
    hyperparameters: Dict[Text, Text],
    eval_configs: tfma.EvalConfig,
    serving_model_dir: Text,
    metadata_connection_config: Optional[metadata_store_pb2.ConnectionConfig] = None,
) -> tfx.dsl.Pipeline:
    components = []

    input_config = example_gen_pb2.Input(
        splits=[
            example_gen_pb2.Input.Split(name=""train"", pattern=""train-00-*.tfrec""),
            example_gen_pb2.Input.Split(name=""eval"", pattern=""val-00-*.tfrec""),
        ]
    )
    example_gen = ImportExampleGen(input_base=data_path, input_config=input_config)
    components.append(example_gen)

    statistics_gen = StatisticsGen(examples=example_gen.outputs[""examples""])
    components.append(statistics_gen)

    schema_gen = tfx.components.ImportSchemaGen(schema_file=schema_path)
    components.append(schema_gen)

    example_validator = ExampleValidator(
        statistics=statistics_gen.outputs[""statistics""],
        schema=schema_gen.outputs[""schema""],
    )
    components.append(example_validator)

    transform_args = {
        ""examples"": example_gen.outputs[""examples""],
        ""schema"": schema_gen.outputs[""schema""],
        ""preprocessing_fn"": modules[""preprocessing_fn""],
    }
    transform = Transform(**transform_args)
    components.append(transform)

    tuner = Tuner(
        tuner_fn=modules[""tuner_fn""],
        examples=transform.outputs[""transformed_examples""],
        schema=schema_gen.outputs[""schema""],
        transform_graph=transform.outputs[""transform_graph""],
        custom_config={""hyperparameters"": hyperparameters},
    )
    components.append(tuner)

    trainer_args = {
        ""run_fn"": modules[""training_fn""],
        ""transformed_examples"": transform.outputs[""transformed_examples""],
        ""transform_graph"": transform.outputs[""transform_graph""],
        ""schema"": schema_gen.outputs[""schema""],
        ""hyperparameters"": tuner.outputs[""best_hyperparameters""],
        ""custom_config"": {""is_local"": True},
    }
    trainer = Trainer(**trainer_args)
    components.append(trainer)

    model_resolver = resolver.Resolver(
        strategy_class=LatestBlessedModelResolver,
        model=Channel(type=Model),
        model_blessing=Channel(type=ModelBlessing),
    ).with_id(""latest_blessed_model_resolver"")
    components.append(model_resolver)

    evaluator = Evaluator(
        examples=example_gen.outputs[""examples""],
        model=trainer.outputs[""model""],
        baseline_model=model_resolver.outputs[""model""],
        eval_config=eval_configs,
    )
    components.append(evaluator)

    pusher_args = {
        ""model"": trainer.outputs[""model""],
        ""model_blessing"": evaluator.outputs[""blessing""],
        ""push_destination"": tfx.proto.PushDestination(
            filesystem=tfx.proto.PushDestination.Filesystem(
                base_directory=serving_model_dir
            )
        ),
    }
    pusher = Pusher(**pusher_args)  # pylint: disable=unused-variable
    components.append(pusher)

    # pusher_args = {
    #     ""model"": trainer.outputs[""model""],
    #     ""model_blessing"": evaluator.outputs[""blessing""],
    #     ""username"": ""chansung"",
    #     ""access_token"": ""<REPLACE_WITH_YOUR_OWN_TOKEN>"",
    #     ""repo_name"": ""vit-e2e-pipeline-hf-integration"",
    # }
    # hf_pusher = HFPusher(**pusher_args)
    # components.append(hf_pusher)

    return pipeline.Pipeline(
        pipeline_name=pipeline_name,
        pipeline_root=pipeline_root,
        components=components,
        enable_cache=False,
        metadata_connection_config=metadata_connection_config,
    )
"
0,mlops-hf-tf-vision-models,intermediate/modules/ViT.py,"import tensorflow as tf
from transformers import TFViTForImageClassification

from .common import LABELS
from .common import PRETRAIN_CHECKPOINT
from .utils import INFO


def build_model():
    id2label = {str(i): c for i, c in enumerate(LABELS)}
    label2id = {c: str(i) for i, c in enumerate(LABELS)}

    model = TFViTForImageClassification.from_pretrained(
        PRETRAIN_CHECKPOINT,
        num_labels=len(LABELS),
        label2id=label2id,
        id2label=id2label,
    )

    model.layers[0].trainable = False

    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    model.compile(optimizer=""adam"", loss=loss, metrics=[""accuracy""])

    INFO(model.summary())

    return model
"
0,mlops-hf-tf-vision-models,intermediate/modules/common.py,"IMAGE_TFREC_KEY = ""image""
IMAGE_SHAPE_TFREC_KEY = ""image_shape""
LABEL_TFREC_KEY = ""label""

MODEL_INPUT_IMAGE_KEY = ""pixel_values""
MODEL_INPUT_LABEL_KEY = ""labels""
IMAGE_MODEL_KEY = ""pixel_values""
LABEL_MODEL_KEY = ""labels""

CONCRETE_INPUT = ""pixel_values""

PRETRAIN_CHECKPOINT = ""google/vit-base-patch16-224-in21k""

LABELS = [""angular_leaf_spot"", ""bean_rust"", ""healthy""]
"
0,mlops-hf-tf-vision-models,intermediate/modules/hyperparams.py,"EPOCHS = 1
BATCH_SIZE = 32

TRAIN_BATCH_SIZE = 32
EVAL_BATCH_SIZE = 32

TRAIN_LENGTH = 1034
EVAL_LENGTH = 128

INPUT_IMG_SIZE = 224
"
0,mlops-hf-tf-vision-models,intermediate/modules/preprocessing.py,"import tensorflow as tf
from .common import IMAGE_TFREC_KEY, LABEL_TFREC_KEY
from .common import IMAGE_MODEL_KEY, LABEL_MODEL_KEY
from .hyperparams import INPUT_IMG_SIZE


def preprocessing_fn(inputs):
    """"""tf.transform's callback function for preprocessing inputs.
    Args:
      inputs: map from feature keys to raw not-yet-transformed features.
    Returns:
      Map from string feature key to transformed feature operations.
    """"""
    # print(inputs)
    outputs = {}

    inputs[IMAGE_TFREC_KEY] = tf.image.resize(
        inputs[IMAGE_TFREC_KEY], [INPUT_IMG_SIZE, INPUT_IMG_SIZE]
    )

    inputs[IMAGE_TFREC_KEY] = inputs[IMAGE_TFREC_KEY] / 255.0
    inputs[IMAGE_TFREC_KEY] = tf.transpose(inputs[IMAGE_TFREC_KEY], [0, 3, 1, 2])

    outputs[IMAGE_MODEL_KEY] = inputs[IMAGE_TFREC_KEY]
    outputs[LABEL_MODEL_KEY] = inputs[LABEL_TFREC_KEY]

    return outputs
"
0,mlops-hf-tf-vision-models,intermediate/modules/signatures.py,"import tensorflow as tf
from transformers import ViTFeatureExtractor

from .common import PRETRAIN_CHECKPOINT
from .common import CONCRETE_INPUT

feature_extractor = ViTFeatureExtractor.from_pretrained(PRETRAIN_CHECKPOINT)


def _normalize_img(
    img, mean=feature_extractor.image_mean, std=feature_extractor.image_std
):
    img = img / 255
    mean = tf.constant(mean)
    std = tf.constant(std)
    return (img - mean) / std


def _preprocess_serving(string_input):
    decoded_input = tf.io.decode_base64(string_input)
    decoded = tf.io.decode_jpeg(decoded_input, channels=3)
    resized = tf.image.resize(decoded, size=(224, 224))
    normalized = _normalize_img(resized)
    normalized = tf.transpose(
        normalized, (2, 0, 1)
    )  # Since HF models are channel-first.
    return normalized


@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])
def _preprocess_fn(string_input):
    decoded_images = tf.map_fn(
        _preprocess_serving, string_input, dtype=tf.float32, back_prop=False
    )
    return {CONCRETE_INPUT: decoded_images}


def model_exporter(model: tf.keras.Model):
    m_call = tf.function(model.call).get_concrete_function(
        tf.TensorSpec(shape=[None, 3, 224, 224], dtype=tf.float32, name=CONCRETE_INPUT)
    )

    @tf.function(input_signature=[tf.TensorSpec([None], tf.string)])
    def serving_fn(string_input):
        labels = tf.constant(list(model.config.id2label.values()), dtype=tf.string)
        images = _preprocess_fn(string_input)

        predictions = m_call(**images)
        indices = tf.argmax(predictions.logits, axis=1)
        pred_source = tf.gather(params=labels, indices=indices)
        probs = tf.nn.softmax(predictions.logits, axis=1)
        pred_confidence = tf.reduce_max(probs, axis=1)
        return {""label"": pred_source, ""confidence"": pred_confidence}

    return serving_fn
"
0,mlops-hf-tf-vision-models,intermediate/modules/train.py,"import tensorflow_transform as tft
from tfx.components.trainer.fn_args_utils import FnArgs

from .train_data import input_fn
from .ViT import build_model
from .signatures import model_exporter

from .hyperparams import TRAIN_BATCH_SIZE, EVAL_BATCH_SIZE
from .hyperparams import TRAIN_LENGTH, EVAL_LENGTH
from .hyperparams import EPOCHS


def run_fn(fn_args: FnArgs):
    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)

    train_dataset = input_fn(
        fn_args.train_files,
        fn_args.data_accessor,
        tf_transform_output,
        is_train=True,
        batch_size=TRAIN_BATCH_SIZE,
    )

    eval_dataset = input_fn(
        fn_args.eval_files,
        fn_args.data_accessor,
        tf_transform_output,
        is_train=False,
        batch_size=EVAL_BATCH_SIZE,
    )

    model = build_model()

    model.fit(
        train_dataset,
        steps_per_epoch=TRAIN_LENGTH // TRAIN_BATCH_SIZE,
        validation_data=eval_dataset,
        validation_steps=EVAL_LENGTH // TRAIN_BATCH_SIZE,
        epochs=EPOCHS,
    )

    model.save(
        fn_args.serving_model_dir, save_format=""tf"", signatures=model_exporter(model)
    )
"
0,mlops-hf-tf-vision-models,intermediate/modules/train_data.py,"from typing import List

import tensorflow as tf
import tensorflow_transform as tft

from tfx_bsl.tfxio import dataset_options
from tfx.components.trainer.fn_args_utils import DataAccessor

from .utils import INFO
from .common import LABEL_MODEL_KEY
from .hyperparams import BATCH_SIZE


def input_fn(
    file_pattern: List[str],
    data_accessor: DataAccessor,
    tf_transform_output: tft.TFTransformOutput,
    is_train: bool = False,
    batch_size: int = BATCH_SIZE,
) -> tf.data.Dataset:
    INFO(f""Reading data from: {file_pattern}"")

    dataset = data_accessor.tf_dataset_factory(
        file_pattern,
        dataset_options.TensorFlowDatasetOptions(
            batch_size=batch_size, label_key=LABEL_MODEL_KEY
        ),
        tf_transform_output.transformed_metadata.schema,
    )

    return dataset
"
0,mlops-hf-tf-vision-models,intermediate/modules/utils.py,"import absl


def INFO(text: str):
    absl.logging.info(text)
"
0,mlops-hf-tf-vision-models,intermediate/pipeline/configs.py,"import os  # pylint: disable=unused-import
import tfx.extensions.google_cloud_ai_platform.constants as vertex_const
import tfx.extensions.google_cloud_ai_platform.trainer.executor as vertex_training_const

PIPELINE_NAME = ""vit-e2e-pipeline-intermediate""

try:
    import google.auth  # pylint: disable=g-import-not-at-top  # pytype: disable=import-error

    try:
        _, GOOGLE_CLOUD_PROJECT = google.auth.default()
    except google.auth.exceptions.DefaultCredentialsError:
        GOOGLE_CLOUD_PROJECT = ""gcp-ml-172005""
except ImportError:
    GOOGLE_CLOUD_PROJECT = ""gcp-ml-172005""

GOOGLE_CLOUD_REGION = ""us-central1""

GCS_BUCKET_NAME = GOOGLE_CLOUD_PROJECT + ""-complete-mlops""
PIPELINE_IMAGE = f""gcr.io/{GOOGLE_CLOUD_PROJECT}/{PIPELINE_NAME}""

OUTPUT_DIR = os.path.join(""gs://"", GCS_BUCKET_NAME)
PIPELINE_ROOT = os.path.join(OUTPUT_DIR, ""tfx_pipeline_output"", PIPELINE_NAME)

DATA_PATH = ""gs://beans-lowres/tfrecords/""
SCHEMA_PATH = ""pipeline/schema.pbtxt""

TRAINING_FN = ""modules.train.run_fn""
PREPROCESSING_FN = ""modules.preprocessing.preprocessing_fn""

EXAMPLE_GEN_BEAM_ARGS = None
TRANSFORM_BEAM_ARGS = None

GCP_AI_PLATFORM_TRAINING_ARGS = {
    vertex_const.ENABLE_VERTEX_KEY: True,
    vertex_const.VERTEX_REGION_KEY: GOOGLE_CLOUD_REGION,
    vertex_training_const.TRAINING_ARGS_KEY: {
        ""project"": GOOGLE_CLOUD_PROJECT,
        ""worker_pool_specs"": [
            {
                ""machine_spec"": {
                    ""machine_type"": ""n1-standard-4"",
                    ""accelerator_type"": ""NVIDIA_TESLA_K80"",
                    ""accelerator_count"": 1,
                },
                ""replica_count"": 1,
                ""container_spec"": {
                    ""image_uri"": PIPELINE_IMAGE,
                },
            }
        ],
    },
    ""use_gpu"": True,
}

fullres_data = os.environ.get(""FULL_RES_DATA"", ""false"")

if fullres_data.lower() == ""true"":
    DATA_PATH = ""gs://beans-fullres/tfrecords/""

    DATAFLOW_SERVICE_ACCOUNT = ""csp-gde-dataflow@gcp-ml-172005.iam.gserviceaccount.com""
    DATAFLOW_MACHINE_TYPE = ""n1-standard-4""
    DATAFLOW_MAX_WORKERS = 4
    DATAFLOW_DISK_SIZE_GB = 100

    EXAMPLE_GEN_BEAM_ARGS = [
        ""--runner=DataflowRunner"",
        ""--project="" + GOOGLE_CLOUD_PROJECT,
        ""--region="" + GOOGLE_CLOUD_REGION,
        ""--service_account_email="" + DATAFLOW_SERVICE_ACCOUNT,
        ""--machine_type="" + DATAFLOW_MACHINE_TYPE,
        ""--experiments=use_runner_v2"",
        ""--max_num_workers="" + str(DATAFLOW_MAX_WORKERS),
        ""--disk_size_gb="" + str(DATAFLOW_DISK_SIZE_GB),
    ]

    TRANSFORM_BEAM_ARGS = [
        ""--runner=DataflowRunner"",
        ""--project="" + GOOGLE_CLOUD_PROJECT,
        ""--region="" + GOOGLE_CLOUD_REGION,
        ""--service_account_email="" + DATAFLOW_SERVICE_ACCOUNT,
        ""--machine_type="" + DATAFLOW_MACHINE_TYPE,
        ""--experiments=use_runner_v2"",
        ""--max_num_workers="" + str(DATAFLOW_MAX_WORKERS),
        ""--disk_size_gb="" + str(DATAFLOW_DISK_SIZE_GB),
        ""--worker_harness_container_image="" + PIPELINE_IMAGE,
    ]

    GCP_AI_PLATFORM_TRAINING_ARGS[vertex_training_const.TRAINING_ARGS_KEY][
        ""worker_pool_specs""
    ] = [
        {
            ""machine_spec"": {
                ""machine_type"": ""n1-standard-8"",
                ""accelerator_type"": ""NVIDIA_TESLA_V100"",
                ""accelerator_count"": 1,
            },
            ""replica_count"": 1,
            ""container_spec"": {
                ""image_uri"": PIPELINE_IMAGE,
            },
        }
    ]


GCP_AI_PLATFORM_SERVING_ARGS = {
    vertex_const.ENABLE_VERTEX_KEY: True,
    vertex_const.VERTEX_REGION_KEY: GOOGLE_CLOUD_REGION,
    vertex_const.VERTEX_CONTAINER_IMAGE_URI_KEY: ""us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest"",
    vertex_const.SERVING_ARGS_KEY: {
        ""project_id"": GOOGLE_CLOUD_PROJECT,
        ""deployed_model_display_name"": PIPELINE_NAME.replace(""-"", ""_""),
        ""endpoint_name"": ""prediction-"" + PIPELINE_NAME.replace(""-"", ""_""),
        ""traffic_split"": {""0"": 100},
        ""machine_type"": ""n1-standard-4"",
        ""min_replica_count"": 1,
        ""max_replica_count"": 1,
    },
}
"
0,mlops-hf-tf-vision-models,intermediate/pipeline/kubeflow_pipeline.py,"from typing import Any, Dict, List, Optional, Text

from tfx import v1 as tfx

from ml_metadata.proto import metadata_store_pb2
from tfx.proto import example_gen_pb2

from tfx.components import ImportExampleGen
from tfx.components import StatisticsGen
from tfx.components import ExampleValidator
from tfx.components import Transform
from tfx.extensions.google_cloud_ai_platform.trainer.component import (
    Trainer as VertexTrainer,
)
from tfx.extensions.google_cloud_ai_platform.pusher.component import (
    Pusher as VertexPusher,
)
from tfx.orchestration import pipeline
from tfx.proto import example_gen_pb2


def create_pipeline(
    pipeline_name: Text,
    pipeline_root: Text,
    data_path: Text,
    schema_path: Text,
    modules: Dict[Text, Text],
    metadata_connection_config: Optional[metadata_store_pb2.ConnectionConfig] = None,
    ai_platform_training_args: Optional[Dict[Text, Text]] = None,
    ai_platform_serving_args: Optional[Dict[Text, Any]] = None,
    example_gen_beam_args: Optional[List] = None,
    transform_beam_args: Optional[List] = None,
) -> tfx.dsl.Pipeline:
    components = []

    input_config = example_gen_pb2.Input(
        splits=[
            example_gen_pb2.Input.Split(name=""train"", pattern=""train-*.tfrec""),
            example_gen_pb2.Input.Split(name=""eval"", pattern=""val-*.tfrec""),
        ]
    )
    example_gen = ImportExampleGen(input_base=data_path, input_config=input_config)
    if example_gen_beam_args is not None:
        example_gen.with_beam_pipeline_args(example_gen_beam_args)
    components.append(example_gen)

    statistics_gen = StatisticsGen(examples=example_gen.outputs[""examples""])
    components.append(statistics_gen)

    schema_gen = tfx.components.ImportSchemaGen(schema_file=schema_path)
    components.append(schema_gen)

    example_validator = ExampleValidator(
        statistics=statistics_gen.outputs[""statistics""],
        schema=schema_gen.outputs[""schema""],
    )
    components.append(example_validator)

    transform_args = {
        ""examples"": example_gen.outputs[""examples""],
        ""schema"": schema_gen.outputs[""schema""],
        ""preprocessing_fn"": modules[""preprocessing_fn""],
    }
    transform = Transform(**transform_args)
    if transform_beam_args is not None:
        transform.with_beam_pipeline_args(transform_beam_args)
    components.append(transform)

    trainer_args = {
        ""run_fn"": modules[""training_fn""],
        ""transformed_examples"": transform.outputs[""transformed_examples""],
        ""transform_graph"": transform.outputs[""transform_graph""],
        ""schema"": schema_gen.outputs[""schema""],
        ""custom_config"": ai_platform_training_args,
    }
    trainer = VertexTrainer(**trainer_args)
    components.append(trainer)

    pusher_args = {
        ""model"": trainer.outputs[""model""],
        ""custom_config"": ai_platform_serving_args,
    }
    pusher = VertexPusher(**pusher_args)  # pylint: disable=unused-variable
    components.append(pusher)

    return pipeline.Pipeline(
        pipeline_name=pipeline_name,
        pipeline_root=pipeline_root,
        components=components,
        enable_cache=True,
        metadata_connection_config=metadata_connection_config,
    )
"
0,mlops-hf-tf-vision-models,intermediate/pipeline/local_pipeline.py,"from typing import Dict, Optional, Text

from tfx import v1 as tfx

from ml_metadata.proto import metadata_store_pb2
from tfx.proto import example_gen_pb2

from tfx.components import ImportExampleGen
from tfx.components import StatisticsGen
from tfx.components import ExampleValidator
from tfx.components import Transform
from tfx.components import Trainer
from tfx.components import Pusher
from tfx.orchestration import pipeline
from tfx.proto import example_gen_pb2


def create_pipeline(
    pipeline_name: Text,
    pipeline_root: Text,
    data_path: Text,
    schema_path: Text,
    modules: Dict[Text, Text],
    serving_model_dir: Text,
    metadata_connection_config: Optional[metadata_store_pb2.ConnectionConfig] = None,
) -> tfx.dsl.Pipeline:
    components = []

    input_config = example_gen_pb2.Input(
        splits=[
            example_gen_pb2.Input.Split(name=""train"", pattern=""train-00-*.tfrec""),
            example_gen_pb2.Input.Split(name=""eval"", pattern=""val-00-*.tfrec""),
        ]
    )
    example_gen = ImportExampleGen(input_base=data_path, input_config=input_config)
    components.append(example_gen)

    statistics_gen = StatisticsGen(examples=example_gen.outputs[""examples""])
    components.append(statistics_gen)

    schema_gen = tfx.components.ImportSchemaGen(schema_file=schema_path)
    components.append(schema_gen)

    example_validator = ExampleValidator(
        statistics=statistics_gen.outputs[""statistics""],
        schema=schema_gen.outputs[""schema""],
    )
    components.append(example_validator)

    transform_args = {
        ""examples"": example_gen.outputs[""examples""],
        ""schema"": schema_gen.outputs[""schema""],
        ""preprocessing_fn"": modules[""preprocessing_fn""],
    }
    transform = Transform(**transform_args)
    components.append(transform)

    trainer_args = {
        ""run_fn"": modules[""training_fn""],
        ""transformed_examples"": transform.outputs[""transformed_examples""],
        ""transform_graph"": transform.outputs[""transform_graph""],
        ""schema"": schema_gen.outputs[""schema""],
    }
    trainer = Trainer(**trainer_args)
    components.append(trainer)

    pusher_args = {
        ""model"": trainer.outputs[""model""],
        ""push_destination"": tfx.proto.PushDestination(
            filesystem=tfx.proto.PushDestination.Filesystem(
                base_directory=serving_model_dir
            )
        ),
    }
    pusher = Pusher(**pusher_args)  # pylint: disable=unused-variable
    components.append(pusher)

    return pipeline.Pipeline(
        pipeline_name=pipeline_name,
        pipeline_root=pipeline_root,
        components=components,
        enable_cache=False,
        metadata_connection_config=metadata_connection_config,
    )
"
0,mlops-hf-tf-vision-models,hf_integration/app/gradio/app.py,"import gradio as gr
import numpy as np
from PIL import Image
import tensorflow as tf
from transformers import ViTFeatureExtractor
from huggingface_hub import from_pretrained_keras

PRETRAIN_CHECKPOINT = ""google/vit-base-patch16-224-in21k""
feature_extractor = ViTFeatureExtractor.from_pretrained(PRETRAIN_CHECKPOINT)

# $MODEL_REPO_ID should be like chansung/test-vit
# $MODEL_VERSION 
MODEL_CKPT = ""$MODEL_REPO_ID@$MODEL_VERSION""
MODEL = from_pretrained_keras(MODEL_CKPT)

RESOLTUION = 224

labels = []

with open(r""labels.txt"", ""r"") as fp:
    for line in fp:
        labels.append(line[:-1])

def normalize_img(
    img, mean=feature_extractor.image_mean, std=feature_extractor.image_std
):
    img = img / 255
    mean = tf.constant(mean)
    std = tf.constant(std)
    return (img - mean) / std

def preprocess_input(image: Image) -> tf.Tensor:
    image = np.array(image)
    image = tf.convert_to_tensor(image)

    image = tf.image.resize(image, (RESOLTUION, RESOLTUION))
    image = normalize_img(image)

    image = tf.transpose(
        image, (2, 0, 1)
    )  # Since HF models are channel-first.

    return {
        ""pixel_values"": tf.expand_dims(image, 0)
    }

def get_predictions(image: Image) -> tf.Tensor:
    preprocessed_image = preprocess_input(image)
    prediction = MODEL.predict(preprocessed_image)
    probs = tf.nn.softmax(prediction['logits'], axis=1)

    confidences = {labels[i]: float(probs[0][i]) for i in range(3)}
    return confidences

title = ""Simple demo for a Image Classification of the Beans Dataset with HF ViT model""

demo = gr.Interface(
    get_predictions,
    gr.inputs.Image(type=""pil""),
    gr.outputs.Label(num_top_classes=3),
    allow_flagging=""never"",
    title=title,
)

demo.launch(debug=True)   
"
0,mlops-hf-tf-vision-models,hf_integration/pipeline/components/HFPusher/__init__.py,"# Copyright 2022 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"
0,mlops-hf-tf-vision-models,hf_integration/pipeline/components/HFPusher/component.py,"# Copyright 2022 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
""""""HuggingFace(HF) Pusher TFX Component.
The HFPusher is used to push model and prototype application to HuggingFace Hub.
""""""
from typing import Text, Dict, Any, Optional

from tfx import types
from tfx.dsl.components.base import base_component, executor_spec
from tfx.types import standard_artifacts
from tfx.types.component_spec import ChannelParameter, ExecutionParameter

from pipeline.components.HFPusher import executor

MODEL_KEY = ""model""
PUSHED_MODEL_KEY = ""pushed_model""
MODEL_BLESSING_KEY = ""model_blessing""


class HFPusherSpec(types.ComponentSpec):
    """"""ComponentSpec for TFX HFPusher Component.""""""

    PARAMETERS = {
        ""username"": ExecutionParameter(type=str),
        ""access_token"": ExecutionParameter(type=str),
        ""repo_name"": ExecutionParameter(type=str),
        ""space_config"": ExecutionParameter(type=Dict[Text, Any], optional=True),
    }
    INPUTS = {
        MODEL_KEY: ChannelParameter(type=standard_artifacts.Model, optional=True),
        MODEL_BLESSING_KEY: ChannelParameter(
            type=standard_artifacts.ModelBlessing, optional=True
        ),
    }
    OUTPUTS = {
        PUSHED_MODEL_KEY: ChannelParameter(type=standard_artifacts.PushedModel),
    }


class HFPusher(base_component.BaseComponent):
    """"""Component for pushing model and application to HuggingFace Hub.

    The `HFPusher` is a [TFX Component](https://www.tensorflow.org/tfx
    /guide/understanding_tfx_pipelines#component), and its primary pur
    pose is to push a model from an upstream component such as [`Train
    er`](https://www.tensorflow.org/tfx/guide/trainer) to HuggingFace
    Model Hub. It also provides a secondary feature that pushes an app
    lication to HuggingFace Space Hub.
    """"""

    SPEC_CLASS = HFPusherSpec
    EXECUTOR_SPEC = executor_spec.ExecutorClassSpec(executor.Executor)

    def __init__(
        self,
        username: str,
        access_token: str,
        repo_name: str,
        space_config: Optional[Dict[Text, Any]] = None,
        model: Optional[types.Channel] = None,
        model_blessing: Optional[types.Channel] = None,
    ):
        """"""The HFPusher TFX component.

        HFPusher pushes a trained or blessed model to HuggingFace Model Hub.
        This is designed to work as a downstream component of Trainer and o
        ptionally Evaluator(optional) components. Trainer gives trained mod
        el, and Evaluator gives information whether the trained model is bl
        essed or not after evaluation of the model. HFPusher component only
        publishes a model when it is blessed. If Evaluator is not specified,
        the input model will always be pushed.

        Args:
        username: the ID of HuggingFace Hub
        access_token: the access token obtained from HuggingFace Hub for the
            given username. Refer to [this document](https://huggingface.co/
            docs/hub/security-tokens) to know how to obtain one.
        repo_name: the name of Model Hub repository where the model will be
            pushed. This should be unique name under the username within th
            e Model Hub. repository is identified as {username}/{repo_name}.
        space_config: optional configurations set when to push an application
            to HuggingFace Space Hub. This is a dictionary, and the following
            information could be set.
            app_path: the path where the application related files are stored.
                this should follow the form either of app.gradio.segmentation
                or app/gradio/segmentation. This is a required parameter when
                space_config is set. This could be a local or GCS paths.
            space_sdk: Space Hub supports gradio, streamit, and static types
                of application. The default is set to gradio.
            placeholders: placeholders to replace in every files under the a
                pp_path. This is used to replace special string with the mod
                el related values. If this is not set, the default placehold
                ers will be used as follows.
                ```
                placeholders = {
                    ""MODEL_REPO_ID"" : ""$MODEL_REPO_ID"",
                    ""MODEL_REPO_URL"": ""$MODEL_REPO_URL"",
                    ""MODEL_VERSION"" : ""$MODEL_VERSION"",
                }
                ```
                In this case, ""$MODEL_REPO_ID"", ""$MODEL_REPO_URL"", ""$MODEL_VE
                RSION"" strings will be replaced with appropriate values at ru
                ntime. If placeholders are set, custom strings will be used.
            repo_name: the name of Space Hub repository where the application
                will be pushed. This should be unique name under the username
                within the Space Hub. repository is identified as {username}/
                {repo_name}. If this is not set, the same name to the Model H
                ub repository will be used.
        model: a TFX input channel containing a Model artifact. this is usually
            comes from the standard [`Trainer`]
            (https://www.tensorflow.org/tfx/guide/trainer) component.
        model_blessing: a TFX input channel containing a ModelBlessing artifact.
            this is usually comes from the standard [`Evaluator`]
            (https://www.tensorflow.org/tfx/guide/evaluator) component.
        Returns:
        a TFX output channel containing a PushedModel artifact. It contains
        information where the model is published at and whether the model is
        pushed or not.

        Raises:
            RuntimeError: if app_path is not set when space_config is provided.
        Example:

        Basic usage example:
        ```py
        trainer = Trainer(...)
        evaluator = Evaluator(...)
        hf_pusher = HFPusher(
            username=""chansung"",
            access_token=<YOUR-HUGGINGFACE-ACCESS-TOKEN>,
            repo_name=""my-model"",
            model=trainer.outputs[""model""],
            model_blessing=evaluator.outputs[""blessing""],
            space_config={
                ""app_path"": ""apps.gradio.semantic_segmentation""
            }
        )
        ```
        """"""

        pushed_model = types.Channel(type=standard_artifacts.PushedModel)

        spec = HFPusherSpec(
            username=username,
            access_token=access_token,
            repo_name=repo_name,
            space_config=space_config,
            model=model,
            model_blessing=model_blessing,
            pushed_model=pushed_model,
        )

        super().__init__(spec=spec)
"
0,mlops-hf-tf-vision-models,hf_integration/pipeline/components/HFPusher/component_test.py,"# Copyright 2022 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
""""""Tests for TFX HuggingFace Pusher Custom Component.""""""

import tensorflow as tf
from tfx.types import standard_artifacts
from tfx.types import channel_utils

from pipeline.components.HFPusher.component import HFPusher


class HFPusherTest(tf.test.TestCase):
    def testConstruct(self):
        test_model = channel_utils.as_channel([standard_artifacts.Model()])
        hf_pusher = HFPusher(
            username=""test_username"",
            access_token=""test_access_token"",
            repo_name=""test_repo_name"",
            model=test_model,
        )

        self.assertEqual(
            standard_artifacts.PushedModel.TYPE_NAME,
            hf_pusher.outputs[""pushed_model""].type_name,
        )


if __name__ == ""__main__"":
    tf.test.main()
"
0,mlops-hf-tf-vision-models,hf_integration/pipeline/components/HFPusher/executor.py,"# Copyright 2022 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
""""""HF Pusher TFX Component Executor. The HF Pusher Executor calls 
the workflow handler runner.deploy_model_for_hf_hub().
""""""

import ast
import time
from typing import Any, Dict, List

from tfx import types
from tfx.components.pusher import executor as tfx_pusher_executor
from tfx.types import artifact_utils, standard_component_specs

from pipeline.components.HFPusher import runner

_USERNAME_KEY = ""username""
_ACCESS_TOKEN_KEY = ""access_token""
_REPO_NAME_KEY = ""repo_name""
_SPACE_CONFIG_KEY = ""space_config""


class Executor(tfx_pusher_executor.Executor):
    """"""Pushes a model and an app to HuggingFace Model and Space Hubs respectively""""""

    def Do(
        self,
        input_dict: Dict[str, List[types.Artifact]],
        output_dict: Dict[str, List[types.Artifact]],
        exec_properties: Dict[str, Any],
    ):
        """"""Overrides the tfx_pusher_executor to leverage some of utility methods
        Args:
          input_dict: Input dict from input key to a list of artifacts, including:
            - model_export: a TFX input channel containing a Model artifact.
            - model_blessing: a TFX input channel containing a ModelBlessing
              artifact.
          output_dict: Output dict from key to a list of artifacts, including:
            - pushed_model: a TFX output channel containing a PushedModel arti
              fact. It contains information where the model is published at an
              d whether the model is pushed or not. furthermore, pushed model
              carries the following information.
              - pushed : integer value to denote if the model is pushed or not.
                This is set to 0 when the input model is not blessed, and it is
                set to 1 when the model is successfully pushed.
              - pushed_version : string value to indicate the current model ver
                sion. This is decided by time.time() Python built-in function.
              - repo_id : model repository ID where the model is pushed to. This
                follows the format of f""{username}/{repo_name}"".
              - branch : branch name where the model is pushed to. The branch na
                me is automatically assigned to the same value of pushed_version.
              - commit_id : the id from the commit history (branch name could be
                sufficient to retreive a certain version of the model) of the mo
                del repository.
              - repo_url : model repository URL. It is something like f""https://
                huggingface.co/{repo_id}/{branch}""
              - space_url : space repository URL. It is something like f""https://
                huggingface.co/{repo_id}""f
          exec_properties: An optional dict of execution properties, including:
            - username: username of the HuggingFace user (can be an individual
              user or an organization)
            - access_token: access token value issued by HuggingFace for the s
              pecified username.
            - repo_name: the repository name to push the current version of the
              model to. The default value is same as the TFX pipeline name.
            - space_config: space_config carries additional values such as:
              - app_path : path where the application templates are in the cont
                ainer that runs the TFX pipeline. This is expressed either apps.
                gradio.img_classifier or apps/gradio.img_classifier.
              - repo_name : the repository name to push the application to. The
                default value is same as the TFX pipeline name
              - space_sdk : either gradio or streamlit. this will decide which a
                pplication framework to be used for the Space repository. The de
                fault value is gradio
              - placeholders : dictionary which placeholders to replace with mod
                el specific information. The keys represents describtions, and t
                he values represents the actual placeholders to replace in the f
                iles under the app_path. There are currently two predefined keys,
                and if placeholders is set to None, the default values will be used.
        """"""
        self._log_startup(input_dict, output_dict, exec_properties)

        model_push = artifact_utils.get_single_instance(
            output_dict[standard_component_specs.PUSHED_MODEL_KEY]
        )

        # if the model is not blessed
        if not self.CheckBlessing(input_dict):
            self._MarkNotPushed(model_push)
            return
        model_path = self.GetModelPath(input_dict)
        model_version_name = f""v{int(time.time())}""

        space_config = exec_properties.get(_SPACE_CONFIG_KEY, None)
        if space_config is not None:
            space_config = ast.literal_eval(space_config)

        pushed_properties = runner.deploy_model_for_hf_hub(
            username=exec_properties.get(_USERNAME_KEY, None),
            access_token=exec_properties.get(_ACCESS_TOKEN_KEY, None),
            repo_name=exec_properties.get(_REPO_NAME_KEY, None),
            space_config=space_config,
            model_path=model_path,
            model_version=model_version_name,
        )

        self._MarkPushed(model_push, pushed_destination=pushed_properties[""repo_url""])
        for key in pushed_properties:
            value = pushed_properties[key]

            if key != ""repo_url"":
                model_push.set_string_custom_property(key, value)
"
0,mlops-hf-tf-vision-models,hf_integration/pipeline/components/HFPusher/runner.py,"# Copyright 2022 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
""""""HuggingFace Pusher runner module.
This module handles the workflow to publish
machine learning model to HuggingFace Hub.
""""""
from typing import Text, Any, Dict, Optional

import tempfile
import tensorflow as tf
from absl import logging
from tfx.utils import io_utils

from huggingface_hub import Repository
from huggingface_hub import HfApi
from requests.exceptions import HTTPError

_MODEL_REPO_KEY = ""MODEL_REPO_ID""
_MODEL_URL_KEY = ""MODEL_REPO_URL""
_MODEL_VERSION_KEY = ""MODEL_VERSION""

_DEFAULT_MODEL_REPO_PLACEHOLDER_KEY = ""$MODEL_REPO_ID""
_DEFAULT_MODEL_URL_PLACEHOLDER_KEY = ""$MODEL_REPO_URL""
_DEFAULT_MODEL_VERSION_PLACEHOLDER_KEY = ""$MODEL_VERSION""


def _replace_placeholders_in_files(
    root_dir: str, placeholder_to_replace: Dict[str, str]
):
    """"""Recursively open every files under the root_dir, and then
    replace special tokens with the given values in placeholder_
    to_replace""""""
    files = tf.io.gfile.listdir(root_dir)
    for file in files:
        path = tf.io.gfile.join(root_dir, file)

        if tf.io.gfile.isdir(path):
            _replace_placeholders_in_files(path, placeholder_to_replace)
        else:
            _replace_placeholders_in_file(path, placeholder_to_replace)


def _replace_placeholders_in_file(
    filepath: str, placeholder_to_replace: Dict[str, str]
):
    """"""replace special tokens with the given values in placeholder_
    to_replace. This function gets called by _replace_placeholders
    _in_files function""""""
    with tf.io.gfile.GFile(filepath, ""r"") as f:
        source_code = f.read()

    for placeholder in placeholder_to_replace:
        source_code = source_code.replace(
            placeholder, placeholder_to_replace[placeholder]
        )

    with tf.io.gfile.GFile(filepath, ""w"") as f:
        f.write(source_code)


def _replace_placeholders(
    target_dir: str,
    placeholders: Dict[str, str],
    model_repo_id: str,
    model_repo_url: str,
    model_version: str,
):
    """"""set placeholder_to_replace before calling _replace_placeholde
    rs_in_files function""""""

    if placeholders is None:
        placeholders = {
            _MODEL_REPO_KEY: _DEFAULT_MODEL_REPO_PLACEHOLDER_KEY,
            _MODEL_URL_KEY: _DEFAULT_MODEL_URL_PLACEHOLDER_KEY,
            _MODEL_VERSION_KEY: _DEFAULT_MODEL_VERSION_PLACEHOLDER_KEY,
        }

    placeholder_to_replace = {
        placeholders[_MODEL_REPO_KEY]: model_repo_id,
        placeholders[_MODEL_URL_KEY]: model_repo_url,
        placeholders[_MODEL_VERSION_KEY]: model_version,
    }
    _replace_placeholders_in_files(target_dir, placeholder_to_replace)


def _replace_files(src_path, dst_path):
    """"""replace the contents(files/folders) of the repository with the
    latest contents""""""

    not_to_delete = ["".gitattributes"", "".git""]

    inside_root_dst_path = tf.io.gfile.listdir(dst_path)

    for content_name in inside_root_dst_path:
        content = f""{dst_path}/{content_name}""

        if content_name not in not_to_delete:
            if tf.io.gfile.isdir(content):
                tf.io.gfile.rmtree(content)
            else:
                tf.io.gfile.remove(content)

    inside_root_src_path = tf.io.gfile.listdir(src_path)

    for content_name in inside_root_src_path:
        content = f""{src_path}/{content_name}""
        dst_content = f""{dst_path}/{content_name}""

        if tf.io.gfile.isdir(content):
            io_utils.copy_dir(content, dst_content)
        else:
            tf.io.gfile.copy(content, dst_content)


def _create_remote_repo(
    access_token: str, repo_id: str, repo_type: str = ""model"", space_sdk: str = None
):
    """"""create a remote repository on HuggingFace Hub platform. HTTPError
    exception is raised when the repository already exists""""""

    logging.info(f""repo_id: {repo_id}"")
    try:
        HfApi().create_repo(
            token=access_token,
            repo_id=repo_id,
            repo_type=repo_type,
            space_sdk=space_sdk,
        )
    except HTTPError:
        logging.warning(
            f""this warning is expected if {repo_id} repository already exists""
        )


def _clone_and_checkout(
    repo_url: str, local_path: str, access_token: str, version: Optional[str] = None
) -> Repository:
    """"""clone the remote repository to the given local_path""""""

    repository = Repository(
        local_dir=local_path, clone_from=repo_url, use_auth_token=access_token
    )

    if version is not None:
        repository.git_checkout(revision=version, create_branch_ok=True)

    return repository


def _push_to_remote_repo(repo: Repository, commit_msg: str, branch: str = ""main""):
    """"""push any changes to the remote repository""""""

    repo.git_add(pattern=""."", auto_lfs_track=True)
    repo.git_commit(commit_message=commit_msg)
    repo.git_push(upstream=f""origin {branch}"")


def deploy_model_for_hf_hub(
    username: str,
    access_token: str,
    repo_name: str,
    model_path: str,
    model_version: str,
    space_config: Optional[Dict[Text, Any]] = None,
) -> Dict[str, str]:
    """"""Executes ML model deployment workflow to HuggingFace Hub. Refer to the
    HFPusher component in component.py for generic description of each parame
    ter. This docstring only explains how the workflow works.

    step 1. push model to the Model Hub
    step 1-1.
        create a repository on the HuggingFace Hub. if there is an existing r
        epository with the given repo_name, that rpository will be overwritten.
    step 1-2.
        clone the created or existing remote repository to the local path. Al
        so, create a branch named with model version.
    step 1-3.
        remove every files under the cloned repository(local), and copies the
        model related files to the cloned local repository path.
    step 1-4.
        push the updated repository to the given branch of remote Model Hub.

    step 2. push application to the Space Hub
    step 2-1.
        create a repository on the HuggingFace Hub. if there is an existing r
        epository with the given repo_name, that rpository will be overwritten.
    step 2-2.
        copies directory where the application related files are stored to a
        temporary directory. Since the files could be hosted in GCS bucket, t
        his process ensures every necessary files are located in the local fil
        e system.
    step 2-3.
        replacek speical tokens in every files under the given directory.
    step 2-4.
        clone the created or existing remote repository to the local path.
    step 2-5.
        remove every files under the cloned repository(local), and copies the
        application related files to the cloned local repository path.
    step 2-6.
        push the updated repository to the remote Space Hub. note that the br
        anch is always set to ""main"", so that HuggingFace Space could build t
        he application automatically when pushed.
    """"""
    outputs = {}

    # step 1
    repo_url_prefix = ""https://huggingface.co""
    repo_id = f""{username}/{repo_name}""
    repo_url = f""{repo_url_prefix}/{repo_id}""

    # step 1-1
    _create_remote_repo(access_token=access_token, repo_id=repo_id)
    logging.info(f""remote repository at {repo_url} is prepared"")

    # step 1-2
    local_path = ""hf_model""
    repository = _clone_and_checkout(
        repo_url=repo_url,
        local_path=local_path,
        access_token=access_token,
        version=model_version,
    )
    logging.info(
        f""remote repository is cloned, and new branch {model_version} is created""
    )

    # step 1-3
    _replace_files(model_path, local_path)
    logging.info(
        ""current version of the model is copied to the cloned local repository""
    )

    # step 1-4
    _push_to_remote_repo(
        repo=repository,
        commit_msg=f""updload new version({model_version})"",
        branch=model_version,
    )
    logging.info(""updates are pushed to the remote repository"")

    outputs[""repo_id""] = repo_id
    outputs[""branch""] = model_version
    outputs[""commit_id""] = f""{repository.git_head_hash()}""
    outputs[""repo_url""] = repo_url

    # step 2
    if space_config is not None:
        if ""app_path"" not in space_config:
            raise RuntimeError(
                f""the app_path is not provided. ""
                f""app_path is required when space_config is set.""
            )

        model_repo_id = repo_id
        model_repo_url = repo_url

        if ""repo_name"" in space_config:
            repo_id = f""{username}/{repo_name}""
            repo_url = f""{repo_url_prefix}/{repo_id}""
        else:
            repo_url = f""{repo_url_prefix}/spaces/{repo_id}""
        app_path = space_config[""app_path""]
        app_path = app_path.replace(""."", ""/"")

        # step 2-1
        _create_remote_repo(
            access_token=access_token,
            repo_id=repo_id,
            repo_type=""space"",
            space_sdk=space_config[""space_sdk""]
            if ""space_sdk"" in space_config
            else ""gradio"",
        )

        # step 2-2
        tmp_dir = tempfile.gettempdir()
        io_utils.copy_dir(app_path, tmp_dir)

        # step 2-3
        _replace_placeholders(
            target_dir=tmp_dir,
            placeholders=space_config[""placeholders""]
            if ""placeholders"" in space_config
            else None,
            model_repo_id=model_repo_id,
            model_repo_url=model_repo_url,
            model_version=model_version,
        )

        # step 2-4
        local_path = ""hf_space""
        repository = _clone_and_checkout(
            repo_url=repo_url,
            local_path=local_path,
            access_token=access_token,
        )

        # step 2-5
        _replace_files(tmp_dir, local_path)

        # step 2-6
        _push_to_remote_repo(
            repo=repository,
            commit_msg=f""upload {model_version} model"",
        )

        outputs[""space_url""] = repo_url

    return outputs
"
0,semantic-segmentation-ml-pipeline,notebooks/gradio_demo_pets.ipynb,"!pip install -q gradio huggingface_hubfrom huggingface_hub import from_pretrained_keras


model_ckpt = ""chansung/segmentation-training-pipeline@v1667662600""

MODEL = from_pretrained_keras(model_ckpt)!wget https://i.ibb.co/p44PvmN/test-image1.png -O test-image1.png
!wget https://i.ibb.co/brJnX3m/test-image2.png -O test-image2.pngfrom PIL import Image 
import numpy as np 
import tensorflow as tf 


RESOLTUION = 128
# MODEL = tf.keras.models.load_model(model_path)

def preprocess_input(image: Image) -> tf.Tensor:
    image = np.array(image)
    image = tf.convert_to_tensor(image)

    image = tf.image.resize(image, (RESOLTUION, RESOLTUION))
    image = image / 255

    return tf.expand_dims(image, 0)


# The below utilities (sidewalk_palette(), get_seg_overlay()) are from:
# https://github.com/deep-diver/semantic-segmentation-ml-pipeline/blob/main/notebooks/inference_from_SavedModel.ipynb

def pets_palette():
    """"""Pets palette that maps each class to RGB values.""""""
    return [
        [  0,  10, 146],
        [ 38,   0,  44],
        [255, 232,   0],
    ]


def get_seg_overlay(image, seg):
    color_seg = np.zeros(
        (seg.shape[0], seg.shape[1], 3), dtype=np.uint8
    )  # height, width, 3
    palette = np.array(pets_palette())
    
    for label, color in enumerate(palette):
        color_seg[seg == label, :] = color

    # Show image + mask
    img = np.array(image) * 0.5 + color_seg * 0.5

    img *= 255
    img = np.clip(img, 0, 255)
    img = img.astype(np.uint8)
    return img


def run_model(image: Image) -> tf.Tensor:
    preprocessed_image = preprocess_input(image)
    prediction = MODEL.predict(preprocessed_image)

    seg_mask = tf.math.argmax(prediction, -1)
    seg_mask = tf.squeeze(seg_mask)
    return seg_mask


def get_predictions(image: Image):
    predicted_segmentation_mask = run_model(image)
    preprocessed_image = preprocess_input(image)
    preprocessed_image = tf.squeeze(preprocessed_image, 0)
    
    pred_img = get_seg_overlay(preprocessed_image.numpy(), predicted_segmentation_mask.numpy())
    return Image.fromarray(pred_img)import gradio as gr

title = ""Simple demo for a semantic segmentation model trained on the Sidewalks dataset.""

description = """"""

Note that the outputs obtained in this demo won't be state-of-the-art. The underlying project has a different objective focusing more on the ops side of
deploying a semantic segmentation model. For more details, check out the repository: https://github.com/deep-diver/semantic-segmentation-ml-pipeline/.

""""""

demo = gr.Interface(
    get_predictions,
    gr.inputs.Image(type=""pil""),
    ""pil"",
    allow_flagging=""never"",
    title=title,
    description=description,
    examples=[[""test-image1.png""], [""test-image2.png""]]
)

demo.launch(debug=True)"
0,semantic-segmentation-ml-pipeline,notebooks/inference_SavedModel_VertexEndpoint.ipynb,"!mkdir ./tmp
!gsutil -m cp -r \
  ""gs://gcp-ml-172005-complete-mlops/tfx_pipeline_output/segmentation-training-pipeline/874401645461/segmentation-training-pipeline-20221107074235/Trainer_-273598625165082624/model/Format-Serving"" \
  ./tmpmodel_path = ""model""

!mkdir {model_path}
!cp -r tmp/Format-Serving/* {model_path}/!saved_model_cli show --dir {model_path} --tag_set serve --signature_def serving_defaultimport tensorflow as tf

model = tf.keras.models.load_model(model_path)test_image_path = tf.keras.utils.get_file(
    ""test-image.png"", ""https://i.ibb.co/F58NjRq/test-image.png""
)

test_label_path = tf.keras.utils.get_file(
    ""gt-image.png"", ""https://i.ibb.co/BjPbS6c/gt-image.png""
)import numpy as np
from tensorflow.keras.applications import mobilenet_v2

with open(test_image_path, ""rb"") as f:
    test_image = f.read()

with open(test_label_path, ""rb"") as f:
    gt_image = f.read()

test_image = tf.io.decode_png(test_image, channels=3)
test_image = tf.image.resize(test_image, size=(128, 128))
test_image = mobilenet_v2.preprocess_input(test_image)
test_gt = tf.io.decode_png(gt_image, channels=1)

input_numpy = np.array(test_image)
input_tensor = tf.convert_to_tensor(input_numpy)
input_tensor = input_tensor[tf.newaxis, ...]prediction = model.predict(input_tensor)
print(prediction.shape)

seg_mask = tf.math.argmax(prediction, -1)
seg_mask = tf.squeeze(seg_mask)
seg_maskimport matplotlib.pyplot as plt

f, axs = plt.subplots(1, 2)
f.set_figheight(10)
f.set_figwidth(20)

axs[0].set_title(""Prediction"", {""fontsize"": 40})
axs[0].imshow(seg_mask)
axs[0].axis(""off"")
axs[1].set_title(""Ground truth"", {""fontsize"": 40})
axs[1].imshow(tf.squeeze(test_gt, axis=2))
axs[1].axis(""off"")

plt.show()import google.auth # pip install -U google-auth
from google.auth.transport.requests import AuthorizedSession

import tensorflow as tf 
import base64
import jsonPROJECT_ID = ""gcp-ml-172005""
REGION = ""us-central1""
ENDPOINT_ID = ""2220630858061053952""!gcloud auth application-default logincredentials, _ = google.auth.default()
service_endpoint = f""https://{REGION}-aiplatform.googleapis.com""
authed_session = AuthorizedSession(credentials)url = ""{}/v1/projects/{}/locations/{}/endpoints/{}:predict"".format(
    service_endpoint, PROJECT_ID, REGION, ENDPOINT_ID
)
print(""Endpoint: "", url)serving_input = list(
    model.signatures[""serving_default""].structured_input_signature[1].keys()
)[0]
print(""Serving function input:"", serving_input)with open(test_image_path, ""rb"") as f:
    image = f.read()

b64str = base64.urlsafe_b64encode(image).decode('utf-8')single_instance_request_body = {
    ""instances"": [
        {serving_input: b64str}
    ]
}

two_instances_request_body = {
    ""instances"": [
        {serving_input: b64str},
        {serving_input: b64str},
    ]
}response = authed_session.post(url, data=json.dumps(single_instance_request_body))
print(response)
print(response.content)import ast

response = response.content.decode('utf-8')
response = ast.literal_eval(response)

seg_mask_ve = tf.convert_to_tensor(response['predictions'])
seg_mask_ve = tf.squeeze(seg_mask_ve)
seg_mask_veimport matplotlib.pyplot as plt

f, axs = plt.subplots(1, 3)
f.set_figheight(10)
f.set_figwidth(30)

axs[0].set_title(""Prediction(local)"", {""fontsize"": 40})
axs[0].imshow(seg_mask)
axs[0].axis(""off"")
axs[1].set_title(""Prediction(vertex endpoint)"", {""fontsize"": 40})
axs[1].imshow(seg_mask_ve)
axs[1].axis(""off"")
axs[2].set_title(""Ground truth"", {""fontsize"": 40})
axs[2].imshow(tf.squeeze(test_gt, axis=2))
axs[2].axis(""off"")

plt.show()import numpy as np
np.testing.assert_allclose(seg_mask, seg_mask_ve)response = authed_session.post(url, data=json.dumps(two_instances_request_body))
print(response)
print(response.content)response = response.content.decode('utf-8')
response = ast.literal_eval(response)

seg_mask = tf.convert_to_tensor(response['predictions'])
seg_mask = tf.squeeze(seg_mask)
seg_mask"
0,semantic-segmentation-ml-pipeline,notebooks/parse_tfrecords_pets.ipynb,"import tensorflow as tfGCS_PATH = ""gs://pets-tfrecords/pets-tfrecords""
BATCH_SIZE = 4
AUTO = tf.data.AUTOTUNEdef parse_tfr(proto):
    feature_description = {
        ""image"": tf.io.VarLenFeature(tf.float32),
        ""image_shape"": tf.io.VarLenFeature(tf.int64),
        ""label"": tf.io.VarLenFeature(tf.float32),
        ""label_shape"": tf.io.VarLenFeature(tf.int64),
    }
    rec = tf.io.parse_single_example(proto, feature_description)
    image_shape = tf.sparse.to_dense(rec[""image_shape""])
    image = tf.reshape(tf.sparse.to_dense(rec[""image""]), image_shape)
    label_shape = tf.sparse.to_dense(rec[""label_shape""])
    label = tf.reshape(tf.sparse.to_dense(rec[""label""]), label_shape)
    return {""pixel_values"": image, ""label"": label}


def prepare_dataset(GCS_PATH=GCS_PATH, split=""train"", batch_size=BATCH_SIZE):
    if split not in [""train"", ""val""]:
        raise ValueError(
            ""Invalid split provided. Supports splits are: `train` and `val`.""
        )

    dataset = tf.data.TFRecordDataset(
        [filename for filename in tf.io.gfile.glob(f""{GCS_PATH}/{split}-*"")],
        num_parallel_reads=AUTO,
    ).map(parse_tfr, num_parallel_calls=AUTO)

    if split == ""train"":
        dataset = dataset.shuffle(batch_size * 2)

    dataset = dataset.batch(batch_size)
    dataset = dataset.prefetch(AUTO)
    return datasettrain_dataset = prepare_dataset()
val_dataset = prepare_dataset(split=""val"")for batch in train_dataset.take(1):
    print(batch[""pixel_values""].shape, batch[""label""].shape)for batch in val_dataset.take(1):
    print(batch[""pixel_values""].shape, batch[""label""].shape)"
0,semantic-segmentation-ml-pipeline,notebooks/parse_tfrecords_sidewalks.ipynb,"import tensorflow as tfGCS_PATH_FULL_RESOUTION = ""gs://sidewalks-tfx-fullres/sidewalks-tfrecords""
GCS_PATH_LOW_RESOLUTION = ""gs://sidewalks-tfx-lowres/sidewalks-tfrecords""
BATCH_SIZE = 4
AUTO = tf.data.AUTOTUNEdef parse_tfr(proto):
    feature_description = {
        ""image"": tf.io.VarLenFeature(tf.float32),
        ""image_shape"": tf.io.VarLenFeature(tf.int64),
        ""label"": tf.io.VarLenFeature(tf.float32),
        ""label_shape"": tf.io.VarLenFeature(tf.int64),
    }
    rec = tf.io.parse_single_example(proto, feature_description)
    image_shape = tf.sparse.to_dense(rec[""image_shape""])
    image = tf.reshape(tf.sparse.to_dense(rec[""image""]), image_shape)
    label_shape = tf.sparse.to_dense(rec[""label_shape""])
    label = tf.reshape(tf.sparse.to_dense(rec[""label""]), label_shape)
    return {""pixel_values"": image, ""label"": label}


def prepare_dataset(GCS_PATH=GCS_PATH_FULL_RESOUTION, split=""train"", batch_size=BATCH_SIZE):
    if split not in [""train"", ""val""]:
        raise ValueError(
            ""Invalid split provided. Supports splits are: `train` and `val`.""
        )

    dataset = tf.data.TFRecordDataset(
        [filename for filename in tf.io.gfile.glob(f""{GCS_PATH}/{split}-*"")],
        num_parallel_reads=AUTO,
    ).map(parse_tfr, num_parallel_calls=AUTO)

    if split == ""train"":
        dataset = dataset.shuffle(batch_size * 2)

    dataset = dataset.batch(batch_size)
    dataset = dataset.prefetch(AUTO)
    return datasettrain_dataset = prepare_dataset()
val_dataset = prepare_dataset(split=""val"")for batch in train_dataset.take(1):
    print(batch[""pixel_values""].shape, batch[""label""].shape)for batch in val_dataset.take(1):
    print(batch[""pixel_values""].shape, batch[""label""].shape)train_dataset = prepare_dataset(GCS_PATH_LOW_RESOLUTION)
val_dataset = prepare_dataset(GCS_PATH_LOW_RESOLUTION, split=""val"")for batch in train_dataset.take(1):
    print(batch[""pixel_values""].shape, batch[""label""].shape)for batch in val_dataset.take(1):
    print(batch[""pixel_values""].shape, batch[""label""].shape)"
0,semantic-segmentation-ml-pipeline,notebooks/tfx_pipeline_pets.ipynb,"!pip install tfxdata_path = ""gs://sidewalks-tfx-lowres/sidewalks-tfrecords""
local_data_path = ""data""
model_file = ""modules/model.py""
model_fn = ""modules.model.run_fn""
preprocessing_file = ""modules/preprocessing.py""
preprocessing_fn = ""modules.preprocessing.preprocessing_fn""!mkdir {local_data_path}
!gsutil cp {data_path}/*-00-*.tfrec {local_data_path}/import tfx
tfx.__version__from tfx import v1 as tfx
from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext
from tfx.components import ImportExampleGen
from tfx.components import SchemaGen
from tfx.components import StatisticsGen
from tfx.components import Trainer
from tfx.components import Transform
from tfx.components import Evaluator
from tfx.proto import example_gen_pb2

from tfx.types import Channel
from tfx.types.standard_artifacts import Model
from tfx.types.standard_artifacts import ModelBlessing
from tfx.dsl.components.common import resolver
from tfx.dsl.experimental.latest_blessed_model_resolver import LatestBlessedModelResolver

import tensorflow_model_analysis as tfmacontext = InteractiveContext()input_config = example_gen_pb2.Input(
    splits=[
        example_gen_pb2.Input.Split(name=""train"", pattern=""train-*.tfrec""),
        example_gen_pb2.Input.Split(name=""eval"", pattern=""val-*.tfrec""),
    ]
)
example_gen = ImportExampleGen(
    input_base=local_data_path, 
    input_config=input_config
)context.run(example_gen)statistics_gen = StatisticsGen(examples=example_gen.outputs[""examples""])context.run(statistics_gen)context.show(statistics_gen.outputs['statistics'])schema_gen = SchemaGen(statistics=statistics_gen.outputs['statistics'])context.run(schema_gen)!mkdir modules%%writefile {preprocessing_file}

import tensorflow as tf
from tensorflow.keras.applications import mobilenet_v2

_INPUT_IMG_SIZE = 128

_IMAGE_KEY = ""image""
_IMAGE_SHAPE_KEY = ""image_shape""
_LABEL_KEY = ""label""
_LABEL_SHAPE_KEY = ""label_shape""


def _transformed_name(key: str) -> str:
    return key + ""_xf""


# output should have the same keys as inputs
def preprocess(inputs):
    image_shape = inputs[_IMAGE_SHAPE_KEY]
    label_shape = inputs[_LABEL_SHAPE_KEY]
    
    images = tf.reshape(inputs[_IMAGE_KEY], [image_shape[0], image_shape[1], 3])
    labels = tf.reshape(inputs[_LABEL_KEY], [label_shape[0], label_shape[1], 1])

    return {
        _IMAGE_KEY: images,
        _IMAGE_SHAPE_KEY: inputs[_IMAGE_SHAPE_KEY],
        _LABEL_KEY: labels,
        _LABEL_SHAPE_KEY: inputs[_LABEL_SHAPE_KEY],
    }


def preprocessing_fn(inputs):
    """"""tf.transform's callback function for preprocessing inputs.
    Args:
      inputs: map from feature keys to raw not-yet-transformed features.
    Returns:
      Map from string feature key to transformed feature operations.
    """"""
    # print(inputs)
    outputs = {}

    features = tf.map_fn(preprocess, inputs)

    features[_IMAGE_KEY] = tf.image.resize(features[_IMAGE_KEY], [_INPUT_IMG_SIZE, _INPUT_IMG_SIZE])
    features[_LABEL_KEY] = tf.image.resize(features[_LABEL_KEY], [_INPUT_IMG_SIZE, _INPUT_IMG_SIZE])

    image_features = mobilenet_v2.preprocess_input(features[_IMAGE_KEY])

    outputs[_transformed_name(_IMAGE_KEY)] = image_features
    outputs[_transformed_name(_LABEL_KEY)] = features[_LABEL_KEY]

    return outputstransform = Transform(
    examples=example_gen.outputs[""examples""],
    schema=schema_gen.outputs[""schema""],
    preprocessing_fn=preprocessing_fn,
)context.run(transform)%%writefile {model_file}

from typing import List, Dict, Tuple

import absl
import tensorflow as tf
import tensorflow_transform as tft
from tensorflow.keras.optimizers import Adam
from tfx_bsl.tfxio import dataset_options
from tfx.components.trainer.fn_args_utils import FnArgs
from tfx.components.trainer.fn_args_utils import DataAccessor

_CONCRETE_INPUT = ""pixel_values""
_RAW_IMG_SIZE = 256
_INPUT_IMG_SIZE = 128
_TRAIN_LENGTH = 800
_EVAL_LENGTH = 200
_TRAIN_BATCH_SIZE = 64
_EVAL_BATCH_SIZE = 64
_EPOCHS = 1
_LR = 0.00006

_IMAGE_KEY = ""image""
_LABEL_KEY = ""label""

def INFO(text: str):
    absl.logging.info(text)

def _transformed_name(key: str) -> str:
    return key + ""_xf""

""""""
    _serving_preprocess, _serving_preprocess_fn, and 
    _model_exporter functions are defined to provide pre-
    processing capabilities when the model is served.
""""""


def _serving_preprocess(string_input):
    decoded_input = tf.io.decode_base64(string_input)
    decoded = tf.io.decode_jpeg(decoded_input, channels=3)
    decoded = decoded / 255
    resized = tf.image.resize(decoded, size=(_INPUT_IMG_SIZE, _INPUT_IMG_SIZE))
    return resized


@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])
def _serving_preprocess_fn(string_input):
    decoded_images = tf.map_fn(
        _serving_preprocess, string_input, dtype=tf.float32, back_prop=False
    )
    return {_CONCRETE_INPUT: decoded_images}


def _model_exporter(model: tf.keras.Model):
    m_call = tf.function(model.call).get_concrete_function(
        tf.TensorSpec(shape=[None, _INPUT_IMG_SIZE, _INPUT_IMG_SIZE, 3], dtype=tf.float32, name=_CONCRETE_INPUT)
    )

    @tf.function(input_signature=[tf.TensorSpec([None], tf.string)])
    def serving_fn(string_input):
        images = _serving_preprocess_fn(string_input)
        logits = m_call(**images)
        seg_mask = tf.math.argmax(logits, -1)
        return {""seg_mask"": seg_mask}

    return serving_fn

def _get_transform_features_signature(model, tf_transform_output):
    # the layer is added as an attribute to the model in order to make sure that
    # the model assets are handled correctly when exporting.    
    model.tft_layer = tf_transform_output.transform_features_layer()

    @tf.function(
        input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string, name=""examples"")]
    )
    def serve_tf_examples_fn(serialized_tf_examples):
        """"""Returns the output to be used in the serving signature.""""""
        feature_spec = tf_transform_output.raw_feature_spec()
        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)

        transformed_features = model.tft_layer(parsed_features)

        return transformed_features
        
    return serve_tf_examples_fn

def _get_tf_examples_serving_signature(model, tf_transform_output):
    """"""
    Returns a serving signature that accepts `tensorflow.Example`.

    This signature will be used for evaluation or bulk inference.
    """"""
    
    model.tft_layer = tf_transform_output.transform_features_layer()

    @tf.function(
        input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string, name=""examples"")]
    )
    def serve_tf_examples_fn(
        serialized_tf_example: tf.Tensor,
    ) -> Dict[str, tf.Tensor]:
        """"""Returns the output to be used in the serving signature.""""""
        # Load the schema of raw examples.
        raw_feature_spec = tf_transform_output.raw_feature_spec()
        # Remove label feature since these will not be present at serving time.
        # Parse the examples using schema into raw features
        raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)

        # Preprocess the raw features
        transformed_features = model.tft_layer(raw_features)

        # Run preprocessed inputs through the model to get the prediction
        outputs = model(transformed_features)

        return {
            _transformed_name(_LABEL_KEY): outputs
        }

    return serve_tf_examples_fn


""""""
    _input_fn reads TFRecord files passed from the upstream 
    TFX component, Transform. Assume the dataset is already 
    transformed appropriately. 
""""""

def _input_fn(
    file_pattern: List[str],
    data_accessor: DataAccessor,
    tf_transform_output: tft.TFTransformOutput,
    is_train: bool = False,
    batch_size: int = 200,
) -> tf.data.Dataset:
    dataset = data_accessor.tf_dataset_factory(
        file_pattern,
        dataset_options.TensorFlowDatasetOptions(
            batch_size=batch_size, label_key=_transformed_name(_LABEL_KEY)
        ),
        tf_transform_output.transformed_metadata.schema,
    )

    return dataset


def _build_model(num_labels) -> tf.keras.Model:
    base_model = tf.keras.applications.MobileNetV2(
        input_shape=[128, 128, 3], include_top=False
    )

    # Use the activations of these layers
    layer_names = [
        ""block_1_expand_relu"",  # 64x64
        ""block_3_expand_relu"",  # 32x32
        ""block_6_expand_relu"",  # 16x16
        ""block_13_expand_relu"",  # 8x8
        ""block_16_project"",  # 4x4
    ]
    base_model_outputs = [base_model.get_layer(name).output for name in layer_names]

    # Create the feature extraction model
    down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)
    down_stack.trainable = False

    up_stack = [
        upsample(512, 3),  # 4x4 -> 8x8
        upsample(256, 3),  # 8x8 -> 16x16
        upsample(128, 3),  # 16x16 -> 32x32
        upsample(64, 3),  # 32x32 -> 64x64
    ]

    inputs = tf.keras.layers.Input(
        shape=[128, 128, 3], name=_transformed_name(_IMAGE_KEY)
    )

    # Downsampling through the model
    skips = down_stack(inputs)
    x = skips[-1]
    skips = reversed(skips[:-1])

    # Upsampling and establishing the skip connections
    for up, skip in zip(up_stack, skips):
        x = up(x)
        concat = tf.keras.layers.Concatenate()
        x = concat([x, skip])

    # This is the last layer of the model
    last = tf.keras.layers.Conv2DTranspose(
        filters=num_labels, kernel_size=3, strides=2, padding=""same"", name=_transformed_name(_LABEL_KEY)
    )  # 64x64 -> 128x128

    x = last(x)

    model = tf.keras.Model(inputs=inputs, outputs=x)
    model.compile(
        optimizer=Adam(learning_rate=_LR),
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=[""sparse_categorical_accuracy""],
    )
    return model


""""""
    InstanceNormalization class and upsample function are
    borrowed from pix2pix in [TensorFlow Example repository](
    https://github.com/tensorflow/examples/tree/master/tensorflow_examples/models/pix2pix)
""""""


class InstanceNormalization(tf.keras.layers.Layer):
    """"""Instance Normalization Layer (https://arxiv.org/abs/1607.08022).""""""

    def __init__(self, epsilon=1e-5):
        super(InstanceNormalization, self).__init__()
        self.epsilon = epsilon

    def build(self, input_shape):
        self.scale = self.add_weight(
            name=""scale"",
            shape=input_shape[-1:],
            initializer=tf.random_normal_initializer(1.0, 0.02),
            trainable=True,
        )

        self.offset = self.add_weight(
            name=""offset"", shape=input_shape[-1:], initializer=""zeros"", trainable=True
        )

    def call(self, x):
        mean, variance = tf.nn.moments(x, axes=[1, 2], keepdims=True)
        inv = tf.math.rsqrt(variance + self.epsilon)
        normalized = (x - mean) * inv
        return self.scale * normalized + self.offset


def upsample(filters, size, norm_type=""batchnorm"", apply_dropout=False):
    """"""Upsamples an input.
    Conv2DTranspose => Batchnorm => Dropout => Relu
    Args:
      filters: number of filters
      size: filter size
      norm_type: Normalization type; either 'batchnorm' or 'instancenorm'.
      apply_dropout: If True, adds the dropout layer
    Returns:
      Upsample Sequential Model
    """"""

    initializer = tf.random_normal_initializer(0.0, 0.02)

    result = tf.keras.Sequential()
    result.add(
        tf.keras.layers.Conv2DTranspose(
            filters,
            size,
            strides=2,
            padding=""same"",
            kernel_initializer=initializer,
            use_bias=False,
        )
    )

    if norm_type.lower() == ""batchnorm"":
        result.add(tf.keras.layers.BatchNormalization())
    elif norm_type.lower() == ""instancenorm"":
        result.add(InstanceNormalization())

    if apply_dropout:
        result.add(tf.keras.layers.Dropout(0.5))

    result.add(tf.keras.layers.ReLU())

    return result


def run_fn(fn_args: FnArgs):
    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)

    train_dataset = _input_fn(
        fn_args.train_files,
        fn_args.data_accessor,
        tf_transform_output,
        is_train=True,
        batch_size=_TRAIN_BATCH_SIZE,
    )

    eval_dataset = _input_fn(
        fn_args.eval_files,
        fn_args.data_accessor,
        tf_transform_output,        
        is_train=False,
        batch_size=_EVAL_BATCH_SIZE,
    )

    num_labels = 35
    model = _build_model(num_labels)

    model.fit(
        train_dataset,
        steps_per_epoch=_TRAIN_LENGTH // _TRAIN_BATCH_SIZE,
        validation_data=eval_dataset,
        validation_steps=_EVAL_LENGTH // _TRAIN_BATCH_SIZE,
        epochs=_EPOCHS,
    )

    model.save(
        fn_args.serving_model_dir, save_format=""tf"", signatures={
            ""serving_default"": _model_exporter(model),
            ""transform_features"": _get_transform_features_signature(
                model, tf_transform_output
            ),
            ""from_examples"": _get_tf_examples_serving_signature(
                model, tf_transform_output
            ),            
        }
    )
trainer = Trainer(
    run_fn=model_fn,
    transformed_examples=transform.outputs[""transformed_examples""],
    transform_graph=transform.outputs[""transform_graph""],
    schema=schema_gen.outputs[""schema""],
)context.run(trainer)"
0,semantic-segmentation-ml-pipeline,notebooks/tfx_pipeline_sidewalks.ipynb,"!pip install tfxdata_path = ""gs://sidewalks-tfx-lowres/sidewalks-tfrecords""
local_data_path = ""data""
model_file = ""modules/model.py""
model_fn = ""modules.model.run_fn""
preprocessing_file = ""modules/preprocessing.py""
preprocessing_fn = ""modules.preprocessing.preprocessing_fn""!mkdir {local_data_path}
!gsutil cp {data_path}/*-00-*.tfrec {local_data_path}/import tfx
tfx.__version__from tfx import v1 as tfx
from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext
from tfx.components import ImportExampleGen
from tfx.components import SchemaGen
from tfx.components import StatisticsGen
from tfx.components import Trainer
from tfx.components import Transform
from tfx.components import Evaluator
from tfx.proto import example_gen_pb2

from tfx.types import Channel
from tfx.types.standard_artifacts import Model
from tfx.types.standard_artifacts import ModelBlessing
from tfx.dsl.components.common import resolver
from tfx.dsl.experimental.latest_blessed_model_resolver import LatestBlessedModelResolver

import tensorflow_model_analysis as tfmacontext = InteractiveContext()input_config = example_gen_pb2.Input(
    splits=[
        example_gen_pb2.Input.Split(name=""train"", pattern=""train-*.tfrec""),
        example_gen_pb2.Input.Split(name=""eval"", pattern=""val-*.tfrec""),
    ]
)
example_gen = ImportExampleGen(
    input_base=local_data_path, 
    input_config=input_config
)context.run(example_gen)statistics_gen = StatisticsGen(examples=example_gen.outputs[""examples""])context.run(statistics_gen)context.show(statistics_gen.outputs['statistics'])schema_gen = SchemaGen(statistics=statistics_gen.outputs['statistics'])context.run(schema_gen)!mkdir modules%%writefile {preprocessing_file}

import tensorflow as tf
from tensorflow.keras.applications import mobilenet_v2

_INPUT_IMG_SIZE = 128

_IMAGE_KEY = ""image""
_IMAGE_SHAPE_KEY = ""image_shape""
_LABEL_KEY = ""label""
_LABEL_SHAPE_KEY = ""label_shape""


def _transformed_name(key: str) -> str:
    return key + ""_xf""


# output should have the same keys as inputs
def preprocess(inputs):
    image_shape = inputs[_IMAGE_SHAPE_KEY]
    label_shape = inputs[_LABEL_SHAPE_KEY]
    
    images = tf.reshape(inputs[_IMAGE_KEY], [image_shape[0], image_shape[1], 3])
    labels = tf.reshape(inputs[_LABEL_KEY], [label_shape[0], label_shape[1], 1])

    return {
        _IMAGE_KEY: images,
        _IMAGE_SHAPE_KEY: inputs[_IMAGE_SHAPE_KEY],
        _LABEL_KEY: labels,
        _LABEL_SHAPE_KEY: inputs[_LABEL_SHAPE_KEY],
    }


def preprocessing_fn(inputs):
    """"""tf.transform's callback function for preprocessing inputs.
    Args:
      inputs: map from feature keys to raw not-yet-transformed features.
    Returns:
      Map from string feature key to transformed feature operations.
    """"""
    # print(inputs)
    outputs = {}

    features = tf.map_fn(preprocess, inputs)

    features[_IMAGE_KEY] = tf.image.resize(features[_IMAGE_KEY], [_INPUT_IMG_SIZE, _INPUT_IMG_SIZE])
    features[_LABEL_KEY] = tf.image.resize(features[_LABEL_KEY], [_INPUT_IMG_SIZE, _INPUT_IMG_SIZE])

    image_features = mobilenet_v2.preprocess_input(features[_IMAGE_KEY])

    outputs[_transformed_name(_IMAGE_KEY)] = image_features
    outputs[_transformed_name(_LABEL_KEY)] = features[_LABEL_KEY]

    return outputstransform = Transform(
    examples=example_gen.outputs[""examples""],
    schema=schema_gen.outputs[""schema""],
    preprocessing_fn=preprocessing_fn,
)context.run(transform)%%writefile {model_file}

from typing import List, Dict, Tuple

import absl
import tensorflow as tf
import tensorflow_transform as tft
from tensorflow.keras.optimizers import Adam
from tfx_bsl.tfxio import dataset_options
from tfx.components.trainer.fn_args_utils import FnArgs
from tfx.components.trainer.fn_args_utils import DataAccessor

_CONCRETE_INPUT = ""pixel_values""
_RAW_IMG_SIZE = 256
_INPUT_IMG_SIZE = 128
_TRAIN_LENGTH = 800
_EVAL_LENGTH = 200
_TRAIN_BATCH_SIZE = 64
_EVAL_BATCH_SIZE = 64
_EPOCHS = 1
_LR = 0.00006

_IMAGE_KEY = ""image""
_LABEL_KEY = ""label""

def INFO(text: str):
    absl.logging.info(text)

def _transformed_name(key: str) -> str:
    return key + ""_xf""

""""""
    _serving_preprocess, _serving_preprocess_fn, and 
    _model_exporter functions are defined to provide pre-
    processing capabilities when the model is served.
""""""


def _serving_preprocess(string_input):
    decoded_input = tf.io.decode_base64(string_input)
    decoded = tf.io.decode_jpeg(decoded_input, channels=3)
    decoded = decoded / 255
    resized = tf.image.resize(decoded, size=(_INPUT_IMG_SIZE, _INPUT_IMG_SIZE))
    return resized


@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])
def _serving_preprocess_fn(string_input):
    decoded_images = tf.map_fn(
        _serving_preprocess, string_input, dtype=tf.float32, back_prop=False
    )
    return {_CONCRETE_INPUT: decoded_images}


def _model_exporter(model: tf.keras.Model):
    m_call = tf.function(model.call).get_concrete_function(
        tf.TensorSpec(shape=[None, _INPUT_IMG_SIZE, _INPUT_IMG_SIZE, 3], dtype=tf.float32, name=_CONCRETE_INPUT)
    )

    @tf.function(input_signature=[tf.TensorSpec([None], tf.string)])
    def serving_fn(string_input):
        images = _serving_preprocess_fn(string_input)
        logits = m_call(**images)
        seg_mask = tf.math.argmax(logits, -1)
        return {""seg_mask"": seg_mask}

    return serving_fn

def _get_transform_features_signature(model, tf_transform_output):
    # the layer is added as an attribute to the model in order to make sure that
    # the model assets are handled correctly when exporting.    
    model.tft_layer = tf_transform_output.transform_features_layer()

    @tf.function(
        input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string, name=""examples"")]
    )
    def serve_tf_examples_fn(serialized_tf_examples):
        """"""Returns the output to be used in the serving signature.""""""
        feature_spec = tf_transform_output.raw_feature_spec()
        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)

        transformed_features = model.tft_layer(parsed_features)

        return transformed_features
        
    return serve_tf_examples_fn

def _get_tf_examples_serving_signature(model, tf_transform_output):
    """"""
    Returns a serving signature that accepts `tensorflow.Example`.

    This signature will be used for evaluation or bulk inference.
    """"""
    
    model.tft_layer = tf_transform_output.transform_features_layer()

    @tf.function(
        input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string, name=""examples"")]
    )
    def serve_tf_examples_fn(
        serialized_tf_example: tf.Tensor,
    ) -> Dict[str, tf.Tensor]:
        """"""Returns the output to be used in the serving signature.""""""
        # Load the schema of raw examples.
        raw_feature_spec = tf_transform_output.raw_feature_spec()
        # Remove label feature since these will not be present at serving time.
        # Parse the examples using schema into raw features
        raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)

        # Preprocess the raw features
        transformed_features = model.tft_layer(raw_features)

        # Run preprocessed inputs through the model to get the prediction
        outputs = model(transformed_features)

        return {
            _transformed_name(_LABEL_KEY): outputs
        }

    return serve_tf_examples_fn


""""""
    _input_fn reads TFRecord files passed from the upstream 
    TFX component, Transform. Assume the dataset is already 
    transformed appropriately. 
""""""

def _input_fn(
    file_pattern: List[str],
    data_accessor: DataAccessor,
    tf_transform_output: tft.TFTransformOutput,
    is_train: bool = False,
    batch_size: int = 200,
) -> tf.data.Dataset:
    dataset = data_accessor.tf_dataset_factory(
        file_pattern,
        dataset_options.TensorFlowDatasetOptions(
            batch_size=batch_size, label_key=_transformed_name(_LABEL_KEY)
        ),
        tf_transform_output.transformed_metadata.schema,
    )

    return dataset


def _build_model(num_labels) -> tf.keras.Model:
    base_model = tf.keras.applications.MobileNetV2(
        input_shape=[128, 128, 3], include_top=False
    )

    # Use the activations of these layers
    layer_names = [
        ""block_1_expand_relu"",  # 64x64
        ""block_3_expand_relu"",  # 32x32
        ""block_6_expand_relu"",  # 16x16
        ""block_13_expand_relu"",  # 8x8
        ""block_16_project"",  # 4x4
    ]
    base_model_outputs = [base_model.get_layer(name).output for name in layer_names]

    # Create the feature extraction model
    down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)
    down_stack.trainable = False

    up_stack = [
        upsample(512, 3),  # 4x4 -> 8x8
        upsample(256, 3),  # 8x8 -> 16x16
        upsample(128, 3),  # 16x16 -> 32x32
        upsample(64, 3),  # 32x32 -> 64x64
    ]

    inputs = tf.keras.layers.Input(
        shape=[128, 128, 3], name=_transformed_name(_IMAGE_KEY)
    )

    # Downsampling through the model
    skips = down_stack(inputs)
    x = skips[-1]
    skips = reversed(skips[:-1])

    # Upsampling and establishing the skip connections
    for up, skip in zip(up_stack, skips):
        x = up(x)
        concat = tf.keras.layers.Concatenate()
        x = concat([x, skip])

    # This is the last layer of the model
    last = tf.keras.layers.Conv2DTranspose(
        filters=num_labels, kernel_size=3, strides=2, padding=""same"", name=_transformed_name(_LABEL_KEY)
    )  # 64x64 -> 128x128

    x = last(x)

    model = tf.keras.Model(inputs=inputs, outputs=x)
    model.compile(
        optimizer=Adam(learning_rate=_LR),
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=[""sparse_categorical_accuracy""],
    )
    return model


""""""
    InstanceNormalization class and upsample function are
    borrowed from pix2pix in [TensorFlow Example repository](
    https://github.com/tensorflow/examples/tree/master/tensorflow_examples/models/pix2pix)
""""""


class InstanceNormalization(tf.keras.layers.Layer):
    """"""Instance Normalization Layer (https://arxiv.org/abs/1607.08022).""""""

    def __init__(self, epsilon=1e-5):
        super(InstanceNormalization, self).__init__()
        self.epsilon = epsilon

    def build(self, input_shape):
        self.scale = self.add_weight(
            name=""scale"",
            shape=input_shape[-1:],
            initializer=tf.random_normal_initializer(1.0, 0.02),
            trainable=True,
        )

        self.offset = self.add_weight(
            name=""offset"", shape=input_shape[-1:], initializer=""zeros"", trainable=True
        )

    def call(self, x):
        mean, variance = tf.nn.moments(x, axes=[1, 2], keepdims=True)
        inv = tf.math.rsqrt(variance + self.epsilon)
        normalized = (x - mean) * inv
        return self.scale * normalized + self.offset


def upsample(filters, size, norm_type=""batchnorm"", apply_dropout=False):
    """"""Upsamples an input.
    Conv2DTranspose => Batchnorm => Dropout => Relu
    Args:
      filters: number of filters
      size: filter size
      norm_type: Normalization type; either 'batchnorm' or 'instancenorm'.
      apply_dropout: If True, adds the dropout layer
    Returns:
      Upsample Sequential Model
    """"""

    initializer = tf.random_normal_initializer(0.0, 0.02)

    result = tf.keras.Sequential()
    result.add(
        tf.keras.layers.Conv2DTranspose(
            filters,
            size,
            strides=2,
            padding=""same"",
            kernel_initializer=initializer,
            use_bias=False,
        )
    )

    if norm_type.lower() == ""batchnorm"":
        result.add(tf.keras.layers.BatchNormalization())
    elif norm_type.lower() == ""instancenorm"":
        result.add(InstanceNormalization())

    if apply_dropout:
        result.add(tf.keras.layers.Dropout(0.5))

    result.add(tf.keras.layers.ReLU())

    return result


def run_fn(fn_args: FnArgs):
    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)

    train_dataset = _input_fn(
        fn_args.train_files,
        fn_args.data_accessor,
        tf_transform_output,
        is_train=True,
        batch_size=_TRAIN_BATCH_SIZE,
    )

    eval_dataset = _input_fn(
        fn_args.eval_files,
        fn_args.data_accessor,
        tf_transform_output,        
        is_train=False,
        batch_size=_EVAL_BATCH_SIZE,
    )

    num_labels = 35
    model = _build_model(num_labels)

    model.fit(
        train_dataset,
        steps_per_epoch=_TRAIN_LENGTH // _TRAIN_BATCH_SIZE,
        validation_data=eval_dataset,
        validation_steps=_EVAL_LENGTH // _TRAIN_BATCH_SIZE,
        epochs=_EPOCHS,
    )

    model.save(
        fn_args.serving_model_dir, save_format=""tf"", signatures={
            ""serving_default"": _model_exporter(model),
            ""transform_features"": _get_transform_features_signature(
                model, tf_transform_output
            ),
            ""from_examples"": _get_tf_examples_serving_signature(
                model, tf_transform_output
            ),            
        }
    )
trainer = Trainer(
    run_fn=model_fn,
    transformed_examples=transform.outputs[""transformed_examples""],
    transform_graph=transform.outputs[""transform_graph""],
    schema=schema_gen.outputs[""schema""],
)context.run(trainer)"
0,semantic-segmentation-ml-pipeline,notebooks/unet_training_sidewalks.ipynb,"import tensorflow as tf
import numpy as npGCS_PATH = ""gs://sidewalks-tfx-hf/sidewalks-tfrecords""
BATCH_SIZE = 2
AUTO = tf.data.AUTOTUNEdef parse_tfr(proto):
    feature_description = {
        ""image"": tf.io.FixedLenFeature([], tf.string), 
        ""label"": tf.io.FixedLenFeature([], tf.string)
    }
    rec = tf.io.parse_single_example(proto, feature_description)

    image = tf.io.parse_tensor(rec[""image""], tf.float32)
    label = tf.io.parse_tensor(rec[""label""], tf.float32)

    return {""pixel_values"": image, ""labels"": label}


def prepare_dataset(split=""train"", batch_size=BATCH_SIZE):
    if split not in [""train"", ""val""]:
        raise ValueError(
            ""Invalid split provided. Supports splits are: `train` and `val`.""
        )

    dataset = tf.data.TFRecordDataset(
        [filename for filename in tf.io.gfile.glob(f""{GCS_PATH}/{split}-*"")],
        num_parallel_reads=AUTO,
    ).map(parse_tfr, num_parallel_calls=AUTO)

    if split == ""train"":
        dataset = dataset.shuffle(batch_size * 2)

    dataset = dataset.batch(batch_size)
    dataset = dataset.prefetch(AUTO)
    return datasettrain_dataset = prepare_dataset()
val_dataset = prepare_dataset(split=""val"")batch = next(iter(train_dataset))
batch[0].shape, batch[1].shapedef preprocess(example_batch):
    images = example_batch[""pixel_values""]
    images = tf.transpose(images, perm=[0, 1, 2, 3]) # (batch_size, height, width, num_channels)
    labels = tf.expand_dims(example_batch[""labels""], -1) # Adds extra dimension, otherwise tf.image.resize won't work.
    labels = tf.transpose(labels, perm=[0, 1, 2, 3]) # So, that TF can evaluation the shapes.

    images = tf.image.resize(images, (128, 128))
    labels = tf.image.resize(labels, (128, 128))

    # images = tf.transpose(images, perm=[0, 3, 1, 2]) # (batch_size, num_channels, height, width)
    labels = tf.squeeze(labels, -1)
    
    return images, labelstrain_dataset = train_dataset.map(preprocess)
val_dataset = val_dataset.map(preprocess)# Investigate a single batch.
batch = next(iter(train_dataset))
batch[0].shape, batch[1].shapedef sidewalk_palette():
    """"""Sidewalk palette that maps each class to RGB values.""""""
    return [
        [0, 0, 0],
        [216, 82, 24],
        [255, 255, 0],
        [125, 46, 141],
        [118, 171, 47],
        [161, 19, 46],
        [255, 0, 0],
        [0, 128, 128],
        [190, 190, 0],
        [0, 255, 0],
        [0, 0, 255],
        [170, 0, 255],
        [84, 84, 0],
        [84, 170, 0],
        [84, 255, 0],
        [170, 84, 0],
        [170, 170, 0],
        [170, 255, 0],
        [255, 84, 0],
        [255, 170, 0],
        [255, 255, 0],
        [33, 138, 200],
        [0, 170, 127],
        [0, 255, 127],
        [84, 0, 127],
        [84, 84, 127],
        [84, 170, 127],
        [84, 255, 127],
        [170, 0, 127],
        [170, 84, 127],
        [170, 170, 127],
        [170, 255, 127],
        [255, 0, 127],
        [255, 84, 127],
        [255, 170, 127],
    ]def get_seg_overlay(image, seg):
    # image = tf.transpose(image, [1, 2, 0])
    color_seg = np.zeros(
        (seg.shape[0], seg.shape[1], 3), dtype=np.uint8
    )  # height, width, 3
    palette = np.array(sidewalk_palette())
    for label, color in enumerate(palette):
        color_seg[seg == label, :] = color

    # Show image + mask
    img = np.array(image) * 0.5 + color_seg * 0.5
    img = img.astype(np.uint8)

    return imggt_img = get_seg_overlay(
    batch[0][0], 
    np.array(batch[1][0])
)import matplotlib.pyplot as plt

f, axs = plt.subplots(1, 1)
f.set_figheight(5)
f.set_figwidth(10)

axs.set_title(""Ground truth"", {""fontsize"": 20})
axs.imshow(gt_img)
axs.axis(""off"")

plt.show()class InstanceNormalization(tf.keras.layers.Layer):
  """"""Instance Normalization Layer (https://arxiv.org/abs/1607.08022).""""""

  def __init__(self, epsilon=1e-5):
    super(InstanceNormalization, self).__init__()
    self.epsilon = epsilon

  def build(self, input_shape):
    self.scale = self.add_weight(
        name='scale',
        shape=input_shape[-1:],
        initializer=tf.random_normal_initializer(1., 0.02),
        trainable=True)

    self.offset = self.add_weight(
        name='offset',
        shape=input_shape[-1:],
        initializer='zeros',
        trainable=True)

  def call(self, x):
    mean, variance = tf.nn.moments(x, axes=[1, 2], keepdims=True)
    inv = tf.math.rsqrt(variance + self.epsilon)
    normalized = (x - mean) * inv
    return self.scale * normalized + self.offsetdef upsample(filters, size, norm_type='batchnorm', apply_dropout=False):
  """"""Upsamples an input.
  Conv2DTranspose => Batchnorm => Dropout => Relu
  Args:
    filters: number of filters
    size: filter size
    norm_type: Normalization type; either 'batchnorm' or 'instancenorm'.
    apply_dropout: If True, adds the dropout layer
  Returns:
    Upsample Sequential Model
  """"""

  initializer = tf.random_normal_initializer(0., 0.02)

  result = tf.keras.Sequential()
  result.add(
      tf.keras.layers.Conv2DTranspose(filters, size, strides=2,
                                      padding='same',
                                      kernel_initializer=initializer,
                                      use_bias=False))

  if norm_type.lower() == 'batchnorm':
    result.add(tf.keras.layers.BatchNormalization())
  elif norm_type.lower() == 'instancenorm':
    result.add(InstanceNormalization())

  if apply_dropout:
    result.add(tf.keras.layers.Dropout(0.5))

  result.add(tf.keras.layers.ReLU())

  return resultbase_model = tf.keras.applications.MobileNetV2(input_shape=[128, 128, 3], include_top=False)

# Use the activations of these layers
layer_names = [
    'block_1_expand_relu',   # 64x64
    'block_3_expand_relu',   # 32x32
    'block_6_expand_relu',   # 16x16
    'block_13_expand_relu',  # 8x8
    'block_16_project',      # 4x4
]
base_model_outputs = [base_model.get_layer(name).output for name in layer_names]

# Create the feature extraction model
down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)

down_stack.trainable = Falseup_stack = [
    upsample(512, 3),  # 4x4 -> 8x8
    upsample(256, 3),  # 8x8 -> 16x16
    upsample(128, 3),  # 16x16 -> 32x32
    upsample(64, 3),   # 32x32 -> 64x64
]def unet_model(output_channels:int):
  inputs = tf.keras.layers.Input(shape=[128, 128, 3], name=""pixel_values"")

  # Downsampling through the model
  skips = down_stack(inputs)
  x = skips[-1]
  skips = reversed(skips[:-1])

  # Upsampling and establishing the skip connections
  for up, skip in zip(up_stack, skips):
    x = up(x)
    concat = tf.keras.layers.Concatenate()
    x = concat([x, skip])

  # This is the last layer of the model
  last = tf.keras.layers.Conv2DTranspose(
      filters=output_channels, kernel_size=3, strides=2,
      padding='same',
      name=""labels"")  #64x64 -> 128x128

  x = last(x)

  return tf.keras.Model(inputs=inputs, outputs=x)OUTPUT_CLASSES = len(sidewalk_palette())

model = unet_model(output_channels=OUTPUT_CLASSES)
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])model.summary()EPOCHS = 5

model_history = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=EPOCHS
)test_batch = next(iter(val_dataset))pred_mask = model.predict(test_batch[0])pred_seg = tf.math.argmax(pred_mask[0], axis=-1)import matplotlib.pyplot as plt

seg_im = get_seg_overlay(test_batch[0][0], pred_seg)

f, axis = plt.subplots(1, 2)
f.set_figheight(5)
f.set_figwidth(20)

axis[0].set_title(""Image"", {""fontsize"": 20})
axis[1].set_title(""Prediction"", {""fontsize"": 20})
axis[0].axis(""off"")
axis[1].axis(""off"")

axis[0].imshow(test_batch[0][0])
axis[1].imshow(seg_im)_IMAGE_SHAPE = (128, 128)
_CONCRETE_INPUT = ""pixel_values""

def _serving_preprocess(string_input):
    decoded_input = tf.io.decode_base64(string_input)
    decoded = tf.io.decode_jpeg(decoded_input, channels=3)
    decoded = decoded / 255
    resized = tf.image.resize(decoded, size=_IMAGE_SHAPE)
    return resized

@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])
def _serving_preprocess_fn(string_input):
    decoded_images = tf.map_fn(
        _serving_preprocess, string_input, dtype=tf.float32, back_prop=False
    )
    return {_CONCRETE_INPUT: decoded_images}


def _model_exporter(model: tf.keras.Model):
    m_call = tf.function(model.call).get_concrete_function(
        tf.TensorSpec(
            shape=[None, 128, 128, 3], dtype=tf.float32, name=_CONCRETE_INPUT
        )
    )

    @tf.function(input_signature=[tf.TensorSpec([None], tf.string)])
    def serving_fn(string_input):
        images = _serving_preprocess_fn(string_input)
        logits = m_call(**images)
        seg_mask = tf.math.argmax(logits, -1)
        return {""seg_mask"": seg_mask}

    return serving_fnmodel.save(
    ""./test_model"", 
    save_format=""tf"",
    signatures=_model_exporter(model)
)model = tf.keras.models.load_model('./test_model')pred_mask = model.predict(test_batch[0])pred_mask.shape"
0,semantic-segmentation-ml-pipeline,tfrecords/create_tfrecords.py,"""""""
Script to generate TFRecord shards from the Sidewalks dataset as shown in this
blog post: https://huggingface.co/blog/fine-tune-segformer.

The recommended way to obtain TFRecord shards is via an Apache Beam
Pipeline with an execution runner like Dataflow. Example:
https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/05_create_dataset/jpeg_to_tfrecord.py.

Usage:
    python create_tfrecords --batch_size 16
    python create_tfrecords --resize 256 # without --resize flag, no resizing is applied

References:

    * https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/05_create_dataset/05_split_tfrecord.ipynb
    * https://www.tensorflow.org/tutorials/images/segmentation
""""""

import argparse
import math
import os
from typing import Tuple

import datasets
import numpy as np
import tensorflow as tf
import tqdm
from PIL import Image

RESOLUTION = 256


def load_sidewalks_dataset(args):
    hf_dataset_identifier = ""segments/sidewalk-semantic""
    ds = datasets.load_dataset(hf_dataset_identifier)

    ds = ds.shuffle(seed=1)
    ds = ds[""train""].train_test_split(test_size=args.split, seed=args.seed)
    train_ds = ds[""train""]
    val_ds = ds[""test""]

    return train_ds, val_ds


def resize_img(
    image: tf.Tensor, label: tf.Tensor, resize: int
) -> Tuple[tf.Tensor, tf.Tensor]:
    image = tf.image.resize(image, (resize, resize))
    label = tf.image.resize(label[..., None], (resize, resize))
    label = tf.squeeze(label, -1)
    return image, label


def process_image(
    image: Image, label: Image, resize: int
) -> Tuple[tf.Tensor, tf.Tensor]:
    image = np.array(image)
    label = np.array(label)

    image = tf.convert_to_tensor(image)
    label = tf.convert_to_tensor(label)

    if resize:
        image, label = resize_img(image, label, resize)

    return image, label


def _int64_feature(value):
    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))


def _float_feature(value):
    return tf.train.Feature(float_list=tf.train.FloatList(value=value))


def create_tfrecord(image: Image, label: Image, resize: int):
    image, label = process_image(image, label, resize)
    image_dims = image.shape
    label_dims = label.shape

    image = tf.reshape(image, [-1])  # flatten to 1D array
    label = tf.reshape(label, [-1])  # flatten to 1D array

    return tf.train.Example(
        features=tf.train.Features(
            feature={
                ""image"": _float_feature(image.numpy()),
                ""image_shape"": _int64_feature(
                    [image_dims[0], image_dims[1], image_dims[2]]
                ),
                ""label"": _float_feature(label.numpy()),
                ""label_shape"": _int64_feature([label_dims[0], label_dims[1]]),
            }
        )
    ).SerializeToString()


def write_tfrecords(root_dir, dataset, split, batch_size, resize):
    print(f""Preparing TFRecords for split: {split}."")

    for step in tqdm.tnrange(int(math.ceil(len(dataset) / batch_size))):
        temp_ds = dataset[step * batch_size : (step + 1) * batch_size]
        shard_size = len(temp_ds[""pixel_values""])
        filename = os.path.join(
            root_dir, ""{}-{:02d}-{}.tfrec"".format(split, step, shard_size)
        )

        with tf.io.TFRecordWriter(filename) as out_file:
            for i in range(shard_size):
                image = temp_ds[""pixel_values""][i]
                label = temp_ds[""label""][i]
                example = create_tfrecord(image, label, resize)
                out_file.write(example)
            print(""Wrote file {} containing {} records"".format(filename, shard_size))


def main(args):
    train_ds, val_ds = load_sidewalks_dataset(args)
    print(""Dataset loaded from HF."")

    if not os.path.exists(args.root_tfrecord_dir):
        os.makedirs(args.root_tfrecord_dir, exist_ok=True)

    print(args.resize)

    write_tfrecords(
        args.root_tfrecord_dir, train_ds, ""train"", args.batch_size, args.resize
    )
    write_tfrecords(args.root_tfrecord_dir, val_ds, ""val"", args.batch_size, args.resize)


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        ""--split"", help=""Train and test split."", default=0.2, type=float
    )
    parser.add_argument(
        ""--seed"",
        help=""Seed to be used while performing train-test splits."",
        default=2022,
        type=int,
    )
    parser.add_argument(
        ""--root_tfrecord_dir"",
        help=""Root directory where the TFRecord shards will be serialized."",
        default=""sidewalks-tfrecords"",
        type=str,
    )
    parser.add_argument(
        ""--batch_size"",
        help=""Number of samples to process in a batch before serializing a single TFRecord shard."",
        default=32,
        type=int,
    )
    parser.add_argument(
        ""--resize"",
        help=""Width and height size the image will be resized to. No resizing will be applied when this isn't set."",
        type=int,
    )

    args = parser.parse_args()
    return args


if __name__ == ""__main__"":
    args = parse_args()
    main(args)
"
0,semantic-segmentation-ml-pipeline,tfrecords/create_tfrecords_pets.py,"""""""
Script to generate TFRecord shards from the Pets dataset as shown in this
tutorial: https://keras.io/examples/vision/oxford_pets_image_segmentation/.

The recommended way to obtain TFRecord shards is via an Apache Beam
Pipeline with an execution runner like Dataflow. Example:
https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/05_create_dataset/jpeg_to_tfrecord.py.

Usage:
    python create_tfrecords_pets.py --batch_size 64
    python create_tfrecords_pets.py --resize 128 # without --resize flag, no resizing is applied

References:

    * https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/05_create_dataset/05_split_tfrecord.ipynb
    * https://www.tensorflow.org/tutorials/images/segmentation
    * https://keras.io/examples/vision/oxford_pets_image_segmentation/
""""""

import argparse
import os
import random
from typing import List, Tuple

import numpy as np
import tensorflow as tf
import tqdm
from PIL import Image

RESOLUTION = 128
SEED = 2022


def load_paths(args) -> Tuple[List[str], List[str]]:
    input_img_paths = sorted(
        [
            os.path.join(args.input_dir, fname)
            for fname in os.listdir(args.input_dir)
            if fname.endswith("".jpg"")
        ]
    )
    target_img_paths = sorted(
        [
            os.path.join(args.target_dir, fname)
            for fname in os.listdir(args.target_dir)
            if fname.endswith("".png"") and not fname.startswith(""."")
        ]
    )
    return input_img_paths, target_img_paths


def resize_img(
    image: tf.Tensor, label: tf.Tensor, resize: int
) -> Tuple[tf.Tensor, tf.Tensor]:
    image = tf.image.resize(image, (resize, resize))
    label = tf.image.resize(label[..., None], (resize, resize))
    label = tf.squeeze(label, -1)
    label -= 1
    return image, label


def process_image(
    image_path: str, label_path: str, resize: int
) -> Tuple[tf.Tensor, tf.Tensor]:
    image = Image.open(image_path).convert(""RGB"")
    label = Image.open(label_path).convert(""L"")

    image = np.array(image)
    label = np.array(label)

    image = tf.convert_to_tensor(image)
    label = tf.convert_to_tensor(label)

    if resize:
        image, label = resize_img(image, label, resize)

    return image, label


def split_paths(img_paths: List[str], target_paths: List[str], split: float):
    val_samples = int(len(img_paths) * split)

    random.Random(SEED).shuffle(img_paths)
    random.Random(SEED).shuffle(target_paths)

    train_img_paths = img_paths[:-val_samples]
    train_target_paths = target_paths[:-val_samples]
    val_img_paths = img_paths[-val_samples:]
    val_target_paths = target_paths[-val_samples:]

    return train_img_paths, train_target_paths, val_img_paths, val_target_paths


def prepare_tf_dataset(
    img_paths: List[str], target_paths: List[str], batch_size: int
) -> tf.data.Dataset:
    tf_dataset = tf.data.Dataset.from_tensor_slices((img_paths, target_paths))
    return tf_dataset.batch(batch_size)


def get_tf_datasets(args) -> Tuple[tf.data.Dataset, tf.data.Dataset]:
    input_img_paths, target_img_paths = load_paths(args)
    train_img_paths, train_target_paths, val_img_paths, val_target_paths = split_paths(
        input_img_paths, target_img_paths, args.split
    )
    train_ds = prepare_tf_dataset(train_img_paths, train_target_paths, args.batch_size)
    val_ds = prepare_tf_dataset(val_img_paths, val_target_paths, args.batch_size)
    return train_ds, val_ds


def _int64_feature(value):
    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))


def _float_feature(value):
    return tf.train.Feature(float_list=tf.train.FloatList(value=value))


def create_tfrecord(image: Image, label: Image, resize: int):
    image, label = process_image(image, label, resize)
    image_dims = image.shape
    label_dims = label.shape

    image = tf.reshape(image, [-1])  # flatten to 1D array
    label = tf.reshape(label, [-1])  # flatten to 1D array

    return tf.train.Example(
        features=tf.train.Features(
            feature={
                ""image"": _float_feature(image.numpy()),
                ""image_shape"": _int64_feature(
                    [image_dims[0], image_dims[1], image_dims[2]]
                ),
                ""label"": _float_feature(label.numpy()),
                ""label_shape"": _int64_feature([label_dims[0], label_dims[1]]),
            }
        )
    ).SerializeToString()


def write_tfrecords(root_dir: str, dataset: tf.data.Dataset, split: str, resize: int):
    print(f""Preparing TFRecords for split: {split}."")

    for shard, (image_paths, label_paths) in enumerate(tqdm.tqdm(dataset)):
        shard_size = image_paths.numpy().shape[0]
        filename = os.path.join(
            root_dir, ""{}-{:02d}-{}.tfrec"".format(split, shard, shard_size)
        )

        with tf.io.TFRecordWriter(filename) as out_file:
            for i in range(shard_size):
                img_path = image_paths[i]
                label_path = label_paths[i]
                example = create_tfrecord(img_path.numpy(), label_path.numpy(), resize)
                out_file.write(example)
            print(""Wrote file {} containing {} records"".format(filename, shard_size))


def main(args):
    train_ds, val_ds = get_tf_datasets(args)
    print(""TensorFlow datasets loaded."")

    if not os.path.exists(args.root_tfrecord_dir):
        os.makedirs(args.root_tfrecord_dir, exist_ok=True)

    write_tfrecords(args.root_tfrecord_dir, train_ds, ""train"", args.resize)
    write_tfrecords(args.root_tfrecord_dir, val_ds, ""val"", args.resize)


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(""--split"", help=""Train and test split."", default=0.2, type=float)
    parser.add_argument(
        ""--input_dir"",
        help=""Path to the directory containing all images."",
        default=""images/"",
        type=str,
    )
    parser.add_argument(
        ""--target_dir"",
        help=""Path to the directory containing all targets."",
        default=""annotations/trimaps/"",
        type=str,
    )
    parser.add_argument(
        ""--root_tfrecord_dir"",
        help=""Root directory where the TFRecord shards will be serialized."",
        default=""pets-tfrecords"",
        type=str,
    )
    parser.add_argument(
        ""--batch_size"",
        help=""Number of samples to process in a batch before serializing a single TFRecord shard."",
        default=32,
        type=int,
    )
    parser.add_argument(
        ""--resize"",
        help=""Width and height size the image will be resized to. No resizing will be applied when this isn't set."",
        type=int,
    )

    args = parser.parse_args()
    return args


if __name__ == ""__main__"":
    args = parse_args()
    main(args)
"
0,semantic-segmentation-ml-pipeline,tfrecords/create_tfrecords_str.py,"""""""
Script to generate TFRecord shards from the Sidewalks dataset as shown in this
blog post: https://huggingface.co/blog/fine-tune-segformer.

The recommended way to obtain TFRecord shards is via an Apache Beam
Pipeline with an execution runner like Dataflow. Example:
https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/05_create_dataset/jpeg_to_tfrecord.py.

Usage:
    python create_tfrecords --batch_size 16

References:

    * https://github.com/sayakpaul/TF-2.0-Hacks/blob/master/Cats_vs_Dogs_TFRecords.ipynb
    * https://www.tensorflow.org/tutorials/images/segmentation
""""""

import argparse
import math
import os
from typing import Tuple

import datasets
import numpy as np
import tensorflow as tf
import tqdm
from PIL import Image

RESOLUTION = 256


def load_sidewalks_dataset(args):
    hf_dataset_identifier = ""segments/sidewalk-semantic""
    ds = datasets.load_dataset(hf_dataset_identifier)

    ds = ds.shuffle(seed=1)
    ds = ds[""train""].train_test_split(test_size=args.split, seed=args.seed)
    train_ds = ds[""train""]
    val_ds = ds[""test""]

    return train_ds, val_ds


def resize_img(image: tf.Tensor, label: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:
    image = tf.image.resize(image, (RESOLUTION, RESOLUTION))
    label = tf.image.resize(label[..., None], (RESOLUTION, RESOLUTION))
    label = tf.squeeze(label, -1)
    return image, label


def process_image(image: Image, label: Image) -> Tuple[tf.Tensor, tf.Tensor]:
    image = np.array(image)
    label = np.array(label)

    image = tf.convert_to_tensor(image)
    label = tf.convert_to_tensor(label)

    image, label = resize_img(image, label)
    image, label = normalize_img(image, label)
    return image, label


def _bytestring_feature(value):
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))


def create_tfrecord(image: Image, label: Image):
    image, label = process_image(image, label)

    image = tf.io.serialize_tensor(image)
    label = tf.io.serialize_tensor(label)

    return tf.train.Example(
        features=tf.train.Features(
            feature={
                ""image"": _bytestring_feature([image.numpy()]),
                ""label"": _bytestring_feature([label.numpy()]),
            }
        )
    ).SerializeToString()


def write_tfrecords(root_dir, dataset, split, batch_size):
    print(f""Preparing TFRecords for split: {split}."")

    for step in tqdm.tnrange(int(math.ceil(len(dataset) / batch_size))):
        temp_ds = dataset[step * batch_size : (step + 1) * batch_size]
        shard_size = len(temp_ds[""pixel_values""])
        filename = os.path.join(
            root_dir, ""{}-{:02d}-{}.tfrec"".format(split, step, shard_size)
        )

        with tf.io.TFRecordWriter(filename) as out_file:
            for i in range(shard_size):
                image = temp_ds[""pixel_values""][i]
                label = temp_ds[""label""][i]
                example = create_tfrecord(image, label)
                out_file.write(example)
            print(""Wrote file {} containing {} records"".format(filename, shard_size))


def main(args):
    train_ds, val_ds = load_sidewalks_dataset(args)
    print(""Dataset loaded from HF."")

    if not os.path.exists(args.root_tfrecord_dir):
        os.makedirs(args.root_tfrecord_dir, exist_ok=True)

    write_tfrecords(args.root_tfrecord_dir, train_ds, ""train"", args.batch_size)
    write_tfrecords(args.root_tfrecord_dir, val_ds, ""val"", args.batch_size)


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        ""--split"", help=""Train and test split."", default=0.2, type=float
    )
    parser.add_argument(
        ""--seed"",
        help=""Seed to be used while performing train-test splits."",
        default=2022,
        type=int,
    )
    parser.add_argument(
        ""--root_tfrecord_dir"",
        help=""Root directory where the TFRecord shards will be serialized."",
        default=""sidewalks-tfrecords"",
        type=str,
    )
    parser.add_argument(
        ""--batch_size"",
        help=""Number of samples to process in a batch before serializing a single TFRecord shard."",
        default=32,
        type=int,
    )

    args = parser.parse_args()
    return args


if __name__ == ""__main__"":
    args = parse_args()
    main(args)
"
0,semantic-segmentation-ml-pipeline,training_pipeline/__init__.py,"# Copyright 2020 Google LLC. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"
0,semantic-segmentation-ml-pipeline,training_pipeline/data_validation.ipynb,"# import required libs
import glob
import os

import tensorflow as tf
import tensorflow_data_validation as tfdv
print('TF version: {}'.format(tf.version.VERSION))
print('TFDV version: {}'.format(tfdv.version.__version__))# Read artifact information from metadata store.
import beam_dag_runner

from tfx.orchestration import metadata
from tfx.types import standard_artifacts

metadata_connection_config = metadata.sqlite_metadata_connection_config(
              beam_dag_runner.METADATA_PATH)
with metadata.Metadata(metadata_connection_config) as store:
    stats_artifacts = store.get_artifacts_by_type(standard_artifacts.ExampleStatistics.TYPE_NAME)
    schema_artifacts = store.get_artifacts_by_type(standard_artifacts.Schema.TYPE_NAME)
    anomalies_artifacts = store.get_artifacts_by_type(standard_artifacts.ExampleAnomalies.TYPE_NAME)# configure output paths
# Exact paths to output artifacts can also be found on KFP Web UI if you are using kubeflow.
stats_path = stats_artifacts[-1].uri
train_stats_file = os.path.join(stats_path, 'train', 'stats_tfrecord')
eval_stats_file = os.path.join(stats_path, 'eval', 'stats_tfrecord')
print(""Train stats file:{}, Eval stats file:{}"".format(
    train_stats_file, eval_stats_file))

schema_file = os.path.join(schema_artifacts[-1].uri, 'schema.pbtxt')
print(""Generated schame file:{}"".format(schema_file))
anomalies_file = os.path.join(anomalies_artifacts[-1].uri, 'anomalies.pbtxt')
print(""Generated anomalies file:{}"".format(anomalies_file))# load generated statistics from StatisticsGen
train_stats = tfdv.load_statistics(train_stats_file)
eval_stats = tfdv.load_statistics(eval_stats_file)
tfdv.visualize_statistics(lhs_statistics=eval_stats, rhs_statistics=train_stats,
                          lhs_name='EVAL_DATASET', rhs_name='TRAIN_DATASET')# load generated schema from SchemaGen
schema = tfdv.load_schema_text(schema_file)
tfdv.display_schema(schema=schema)# load data vaildation result from ExampleValidator
anomalies = tfdv.load_anomalies_text(anomalies_file)
tfdv.display_anomalies(anomalies)"
0,semantic-segmentation-ml-pipeline,training_pipeline/kubeflow_runner.py,"from absl import logging
from tfx import v1 as tfx
from tfx.orchestration.kubeflow.v2 import kubeflow_v2_dag_runner as runner
from tfx.proto import trainer_pb2

from pipeline import configs, pipeline


def run():
    runner_config = runner.KubeflowV2DagRunnerConfig(default_image=configs.PIPELINE_IMAGE)

    runner.KubeflowV2DagRunner(
        config=runner_config,
        output_filename=configs.PIPELINE_NAME + ""_pipeline.json"",
    ).run(
        pipeline.create_pipeline(
            pipeline_name=configs.PIPELINE_NAME,
            pipeline_root=configs.PIPELINE_ROOT,
            data_path=configs.DATA_PATH,
            schema_path=configs.SCHEMA_PATH,
            modules={
                ""training_fn"": configs.TRAINING_FN,
                ""preprocessing_fn"": configs.PREPROCESSING_FN,
            },
            train_args=trainer_pb2.TrainArgs(num_steps=configs.TRAIN_NUM_STEPS),
            eval_args=trainer_pb2.EvalArgs(num_steps=configs.EVAL_NUM_STEPS),
            eval_configs=configs.EVAL_CONFIGS,
            ai_platform_training_args=configs.GCP_AI_PLATFORM_TRAINING_ARGS,
            ai_platform_serving_args=configs.GCP_AI_PLATFORM_SERVING_ARGS,
            example_gen_beam_args=configs.EXAMPLE_GEN_BEAM_ARGS,
            transform_beam_args=configs.TRANSFORM_BEAM_ARGS,
            hf_pusher_args=configs.HF_PUSHER_ARGS,
        )
    )


if __name__ == ""__main__"":
    logging.set_verbosity(logging.INFO)
    run()
"
0,semantic-segmentation-ml-pipeline,training_pipeline/local_runner.py,"import os
from absl import logging

from tfx import v1 as tfx
from tfx.orchestration.data_types import RuntimeParameter
from pipeline import configs
from pipeline import local_pipeline

# TFX pipeline produces many output files and metadata. All output data will be
# stored under this OUTPUT_DIR.
# NOTE: It is recommended to have a separated OUTPUT_DIR which is *outside* of
#       the source code structure. Please change OUTPUT_DIR to other location
#       where we can store outputs of the pipeline.
OUTPUT_DIR = "".""

# TFX produces two types of outputs, files and metadata.
# - Files will be created under PIPELINE_ROOT directory.
# - Metadata will be written to SQLite database in METADATA_PATH.
PIPELINE_ROOT = os.path.join(OUTPUT_DIR, ""tfx_pipeline_output"", configs.PIPELINE_NAME)
METADATA_PATH = os.path.join(
    OUTPUT_DIR, ""tfx_metadata"", configs.PIPELINE_NAME, ""metadata.db""
)

# The last component of the pipeline, ""Pusher"" will produce serving model under
# SERVING_MODEL_DIR.
SERVING_MODEL_DIR = os.path.join(PIPELINE_ROOT, ""serving_model"")

# Specifies data file directory. DATA_PATH should be a directory containing CSV
# files for CsvExampleGen in this example. By default, data files are in the
# `data` directory.
# NOTE: If you upload data files to GCS(which is recommended if you use
#       Kubeflow), you can use a path starting ""gs://YOUR_BUCKET_NAME/path"" for
#       DATA_PATH. For example,
#       DATA_PATH = 'gs://bucket/penguin/csv/'.
# TODO(step 4): Specify the path for your data.
DATA_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), ""data"")


def run():
    """"""Define a pipeline.""""""

    tfx.orchestration.LocalDagRunner().run(
        local_pipeline.create_pipeline(
            pipeline_name=configs.PIPELINE_NAME,
            pipeline_root=PIPELINE_ROOT,
            data_path=configs.DATA_PATH,
            schema_path=configs.SCHEMA_PATH,
            modules={
                ""training_fn"": configs.TRAINING_FN,
                ""preprocessing_fn"": configs.PREPROCESSING_FN,
            },
            train_args=tfx.proto.TrainArgs(num_steps=configs.TRAIN_NUM_STEPS),
            eval_args=tfx.proto.EvalArgs(num_steps=configs.EVAL_NUM_STEPS),
            eval_configs=configs.EVAL_CONFIGS,
            serving_model_dir=SERVING_MODEL_DIR,
            metadata_connection_config=tfx.orchestration.metadata.sqlite_metadata_connection_config(
                METADATA_PATH
            ),
        )
    )


if __name__ == ""__main__"":
    logging.set_verbosity(logging.INFO)
    run()
"
0,semantic-segmentation-ml-pipeline,training_pipeline/model_analysis.ipynb,"# import required libs
import glob
import os

import tensorflow as tf
import tensorflow_model_analysis as tfma
print('TF version: {}'.format(tf.version.VERSION))
print('TFMA version: {}'.format(tfma.version.VERSION_STRING))# Read artifact information from metadata store.
import beam_dag_runner

from tfx.orchestration import metadata
from tfx.types import standard_artifacts

metadata_connection_config = metadata.sqlite_metadata_connection_config(
              beam_dag_runner.METADATA_PATH)
with metadata.Metadata(metadata_connection_config) as store:
    model_eval_artifacts = store.get_artifacts_by_type(standard_artifacts.ModelEvaluation.TYPE_NAME)# configure output paths
# Exact paths to output artifacts can be found in the execution logs
# or KFP Web UI if you are using kubeflow.
model_eval_path = model_eval_artifacts[-1].uri
print(""Generated model evaluation result:{}"".format(model_eval_path))eval_result = tfma.load_eval_result(model_eval_path)
tfma.view.render_slicing_metrics(eval_result, slicing_spec = tfma.slicer.SingleSliceSpec(columns=['trip_start_hour']))"
0,semantic-segmentation-ml-pipeline,training_pipeline/models/__init__.py,"# Copyright 2020 Google LLC. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"
0,semantic-segmentation-ml-pipeline,training_pipeline/models/common.py,"IMAGE_KEY = ""image""
IMAGE_SHAPE_KEY = ""image_shape""
LABEL_KEY = ""label""
LABEL_SHAPE_KEY = ""label_shape""

CONCRETE_INPUT = ""pixel_values""
NUM_LABELS = 3
"
0,semantic-segmentation-ml-pipeline,training_pipeline/models/hyperparams.py,"INPUT_IMG_SIZE = 128

TRAIN_BATCH_SIZE = 64
EVAL_BATCH_SIZE = 64

EPOCHS = 10
"
0,semantic-segmentation-ml-pipeline,training_pipeline/models/preprocessing.py,"import tensorflow as tf
from tensorflow.keras.applications import mobilenet_v2
from .utils import transformed_name
from .common import IMAGE_KEY, LABEL_KEY

def preprocessing_fn(inputs):
    """"""tf.transform's callback function for preprocessing inputs.
    Args:
      inputs: map from feature keys to raw not-yet-transformed features.
    Returns:
      Map from string feature key to transformed feature operations.
    """"""
    # print(inputs)
    outputs = {}

    image_features = mobilenet_v2.preprocess_input(inputs[IMAGE_KEY])

    outputs[transformed_name(IMAGE_KEY)] = image_features
    outputs[transformed_name(LABEL_KEY)] = inputs[LABEL_KEY]

    return outputs
"
0,semantic-segmentation-ml-pipeline,training_pipeline/models/signatures.py,"from typing import Dict

import tensorflow as tf
import tensorflow_transform as tft
from tensorflow.keras.applications import mobilenet_v2

from .utils import transformed_name
from .common import IMAGE_KEY, LABEL_KEY, CONCRETE_INPUT
from .hyperparams import INPUT_IMG_SIZE


def _serving_preprocess(string_input):
    """"""
    _serving_preprocess turns base64 encoded string data into a
    string type of Tensor. Then it is decoded as 3 channel based
    uint8 type of Tensor. Finally, it is normalized and resized
    to the size the model expects.
    """"""
    decoded_input = tf.io.decode_base64(string_input)
    decoded = tf.io.decode_jpeg(decoded_input, channels=3)
    resized = tf.image.resize(decoded, size=(INPUT_IMG_SIZE, INPUT_IMG_SIZE))
    normalized = mobilenet_v2.preprocess_input(resized)
    return normalized


@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])
def _serving_preprocess_fn(string_input):
    """"""
    _serving_preprocess_fn simply iteratively applies _serving_preprocess
    function to each entry of the batch of requests. So, the output is the
    preprocessed batch of requests being ready to be fed into the model.
    """"""
    decoded_images = tf.map_fn(
        _serving_preprocess, string_input, dtype=tf.float32, back_prop=False
    )
    return {transformed_name(IMAGE_KEY): decoded_images}


def model_exporter(model: tf.keras.Model):
    """"""
    model_exporter will be assigned to the ""serving_defaults"" signature.
    that means, when clients sends requests to the endpoint of this model
    hosted on TF Serving, the serving_fn which model_exporter returns will
    be the first point where the request payloads are going to be accepted.
    """"""
    m_call = tf.function(model.call).get_concrete_function(
        tf.TensorSpec(
            shape=[None, INPUT_IMG_SIZE, INPUT_IMG_SIZE, 3],
            dtype=tf.float32,
            name=transformed_name(IMAGE_KEY),
        )
    )

    @tf.function(input_signature=[tf.TensorSpec([None], tf.string)])
    def serving_fn(string_input):
        """"""
        serving_fn simply preprocesses the request payloads with the
        _serving_preprocess_fn, then the preprocessed data will be fed
        into the model. The model outputs predictions of the request p
        ayloads, then it will be returned back to the client after app
        lying postprocess of tf.math.argmax to the outputs(logits)
        """"""
        images = _serving_preprocess_fn(string_input)
        logits = m_call(**images)
        seg_mask = tf.math.argmax(logits, -1)
        return {""seg_mask"": seg_mask}

    return serving_fn

""""""
    Note that transform_features_signature and tf_examples_serving_signature
    functions exist only for model evaluation purposes with Evaluator component.
""""""


def transform_features_signature(
    model: tf.keras.Model, tf_transform_output: tft.TFTransformOutput
):
    """"""
    transform_features_signature simply returns a function that transforms
    any data of the type of tf.Example which is denoted as the type of sta
    ndard_artifacts.Examples in TFX. The purpose of this function is to ap
    ply Transform Graph obtained from Transform component to the data prod
    uced by ImportExampleGen. This function will be used in the Evaluator
    component, so the raw evaluation inputs from ImportExampleGen can be a
    pporiately transformed that the model could understand.
    """"""

    # basically, what Transform component emits is a SavedModel that knows
    # how to transform data. transform_features_layer() simply returns the
    # layer from the Transform.
    model.tft_layer = tf_transform_output.transform_features_layer()

    @tf.function(
        input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string, name=""examples"")]
    )
    def serve_tf_examples_fn(serialized_tf_examples):
        """"""
        raw_feature_spec returns a set of feature maps(dict) for the input
        TFRecords based on the knowledge that Transform component has lear
        ned(learn doesn't mean training here). By using this information,
        the raw data from ImportExampleGen could be parsed with tf.io.parse
        _example utility function.

        Then, it is passed to the model.tft_layer, so the final output we
        get is the transformed data of the raw input.
        """"""
        feature_spec = tf_transform_output.raw_feature_spec()
        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)

        transformed_features = model.tft_layer(parsed_features)

        return transformed_features

    return serve_tf_examples_fn


def tf_examples_serving_signature(model, tf_transform_output):
    """"""
    tf_examples_serving_signature simply returns a function that performs
    data transformation(preprocessing) and model prediction in a sequential
    manner. How data transformation is done is idential to the process of
    transform_features_signature function.
    """"""

    model.tft_layer = tf_transform_output.transform_features_layer()

    @tf.function(
        input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string, name=""examples"")]
    )
    def serve_tf_examples_fn(
        serialized_tf_example: tf.Tensor,
    ) -> Dict[str, tf.Tensor]:
        raw_feature_spec = tf_transform_output.raw_feature_spec()
        raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)

        transformed_features = model.tft_layer(raw_features)
        outputs = model(transformed_features)

        return {transformed_name(LABEL_KEY): outputs}

    return serve_tf_examples_fn
"
0,semantic-segmentation-ml-pipeline,training_pipeline/models/train.py,"from typing import List

import tensorflow as tf
import tensorflow_transform as tft
from tfx.components.trainer.fn_args_utils import DataAccessor, FnArgs
from tfx_bsl.tfxio import dataset_options

from .common import IMAGE_KEY, LABEL_KEY, NUM_LABELS
from .hyperparams import EPOCHS, EVAL_BATCH_SIZE, TRAIN_BATCH_SIZE
from .signatures import (
    model_exporter,
    tf_examples_serving_signature,
    transform_features_signature,
)
from .unet import build_model
from .utils import transformed_name

""""""
    _input_fn reads TFRecord files with the given file_pattern passed down 
    from the upstream TFX component, Transform. The file patterns are inter
    nally determined by Transform component, and they are automatically acce
    ssible through fn_args.train_files and fn_args.eval_files in the run_fn 
    function. Assume the dataset is already transformed appropriately. 
""""""


def _input_fn(
    file_pattern: List[str],
    data_accessor: DataAccessor,
    tf_transform_output: tft.TFTransformOutput,
    is_train: bool = False,
    batch_size: int = 200,
) -> tf.data.Dataset:
    """"""
    DataAccessor is responsible for accessing the data on disk, and
    TensorFlowDatasetOptions provides options for TFXIO's TensorFlowDataset.

    the factory function tf_dataset_factory takes three inputs of List[str],
    dataset_options.TensorFlowDatasetOptions, and schema_pb2.Schema. The
    schema_pb2.Schema holds the information how the TFRecords are structured,
    like what kind of features are accessible. In this case, there are two
    features of image_xf and label_xf, and they are the preprocessed results
    from Transform component.
    - Transform component simply preprocess the raw inputs, then returns the
    transformed output in TFRecord format. tf_dataset_factory is just a handy
    method to access TFRecord, and it is not strongly coupled with Transform
    component.

    by giving label_key option in the TensorFlowDataset, the tf_dataset_factory
    outputs the dataset in the form of Tuple[Dict[str, Tensor], Tensor]. Here,
    the second term will hold label information, and the first term holds what
    ever the rest is in the dataset (image_xf for this case).

    then, in the modeling part, you should have input layers with the names
    appearing in the first term Dict[str, Tensor]. For instance:
        inputs = tf.keras.layers.Input(..., name=""image_xf"")

    you could get rid of the label_key option, and it is totally optional. But
    then, you should have the output layer named with the label key. Otherwise,
    the model does not know which data from the Tuple to feed in the model. If
    you use label_key option, it it will be directly used in the output layer.
    """"""

    dataset = data_accessor.tf_dataset_factory(
        file_pattern,
        dataset_options.TensorFlowDatasetOptions(
            batch_size=batch_size, label_key=transformed_name(LABEL_KEY), shuffle=is_train
        ),
        tf_transform_output.transformed_metadata.schema,
    )

    return dataset


def run_fn(fn_args: FnArgs):
    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)

    train_dataset = _input_fn(
        fn_args.train_files,
        fn_args.data_accessor,
        tf_transform_output,
        is_train=True,
        batch_size=TRAIN_BATCH_SIZE,
    )

    eval_dataset = _input_fn(
        fn_args.eval_files,
        fn_args.data_accessor,
        tf_transform_output,
        is_train=False,
        batch_size=EVAL_BATCH_SIZE,
    )
    model = build_model(
        transformed_name(IMAGE_KEY), transformed_name(LABEL_KEY), NUM_LABELS
    )

    model.fit(
        train_dataset,
        steps_per_epoch=fn_args.train_steps,
        validation_data=eval_dataset,
        validation_steps=fn_args.eval_steps,
        epochs=EPOCHS,
    )

    model.save(
        fn_args.serving_model_dir,
        save_format=""tf"",
        signatures={
            ""serving_default"": model_exporter(model),
            ""transform_features"": transform_features_signature(
                model, tf_transform_output
            ),
            ""from_examples"": tf_examples_serving_signature(model, tf_transform_output),
        },
    )
"
0,semantic-segmentation-ml-pipeline,training_pipeline/models/unet.py,"import tensorflow as tf

from .hyperparams import INPUT_IMG_SIZE

""""""
    _build_model builds a UNET model. The implementation codes are
    borrowed from the [TF official tutorial on Semantic Segmentation]
    (https://www.tensorflow.org/tutorials/images/segmentation)
""""""


def build_model(input_name, label_name, num_labels) -> tf.keras.Model:
    base_model = tf.keras.applications.MobileNetV2(
        input_shape=[INPUT_IMG_SIZE, INPUT_IMG_SIZE, 3], include_top=False
    )

    # Use the activations of these layers
    layer_names = [
        ""block_1_expand_relu"",  # 64x64
        ""block_3_expand_relu"",  # 32x32
        ""block_6_expand_relu"",  # 16x16
        ""block_13_expand_relu"",  # 8x8
        ""block_16_project"",  # 4x4
    ]
    base_model_outputs = [base_model.get_layer(name).output for name in layer_names]

    # Create the feature extraction model
    down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)
    down_stack.trainable = False

    up_stack = [
        upsample(512, 3),  # 4x4 -> 8x8
        upsample(256, 3),  # 8x8 -> 16x16
        upsample(128, 3),  # 16x16 -> 32x32
        upsample(64, 3),  # 32x32 -> 64x64
    ]

    inputs = tf.keras.layers.Input(
        shape=[INPUT_IMG_SIZE, INPUT_IMG_SIZE, 3], name=input_name
    )

    # Downsampling through the model
    skips = down_stack(inputs)
    x = skips[-1]
    skips = reversed(skips[:-1])

    # Upsampling and establishing the skip connections
    for up, skip in zip(up_stack, skips):
        x = up(x)
        concat = tf.keras.layers.Concatenate()
        x = concat([x, skip])

    # This is the last layer of the model
    last = tf.keras.layers.Conv2DTranspose(
        filters=num_labels,
        kernel_size=3,
        strides=2,
        padding=""same"",
        name=label_name,
    )  # 64x64 -> 128x128

    x = last(x)

    model = tf.keras.Model(inputs=inputs, outputs=x)
    model.compile(
        optimizer=""adam"",
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        # by suppling ""accuracy"", Keras will automatically infer the appropriate variant.
        # in this case, sparse_categorical_accuracy will be chosen.
        metrics=[""accuracy""],
    )
    return model


""""""
    InstanceNormalization class and upsample function are
    borrowed from pix2pix in [TensorFlow Example repository](
    https://github.com/tensorflow/examples/tree/master/tensorflow_examples/models/pix2pix)
""""""


class InstanceNormalization(tf.keras.layers.Layer):
    """"""Instance Normalization Layer (https://arxiv.org/abs/1607.08022).""""""

    def __init__(self, epsilon=1e-5):
        super(InstanceNormalization, self).__init__()
        self.epsilon = epsilon

    def build(self, input_shape):
        self.scale = self.add_weight(
            name=""scale"",
            shape=input_shape[-1:],
            initializer=tf.random_normal_initializer(1.0, 0.02),
            trainable=True,
        )

        self.offset = self.add_weight(
            name=""offset"", shape=input_shape[-1:], initializer=""zeros"", trainable=True
        )

    def call(self, x):
        mean, variance = tf.nn.moments(x, axes=[1, 2], keepdims=True)
        inv = tf.math.rsqrt(variance + self.epsilon)
        normalized = (x - mean) * inv
        return self.scale * normalized + self.offset


def upsample(filters, size, norm_type=""batchnorm"", apply_dropout=False):
    """"""Upsamples an input.
    Conv2DTranspose => Batchnorm => Dropout => Relu
    Args:
      filters: number of filters
      size: filter size
      norm_type: Normalization type; either 'batchnorm' or 'instancenorm'.
      apply_dropout: If True, adds the dropout layer
    Returns:
      Upsample Sequential Model
    """"""

    initializer = tf.random_normal_initializer(0.0, 0.02)

    result = tf.keras.Sequential()
    result.add(
        tf.keras.layers.Conv2DTranspose(
            filters,
            size,
            strides=2,
            padding=""same"",
            kernel_initializer=initializer,
            use_bias=False,
        )
    )

    if norm_type.lower() == ""batchnorm"":
        result.add(tf.keras.layers.BatchNormalization())
    elif norm_type.lower() == ""instancenorm"":
        result.add(InstanceNormalization())

    if apply_dropout:
        result.add(tf.keras.layers.Dropout(0.5))

    result.add(tf.keras.layers.ReLU())

    return result
"
0,semantic-segmentation-ml-pipeline,training_pipeline/models/utils.py,"import absl


def INFO(text: str):
    absl.logging.info(text)


def transformed_name(key: str) -> str:
    return key + ""_xf""
"
0,semantic-segmentation-ml-pipeline,training_pipeline/pipeline/configs.py,"import os  # pylint: disable=unused-import

import tensorflow_model_analysis as tfma
import tfx.extensions.google_cloud_ai_platform.constants as vertex_const
import tfx.extensions.google_cloud_ai_platform.trainer.executor as vertex_training_const

PIPELINE_NAME = ""segmentation-training-pipeline""

try:
    import google.auth  # pylint: disable=g-import-not-at-top  # pytype: disable=import-error

    try:
        _, GOOGLE_CLOUD_PROJECT = google.auth.default()
    except google.auth.exceptions.DefaultCredentialsError:
        GOOGLE_CLOUD_PROJECT = ""gcp-ml-172005""
except ImportError:
    GOOGLE_CLOUD_PROJECT = ""gcp-ml-172005""

GOOGLE_CLOUD_REGION = ""us-central1""

GCS_BUCKET_NAME = GOOGLE_CLOUD_PROJECT + ""-complete-mlops""
PIPELINE_IMAGE = f""gcr.io/{GOOGLE_CLOUD_PROJECT}/{PIPELINE_NAME}""

OUTPUT_DIR = os.path.join(""gs://"", GCS_BUCKET_NAME)
PIPELINE_ROOT = os.path.join(OUTPUT_DIR, ""tfx_pipeline_output"", PIPELINE_NAME)

DATA_PATH = ""gs://pets-tfrecords/pets-tfrecords/""
SCHEMA_PATH = ""pipeline/schema.pbtxt"" # GCS path is also allowed

PREPROCESSING_FN = ""models.preprocessing.preprocessing_fn""
TRAINING_FN = ""models.train.run_fn""
TUNER_FN = ""models.train.tuner_fn""
CLOUD_TUNER_FN = ""models.train.tuner_fn""

GRADIO_APP_PATH = ""apps.gradio.img_classifier""
MODEL_HUB_REPO_PLACEHOLDER = ""$MODEL_REPO_ID""
MODEL_HUB_URL_PLACEHOLDER = ""$MODEL_REPO_URL""
MODEL_VERSION_PLACEHOLDER = ""$MODEL_VERSION""

TRAIN_NUM_STEPS = 160
EVAL_NUM_STEPS = 4

EXAMPLE_GEN_BEAM_ARGS = None
TRANSFORM_BEAM_ARGS = None

""""""
EVAL_CONFIGS is to configuration for the Evaluator component to define 
how it is going to evalua the model performance. The full spec follows 
the EvalConfig protocol buffer message, which can be found here: 
https://github.com/tensorflow/model-analysis/blob/v0.41.0/tensorflow_model_analysis/proto/config.proto

tfma.ModelSpec
    signature_name is one of the signature in the SavedModel from
    Trainer component, and it will be used to make predictions on 
    given data. preprocessing_function_names allows us to include
    a set of transformation(preprocessing) signatures in the Saved
    Model from Trainer component. 

    label_key and prediction_key will be used to compare the ground
    truth and prediction results.

slicing_specs
    we use the entire dataset to evaluate the model performance. If
    you want to evaluate the model based on different slices of data
    set, you should prepare TFRecords to have multiple features which
    of each corresponds to each slices(or categories), then write the
    slicing_specs options accordingly. Also we can evaluate the model 
    performance on different slices of data differently with PerSlice
    MetricThreshold in the metrics_specs section.
""""""
EVAL_CONFIGS = tfma.EvalConfig(
    model_specs=[
        tfma.ModelSpec(
            # the names in the signature_name preprocessing_function_names
            # are defined in the `signatures` parameter when model.save()
            # you can find how it is done in models/train.py
            signature_name=""from_examples"",
            preprocessing_function_names=[""transform_features""],
            label_key=""label_xf"",
            prediction_key=""label_xf"",
        )
    ],
    slicing_specs=[tfma.SlicingSpec()],
    metrics_specs=[
        tfma.MetricsSpec(
            metrics=[
                tfma.MetricConfig(
                    class_name=""SparseCategoricalAccuracy"",
                    threshold=tfma.MetricThreshold(
                        # value_threshold is normally defined to set the minimum
                        # performance threshold. That means a model whose perfor
                        # mance is better than this is going to be the first model,
                        # and it also means that it is only used when there is no
                        # model deployed in production yet.
                        value_threshold=tfma.GenericValueThreshold(
                            lower_bound={""value"": 0.55}
                        ),
                        # We can specify two models in the Evaluator component. One
                        # is the currently trained model, and the other one is the
                        # best model currently deployed(retrieved from the Artifact
                        # Store). change_threshold let us to define the threshold by
                        # how much the currently trained model should be better than
                        # the previous model to replace it.
                        change_threshold=tfma.GenericChangeThreshold(
                            direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                            absolute={""value"": -1e-3},
                        ),
                    ),
                )
            ]
        )
    ],
)

GCP_AI_PLATFORM_TRAINING_ARGS = {
    vertex_const.ENABLE_VERTEX_KEY: True,
    vertex_const.VERTEX_REGION_KEY: GOOGLE_CLOUD_REGION,
    vertex_training_const.TRAINING_ARGS_KEY: {
        ""project"": GOOGLE_CLOUD_PROJECT,
        ""worker_pool_specs"": [
            {
                ""machine_spec"": {
                    ""machine_type"": ""n1-standard-4"",
                    ""accelerator_type"": ""NVIDIA_TESLA_K80"",
                    ""accelerator_count"": 1,
                },
                ""replica_count"": 1,
                ""container_spec"": {
                    ""image_uri"": PIPELINE_IMAGE,
                },
            }
        ],
    },
    ""use_gpu"": True,
}

fullres_data = os.environ.get(""ENABLE_DATAFLOW"", ""false"")

if fullres_data.lower() == ""true"":
    DATA_PATH = ""gs://sidewalks-tfx-fullres/sidewalks-tfrecords/""

    DATAFLOW_SERVICE_ACCOUNT = ""csp-gde-dataflow@gcp-ml-172005.iam.gserviceaccount.com""
    DATAFLOW_MACHINE_TYPE = ""n1-standard-4""
    DATAFLOW_MAX_WORKERS = 4
    DATAFLOW_DISK_SIZE_GB = 100

    EXAMPLE_GEN_BEAM_ARGS = [
        ""--runner=DataflowRunner"",
        ""--project="" + GOOGLE_CLOUD_PROJECT,
        ""--region="" + GOOGLE_CLOUD_REGION,
        ""--service_account_email="" + DATAFLOW_SERVICE_ACCOUNT,
        ""--machine_type="" + DATAFLOW_MACHINE_TYPE,
        ""--experiments=use_runner_v2"",
        ""--max_num_workers="" + str(DATAFLOW_MAX_WORKERS),
        ""--disk_size_gb="" + str(DATAFLOW_DISK_SIZE_GB),
    ]

    TRANSFORM_BEAM_ARGS = [
        ""--runner=DataflowRunner"",
        ""--project="" + GOOGLE_CLOUD_PROJECT,
        ""--region="" + GOOGLE_CLOUD_REGION,
        ""--service_account_email="" + DATAFLOW_SERVICE_ACCOUNT,
        ""--machine_type="" + DATAFLOW_MACHINE_TYPE,
        ""--experiments=use_runner_v2"",
        ""--max_num_workers="" + str(DATAFLOW_MAX_WORKERS),
        ""--disk_size_gb="" + str(DATAFLOW_DISK_SIZE_GB),
        ""--worker_harness_container_image="" + PIPELINE_IMAGE,
    ]

    GCP_AI_PLATFORM_TRAINING_ARGS[vertex_training_const.TRAINING_ARGS_KEY][
        ""worker_pool_specs""
    ] = [
        {
            ""machine_spec"": {
                ""machine_type"": ""n1-standard-8"",
                ""accelerator_type"": ""NVIDIA_TESLA_V100"",
                ""accelerator_count"": 1,
            },
            ""replica_count"": 1,
            ""container_spec"": {
                ""image_uri"": PIPELINE_IMAGE,
            },
        }
    ]


GCP_AI_PLATFORM_SERVING_ARGS = {
    vertex_const.ENABLE_VERTEX_KEY: True,
    vertex_const.VERTEX_REGION_KEY: GOOGLE_CLOUD_REGION,
    vertex_const.VERTEX_CONTAINER_IMAGE_URI_KEY: ""us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest"",
    vertex_const.SERVING_ARGS_KEY: {
        ""project_id"": GOOGLE_CLOUD_PROJECT,
        ""deployed_model_display_name"": PIPELINE_NAME.replace(""-"", ""_""),
        ""endpoint_name"": ""prediction-"" + PIPELINE_NAME.replace(""-"", ""_""),
        ""traffic_split"": {""0"": 100},
        ""machine_type"": ""n1-standard-4"",
        ""min_replica_count"": 1,
        ""max_replica_count"": 1,
    },
}

HF_PUSHER_ARGS = {
    ""username"": ""chansung"",
    ""access_token"": ""$HF_ACCESS_TOKEN"",
    ""repo_name"": PIPELINE_NAME,
    ""space_config"": {
        ""app_path"": ""apps.gradio.semantic_segmentation"",
    },
}
"
0,semantic-segmentation-ml-pipeline,training_pipeline/pipeline/local_pipeline.py,"from typing import Any, Dict, List, Optional, Text

from tfx import v1 as tfx
import tensorflow_model_analysis as tfma

from ml_metadata.proto import metadata_store_pb2
from tfx.proto import example_gen_pb2

import absl
import tensorflow_model_analysis as tfma
from tfx.components import ImportExampleGen
from tfx.components import StatisticsGen
from tfx.components import Trainer
from tfx.components import Transform
from tfx.components import Evaluator
from tfx.components import Pusher
from tfx.orchestration import pipeline
from tfx.proto import example_gen_pb2
from tfx.proto import trainer_pb2
from tfx.types import Channel

from tfx.types.standard_artifacts import Model
from tfx.types.standard_artifacts import ModelBlessing
from tfx.dsl.components.common import resolver
from tfx.dsl.experimental.latest_blessed_model_resolver import (
    LatestBlessedModelResolver,
)

def create_pipeline(
    pipeline_name: Text,
    pipeline_root: Text,
    data_path: Text,
    schema_path: Text,
    modules: Dict[Text, Text],
    train_args: trainer_pb2.TrainArgs,
    eval_args: trainer_pb2.EvalArgs,
    eval_configs: tfma.EvalConfig,
    serving_model_dir: Text,
    metadata_connection_config: Optional[metadata_store_pb2.ConnectionConfig] = None,
) -> tfx.dsl.Pipeline:
    components = []

    input_config = example_gen_pb2.Input(
        splits=[
            example_gen_pb2.Input.Split(name=""train"", pattern=""train-00-*.tfrec""),
            example_gen_pb2.Input.Split(name=""eval"", pattern=""val-00-*.tfrec""),
        ]
    )
    example_gen = ImportExampleGen(input_base=data_path, input_config=input_config)
    components.append(example_gen)

    statistics_gen = StatisticsGen(examples=example_gen.outputs[""examples""])
    components.append(statistics_gen)

    schema_gen = tfx.components.ImportSchemaGen(schema_file=schema_path)
    components.append(schema_gen)

    transform = Transform(
        examples=example_gen.outputs[""examples""],
        schema=schema_gen.outputs[""schema""],
        preprocessing_fn=modules[""preprocessing_fn""],
    )
    components.append(transform)

    trainer = Trainer(
        run_fn=modules[""training_fn""],
        transformed_examples=transform.outputs[""transformed_examples""],
        transform_graph=transform.outputs[""transform_graph""],
        schema=schema_gen.outputs[""schema""],
        train_args=train_args,
        eval_args=eval_args,
    )
    components.append(trainer)

    model_resolver = resolver.Resolver(
        strategy_class=LatestBlessedModelResolver,
        model=Channel(type=Model),
        model_blessing=Channel(type=ModelBlessing),
    ).with_id(""latest_blessed_model_resolver"")
    components.append(model_resolver)

    evaluator = Evaluator(
        examples=example_gen.outputs[""examples""],
        model=trainer.outputs[""model""],
        baseline_model=model_resolver.outputs[""model""],
        eval_config=eval_configs,
    )
    components.append(evaluator)

    pusher_args = {
        ""model"": trainer.outputs[""model""],
        ""model_blessing"": evaluator.outputs['blessing'],
        ""push_destination"": tfx.proto.PushDestination(
            filesystem=tfx.proto.PushDestination.Filesystem(
                base_directory=serving_model_dir
            )
        ),
    }
    pusher = Pusher(**pusher_args)  # pylint: disable=unused-variable
    components.append(pusher)

    return pipeline.Pipeline(
        pipeline_name=pipeline_name,
        pipeline_root=pipeline_root,
        components=components,
        enable_cache=False,
        metadata_connection_config=metadata_connection_config,
    )
"
0,semantic-segmentation-ml-pipeline,training_pipeline/pipeline/pipeline.py,"from typing import Any, Dict, List, Optional, Text

import tensorflow_model_analysis as tfma
from ml_metadata.proto import metadata_store_pb2
from tfx import v1 as tfx
from tfx.components import (
    Evaluator,
    ImportExampleGen,
    StatisticsGen,
    Transform,
)
from tfx.dsl.components.common import resolver
from tfx.dsl.experimental.latest_blessed_model_resolver import LatestBlessedModelResolver
from tfx.extensions.google_cloud_ai_platform.pusher.component import (
    Pusher as VertexPusher,
)
from tfx.extensions.google_cloud_ai_platform.trainer.component import (
    Trainer as VertexTrainer,
)
from tfx.orchestration import pipeline
from tfx.proto import example_gen_pb2, trainer_pb2
from tfx.types import Channel
from tfx.types.standard_artifacts import Model, ModelBlessing

from pipeline.components.HFPusher.component import HFPusher


def create_pipeline(
    pipeline_name: Text,
    pipeline_root: Text,
    data_path: Text,
    schema_path: Text,
    modules: Dict[Text, Text],
    train_args: trainer_pb2.TrainArgs,
    eval_args: trainer_pb2.EvalArgs,
    eval_configs: tfma.EvalConfig,
    metadata_connection_config: Optional[metadata_store_pb2.ConnectionConfig] = None,
    ai_platform_training_args: Optional[Dict[Text, Text]] = None,
    ai_platform_serving_args: Optional[Dict[Text, Any]] = None,
    example_gen_beam_args: Optional[List] = None,
    transform_beam_args: Optional[List] = None,
    hf_pusher_args: Optional[Dict[Text, Any]] = None,
) -> tfx.dsl.Pipeline:
    components = []

    # Data splitting config.
    input_config = example_gen_pb2.Input(
        splits=[
            example_gen_pb2.Input.Split(name=""train"", pattern=""train-*.tfrec""),
            example_gen_pb2.Input.Split(name=""eval"", pattern=""val-*.tfrec""),
        ]
    )

    # Data input (pipeline starts here).
    example_gen = ImportExampleGen(input_base=data_path, input_config=input_config)
    if example_gen_beam_args is not None:
        example_gen.with_beam_pipeline_args(example_gen_beam_args)
    components.append(example_gen)

    # Generate stats from the data. Useful for preprocessing, post-processing,
    # anomaly detection, etc.
    statistics_gen = StatisticsGen(examples=example_gen.outputs[""examples""])
    components.append(statistics_gen)

    schema_gen = tfx.components.ImportSchemaGen(schema_file=schema_path)
    components.append(schema_gen)

    # Apply any preprocessing. Transformations get saved as a graph in a SavedModel.
    transform = Transform(
        examples=example_gen.outputs[""examples""],
        schema=schema_gen.outputs[""schema""],
        preprocessing_fn=modules[""preprocessing_fn""],
    )
    if transform_beam_args is not None:
        transform.with_beam_pipeline_args(transform_beam_args)
    components.append(transform)

    # Training.
    trainer_args = {
        ""run_fn"": modules[""training_fn""],
        ""transformed_examples"": transform.outputs[""transformed_examples""],
        ""transform_graph"": transform.outputs[""transform_graph""],
        ""schema"": schema_gen.outputs[""schema""],
        ""train_args"": train_args,
        ""eval_args"": eval_args,
        ""custom_config"": ai_platform_training_args,
    }
    trainer = VertexTrainer(**trainer_args)
    components.append(trainer)

    # Resolver component - did we do better than the previous model?
    model_resolver = resolver.Resolver(
        strategy_class=LatestBlessedModelResolver,
        model=Channel(type=Model),
        model_blessing=Channel(type=ModelBlessing),
    ).with_id(""latest_blessed_model_resolver"")
    components.append(model_resolver)

    # Evaluate the model.
    evaluator = Evaluator(
        examples=example_gen.outputs[""examples""],
        model=trainer.outputs[""model""],
        baseline_model=model_resolver.outputs[""model""],
        eval_config=eval_configs,
    )
    components.append(evaluator)

    # Based on blessing status, push the model to prod (deployment stage.)
    pusher_args = {
        ""model"": trainer.outputs[""model""],
        ""model_blessing"": evaluator.outputs[""blessing""],
        ""custom_config"": ai_platform_serving_args,
    }
    pusher = VertexPusher(**pusher_args)  # pylint: disable=unused-variable
    components.append(pusher)

    # Push the blesses model to HF hub and deploy a demo app on Hugging Face
    # Spaces.
    hf_pusher_args[""model""] = trainer.outputs[""model""]
    hf_pusher_args[""model_blessing""] = evaluator.outputs[""blessing""]
    hf_pusher = HFPusher(**hf_pusher_args)
    components.append(hf_pusher)

    return pipeline.Pipeline(
        pipeline_name=pipeline_name,
        pipeline_root=pipeline_root,
        components=components,
        enable_cache=True,
        metadata_connection_config=metadata_connection_config,
    )
"
0,semantic-segmentation-ml-pipeline,training_pipeline/apps/gradio/semantic_segmentation/app.py,"import gradio as gr
import numpy as np
import tensorflow as tf
from huggingface_hub import from_pretrained_keras
from PIL import Image

MODEL_CKPT = ""$MODEL_REPO_ID@$MODEL_VERSION""
MODEL = from_pretrained_keras(MODEL_CKPT)

RESOLTUION = 128

PETS_PALETTE = []
with open(r""./palette.txt"", ""r"") as fp:
    for line in fp:
        if ""#"" not in line:
            tmp_list = list(map(int, line[:-1].strip(""]["").split("", "")))
            PETS_PALETTE.append(tmp_list)


def preprocess_input(image: Image) -> tf.Tensor:
    image = np.array(image)
    image = tf.convert_to_tensor(image)

    image = tf.image.resize(image, (RESOLTUION, RESOLTUION))
    image = image / 255

    return tf.expand_dims(image, 0)


# The below utility get_seg_overlay() are from:
# https://github.com/deep-diver/semantic-segmentation-ml-pipeline/blob/main/notebooks/inference_from_SavedModel.ipynb


def get_seg_overlay(image, seg):
    color_seg = np.zeros(
        (seg.shape[0], seg.shape[1], 3), dtype=np.uint8
    )  # height, width, 3
    palette = np.array(PETS_PALETTE)

    for label, color in enumerate(palette):
        color_seg[seg == label, :] = color

    # Show image + mask
    img = np.array(image) * 0.5 + color_seg * 0.5

    img *= 255
    img = np.clip(img, 0, 255)
    img = img.astype(np.uint8)
    return img


def run_model(image: Image) -> tf.Tensor:
    preprocessed_image = preprocess_input(image)
    prediction = MODEL.predict(preprocessed_image)

    seg_mask = tf.math.argmax(prediction, -1)
    seg_mask = tf.squeeze(seg_mask)
    return seg_mask


def get_predictions(image: Image):
    predicted_segmentation_mask = run_model(image)
    preprocessed_image = preprocess_input(image)
    preprocessed_image = tf.squeeze(preprocessed_image, 0)

    pred_img = get_seg_overlay(
        preprocessed_image.numpy(), predicted_segmentation_mask.numpy()
    )
    return Image.fromarray(pred_img)


title = (
    ""Simple demo for a semantic segmentation model trained on the PETS dataset to classify inside, outside, and border of an object.""
)

description = """"""

Note that the outputs obtained in this demo won't be state-of-the-art. The underlying project has a different objective focusing more on the ops side of
deploying a semantic segmentation model. For more details, check out the repository: https://github.com/deep-diver/semantic-segmentation-ml-pipeline/.

""""""

demo = gr.Interface(
    get_predictions,
    gr.inputs.Image(type=""pil""),
    ""pil"",
    allow_flagging=""never"",
    title=title,
    description=description,
    examples=[[""test-image1.png""], [""test-image2.png""], [""test-image3.png""], [""test-image4.png""], [""test-image5.png""]],
)

demo.launch()
"
0,semantic-segmentation-ml-pipeline,training_pipeline/pipeline/components/HFPusher/__init__.py,"# Copyright 2022 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# =============================================================================="
0,semantic-segmentation-ml-pipeline,training_pipeline/pipeline/components/HFPusher/component.py,"# Copyright 2022 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
""""""HuggingFace(HF) Pusher TFX Component.
The HFPusher is used to push model and prototype application to HuggingFace Hub.
""""""
from typing import Text, Dict, Any, Optional

from tfx import types
from tfx.dsl.components.base import base_component, executor_spec
from tfx.types import standard_artifacts
from tfx.types.component_spec import ChannelParameter, ExecutionParameter

from pipeline.components.HFPusher import executor

MODEL_KEY = ""model""
PUSHED_MODEL_KEY = ""pushed_model""
MODEL_BLESSING_KEY = ""model_blessing""


class HFPusherSpec(types.ComponentSpec):
    """"""ComponentSpec for TFX HFPusher Component.""""""

    PARAMETERS = {
        ""username"": ExecutionParameter(type=str),
        ""access_token"": ExecutionParameter(type=str),
        ""repo_name"": ExecutionParameter(type=str),
        ""space_config"": ExecutionParameter(type=Dict[Text, Any], optional=True),
    }
    INPUTS = {
        MODEL_KEY: ChannelParameter(type=standard_artifacts.Model, optional=True),
        MODEL_BLESSING_KEY: ChannelParameter(
            type=standard_artifacts.ModelBlessing, optional=True
        ),
    }
    OUTPUTS = {
        PUSHED_MODEL_KEY: ChannelParameter(type=standard_artifacts.PushedModel),
    }


class HFPusher(base_component.BaseComponent):
    """"""Component for pushing model and application to HuggingFace Hub.

    The `HFPusher` is a [TFX Component](https://www.tensorflow.org/tfx
    /guide/understanding_tfx_pipelines#component), and its primary pur
    pose is to push a model from an upstream component such as [`Train
    er`](https://www.tensorflow.org/tfx/guide/trainer) to HuggingFace
    Model Hub. It also provides a secondary feature that pushes an app
    lication to HuggingFace Space Hub.
    """"""

    SPEC_CLASS = HFPusherSpec
    EXECUTOR_SPEC = executor_spec.ExecutorClassSpec(executor.Executor)

    def __init__(
        self,
        username: str,
        access_token: str,
        repo_name: str,
        space_config: Optional[Dict[Text, Any]] = None,
        model: Optional[types.Channel] = None,
        model_blessing: Optional[types.Channel] = None,
    ):
        """"""The HFPusher TFX component.

        HFPusher pushes a trained or blessed model to HuggingFace Model Hub.
        This is designed to work as a downstream component of Trainer and o
        ptionally Evaluator(optional) components. Trainer gives trained mod
        el, and Evaluator gives information whether the trained model is bl
        essed or not after evaluation of the model. HFPusher component only
        publishes a model when it is blessed. If Evaluator is not specified,
        the input model will always be pushed.

        Args:
        username: the ID of HuggingFace Hub
        access_token: the access token obtained from HuggingFace Hub for the
            given username. Refer to [this document](https://huggingface.co/
            docs/hub/security-tokens) to know how to obtain one.
        repo_name: the name of Model Hub repository where the model will be
            pushed. This should be unique name under the username within th
            e Model Hub. repository is identified as {username}/{repo_name}.
        space_config: optional configurations set when to push an application
            to HuggingFace Space Hub. This is a dictionary, and the following
            information could be set.
            app_path: the path where the application related files are stored.
                this should follow the form either of app.gradio.segmentation
                or app/gradio/segmentation. This is a required parameter when
                space_config is set. This could be a local or GCS paths.
            space_sdk: Space Hub supports gradio, streamit, and static types
                of application. The default is set to gradio.
            placeholders: placeholders to replace in every files under the a
                pp_path. This is used to replace special string with the mod
                el related values. If this is not set, the default placehold
                ers will be used as follows.
                ```
                placeholders = {
                    ""MODEL_REPO_ID"" : ""$MODEL_REPO_ID"",
                    ""MODEL_REPO_URL"": ""$MODEL_REPO_URL"",
                    ""MODEL_VERSION"" : ""$MODEL_VERSION"",
                }
                ```
                In this case, ""$MODEL_REPO_ID"", ""$MODEL_REPO_URL"", ""$MODEL_VE
                RSION"" strings will be replaced with appropriate values at ru
                ntime. If placeholders are set, custom strings will be used.
            repo_name: the name of Space Hub repository where the application
                will be pushed. This should be unique name under the username
                within the Space Hub. repository is identified as {username}/
                {repo_name}. If this is not set, the same name to the Model H
                ub repository will be used.
        model: a TFX input channel containing a Model artifact. this is usually
            comes from the standard [`Trainer`]
            (https://www.tensorflow.org/tfx/guide/trainer) component.
        model_blessing: a TFX input channel containing a ModelBlessing artifact.
            this is usually comes from the standard [`Evaluator`]
            (https://www.tensorflow.org/tfx/guide/evaluator) component.
        Returns:
        a TFX output channel containing a PushedModel artifact. It contains
        information where the model is published at and whether the model is
        pushed or not.

        Raises:
            RuntimeError: if app_path is not set when space_config is provided.
        Example:

        Basic usage example:
        ```py
        trainer = Trainer(...)
        evaluator = Evaluator(...)
        hf_pusher = HFPusher(
            username=""chansung"",
            access_token=<YOUR-HUGGINGFACE-ACCESS-TOKEN>,
            repo_name=""my-model"",
            model=trainer.outputs[""model""],
            model_blessing=evaluator.outputs[""blessing""],
            space_config={
                ""app_path"": ""apps.gradio.semantic_segmentation""
            }
        )
        ```
        """"""

        pushed_model = types.Channel(type=standard_artifacts.PushedModel)

        spec = HFPusherSpec(
            username=username,
            access_token=access_token,
            repo_name=repo_name,
            space_config=space_config,
            model=model,
            model_blessing=model_blessing,
            pushed_model=pushed_model,
        )

        super().__init__(spec=spec)
"
0,semantic-segmentation-ml-pipeline,training_pipeline/pipeline/components/HFPusher/component_test.py,"# Copyright 2022 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
""""""Tests for TFX HuggingFace Pusher Custom Component.""""""

import tensorflow as tf
from tfx.types import standard_artifacts
from tfx.types import channel_utils

from pipeline.components.HFPusher.component import HFPusher


class HFPusherTest(tf.test.TestCase):
    def testConstruct(self):
        test_model = channel_utils.as_channel([standard_artifacts.Model()])
        hf_pusher = HFPusher(
            username=""test_username"",
            access_token=""test_access_token"",
            repo_name=""test_repo_name"",
            model=test_model,
        )

        self.assertEqual(
            standard_artifacts.PushedModel.TYPE_NAME,
            hf_pusher.outputs[""pushed_model""].type_name,
        )


if __name__ == ""__main__"":
    tf.test.main()
"
0,semantic-segmentation-ml-pipeline,training_pipeline/pipeline/components/HFPusher/executor.py,"# Copyright 2022 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
""""""HF Pusher TFX Component Executor. The HF Pusher Executor calls 
the workflow handler runner.deploy_model_for_hf_hub().
""""""

import ast
import time
from typing import Any, Dict, List

from tfx import types
from tfx.components.pusher import executor as tfx_pusher_executor
from tfx.types import artifact_utils, standard_component_specs

from pipeline.components.HFPusher import runner

_USERNAME_KEY = ""username""
_ACCESS_TOKEN_KEY = ""access_token""
_REPO_NAME_KEY = ""repo_name""
_SPACE_CONFIG_KEY = ""space_config""


class Executor(tfx_pusher_executor.Executor):
    """"""Pushes a model and an app to HuggingFace Model and Space Hubs respectively""""""

    def Do(
        self,
        input_dict: Dict[str, List[types.Artifact]],
        output_dict: Dict[str, List[types.Artifact]],
        exec_properties: Dict[str, Any],
    ):
        """"""Overrides the tfx_pusher_executor to leverage some of utility methods
        Args:
          input_dict: Input dict from input key to a list of artifacts, including:
            - model_export: a TFX input channel containing a Model artifact.
            - model_blessing: a TFX input channel containing a ModelBlessing
              artifact.
          output_dict: Output dict from key to a list of artifacts, including:
            - pushed_model: a TFX output channel containing a PushedModel arti
              fact. It contains information where the model is published at an
              d whether the model is pushed or not. furthermore, pushed model
              carries the following information.
              - pushed : integer value to denote if the model is pushed or not.
                This is set to 0 when the input model is not blessed, and it is
                set to 1 when the model is successfully pushed.
              - pushed_version : string value to indicate the current model ver
                sion. This is decided by time.time() Python built-in function.
              - repo_id : model repository ID where the model is pushed to. This
                follows the format of f""{username}/{repo_name}"".
              - branch : branch name where the model is pushed to. The branch na
                me is automatically assigned to the same value of pushed_version.
              - commit_id : the id from the commit history (branch name could be
                sufficient to retreive a certain version of the model) of the mo
                del repository.
              - repo_url : model repository URL. It is something like f""https://
                huggingface.co/{repo_id}/{branch}""
              - space_url : space repository URL. It is something like f""https://
                huggingface.co/{repo_id}""f
          exec_properties: An optional dict of execution properties, including:
            - username: username of the HuggingFace user (can be an individual
              user or an organization)
            - access_token: access token value issued by HuggingFace for the s
              pecified username.
            - repo_name: the repository name to push the current version of the
              model to. The default value is same as the TFX pipeline name.
            - space_config: space_config carries additional values such as:
              - app_path : path where the application templates are in the cont
                ainer that runs the TFX pipeline. This is expressed either apps.
                `gradio.img_classifier` or `apps/gradio.img_classifier`.
              - repo_name : the repository name to push the application to. The
                default value is same as the TFX pipeline name
              - space_sdk : either gradio or streamlit. this will decide which a
                pplication framework to be used for the Space repository. The de
                fault value is gradio
              - placeholders : dictionary which placeholders to replace with mod
                el specific information. The keys represents descriptions, and t
                he values represents the actual placeholders to replace in the f
                iles under the `app_path`. There are currently two predefined keys,
                and if placeholders is set to None, the default values will be used.
        """"""
        self._log_startup(input_dict, output_dict, exec_properties)

        model_push = artifact_utils.get_single_instance(
            output_dict[standard_component_specs.PUSHED_MODEL_KEY]
        )

        # if the model is not blessed
        if not self.CheckBlessing(input_dict):
            self._MarkNotPushed(model_push)
            return
        model_path = self.GetModelPath(input_dict)
        model_version_name = f""v{int(time.time())}""

        space_config = exec_properties.get(_SPACE_CONFIG_KEY, None)
        if space_config is not None:
            space_config = ast.literal_eval(space_config)

        pushed_properties = runner.deploy_model_for_hf_hub(
            username=exec_properties.get(_USERNAME_KEY, None),
            access_token=exec_properties.get(_ACCESS_TOKEN_KEY, None),
            repo_name=exec_properties.get(_REPO_NAME_KEY, None),
            space_config=space_config,
            model_path=model_path,
            model_version=model_version_name,
        )

        self._MarkPushed(model_push, pushed_destination=pushed_properties[""repo_url""])
        for key in pushed_properties:
            value = pushed_properties[key]

            if key != ""repo_url"":
                model_push.set_string_custom_property(key, value)
"
0,semantic-segmentation-ml-pipeline,training_pipeline/pipeline/components/HFPusher/model_card.py,"from huggingface_hub import ModelCard, ModelCardData


def create_card(template_path, model_metadata, **template_kwargs):
    """"""Creates model card.

    Args:
        template_path (str): Path to the jinja template model is based on.
        model_metadata (dict): Dict of card metadata.
        Refer to the link to know what you can pass to the metadata section:
        https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/modelcard_template.md

    Returns:
        Model card (ModelCard): instantiated and filled ModelCard.
    """"""
    if model_metadata is None:
        model_metadata = {}
    model_metadata[""library_name""] = ""tfx""
    if ""tags"" in model_metadata:
        model_metadata[""tags""].append(""keras"")
    else:
        model_metadata[""tags""] = ""keras""
    card_data = ModelCardData(**{v: k for k, v in model_metadata.items()})
    model_card = ModelCard.from_template(card_data,
                                        template_path,
                                        **template_kwargs)
    return model_card
    
"
0,semantic-segmentation-ml-pipeline,training_pipeline/pipeline/components/HFPusher/runner.py,"# Copyright 2022 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
""""""HuggingFace Pusher runner module.
This module handles the workflow to publish
machine learning model to HuggingFace Hub.
""""""
from typing import Text, Any, Dict, Optional

import mimetypes
import tempfile
import tensorflow as tf
from absl import logging
from tfx.utils import io_utils
from pipeline.components.HFPusher.model_card import create_card
from pathlib import Path

from huggingface_hub import Repository
from huggingface_hub import HfApi
from requests.exceptions import HTTPError

_MODEL_REPO_KEY = ""MODEL_REPO_ID""
_MODEL_URL_KEY = ""MODEL_REPO_URL""
_MODEL_VERSION_KEY = ""MODEL_VERSION""

_DEFAULT_MODEL_REPO_PLACEHOLDER_KEY = ""$MODEL_REPO_ID""
_DEFAULT_MODEL_URL_PLACEHOLDER_KEY = ""$MODEL_REPO_URL""
_DEFAULT_MODEL_VERSION_PLACEHOLDER_KEY = ""$MODEL_VERSION""

def _is_text_file(path):
    mimetype = mimetypes.guess_type(path)
    if mimetype[0] != None:
        return 'text' in mimetype[0]
    
    return False

def _replace_placeholders_in_files(
    root_dir: str, placeholder_to_replace: Dict[str, str]
):
    """"""Recursively open every files under the root_dir, and then
    replace special tokens with the given values in placeholder_
    to_replace""""""
    files = tf.io.gfile.listdir(root_dir)
    for file in files:
        path = tf.io.gfile.join(root_dir, file)

        if tf.io.gfile.isdir(path):
            _replace_placeholders_in_files(path, placeholder_to_replace)
        else:
            _replace_placeholders_in_file(path, placeholder_to_replace)


def _replace_placeholders_in_file(
    filepath: str, placeholder_to_replace: Dict[str, str]
):
    """"""replace special tokens with the given values in placeholder_
    to_replace. This function gets called by _replace_placeholders
    _in_files function""""""
    if _is_text_file(filepath):
        with tf.io.gfile.GFile(filepath, ""r"") as f:
            source_code = f.read()

        for placeholder in placeholder_to_replace:
            source_code = source_code.replace(
                placeholder, placeholder_to_replace[placeholder]
            )

        with tf.io.gfile.GFile(filepath, ""w"") as f:
            f.write(source_code)


def _replace_placeholders(
    target_dir: str,
    placeholders: Dict[str, str],
    model_repo_id: str,
    model_repo_url: str,
    model_version: str,
):
    """"""set placeholder_to_replace before calling _replace_placeholde
    rs_in_files function""""""

    if placeholders is None:
        placeholders = {
            _MODEL_REPO_KEY: _DEFAULT_MODEL_REPO_PLACEHOLDER_KEY,
            _MODEL_URL_KEY: _DEFAULT_MODEL_URL_PLACEHOLDER_KEY,
            _MODEL_VERSION_KEY: _DEFAULT_MODEL_VERSION_PLACEHOLDER_KEY,
        }

    placeholder_to_replace = {
        placeholders[_MODEL_REPO_KEY]: model_repo_id,
        placeholders[_MODEL_URL_KEY]: model_repo_url,
        placeholders[_MODEL_VERSION_KEY]: model_version,
    }
    _replace_placeholders_in_files(target_dir, placeholder_to_replace)


def _replace_files(src_path, dst_path):
    """"""Replace the contents(files/folders) of the repository with the
    latest contents.

    Args:
        src_path (path or str): Path of the source repository where latest
        content exists.
        dst_path (path or str): Path to the destination repository that
        contains old files.
    """"""

    not_to_delete = ["".gitattributes"", "".git""]

    inside_root_dst_path = tf.io.gfile.listdir(dst_path)

    for content_name in inside_root_dst_path:
        content = f""{dst_path}/{content_name}""

        if content_name not in not_to_delete:
            if tf.io.gfile.isdir(content):
                tf.io.gfile.rmtree(content)
            else:
                tf.io.gfile.remove(content)

    inside_root_src_path = tf.io.gfile.listdir(src_path)

    for content_name in inside_root_src_path:
        content = f""{src_path}/{content_name}""
        dst_content = f""{dst_path}/{content_name}""

        if tf.io.gfile.isdir(content):
            io_utils.copy_dir(content, dst_content)
        else:
            tf.io.gfile.copy(content, dst_content)


def _create_remote_repo(
    access_token: str, repo_id: str, repo_type: str = ""model"", space_sdk: str = None
):
    """"""Create a remote repository on HuggingFace Hub platform. HTTPError
    exception is raised when the repository already exists

    Args:
        access_token (str): Hugging Face Hub token with write access.
        repo_id (str): Repository ID to push the repository.
        repo_type (str, optional): Repository type. Defaults to ""model"".
        space_sdk (str, optional): SDK for Space. Defaults to None.
    """"""

    logging.info(f""repo_id: {repo_id}"")
    try:
        HfApi().create_repo(
            token=access_token,
            repo_id=repo_id,
            repo_type=repo_type,
            space_sdk=space_sdk,
        )
    except HTTPError:
        logging.warning(
            f""this warning is expected if {repo_id} repository already exists""
        )


def _clone_and_checkout(
    repo_url: str, local_path: str, access_token: str, version: Optional[str] = None
) -> Repository:
    """"""clone the remote repository to the given local_path""""""

    repository = Repository(
        local_dir=local_path, clone_from=repo_url, use_auth_token=access_token
    )

    if version is not None:
        repository.git_checkout(revision=version, create_branch_ok=True)

    return repository


def _push_to_remote_repo(repo: Repository, commit_msg: str, branch: str = ""main""):
    """"""Push any changes to the remote repository

    Args:
        repo (Repository): Repository class from huggingface_hub.
        commit_msg (str): Commit message.
        branch (str, optional): Branch to push the model. Defaults to ""main"".
    """"""

    repo.git_add(pattern=""."", auto_lfs_track=True)
    repo.git_commit(commit_message=commit_msg)
    repo.git_push(upstream=f""origin {branch}"")


def deploy_model_for_hf_hub(
    username: str,
    access_token: str,
    repo_name: str,
    model_path: str,
    model_version: str,
    space_config: Optional[Dict[Text, Any]] = None,
    model_metadata: Optional[Dict] = None,
    **template_kwargs,

) -> Dict[str, str]:
    """"""Executes ML model deployment workflow to HuggingFace Hub. Refer to the
    HFPusher component in component.py for generic description of each parame
    ter. This docstring only explains how the workflow works.

    step 1. push model to the Model Hub
    step 1-1.
        create a repository on the HuggingFace Hub. if there is an existing r
        epository with the given repo_name, that rpository will be overwritten.
    step 1-2.
        clone the created or existing remote repository to the local path. Al
        so, create a branch named with model version.
    step 1-3.
        remove every files under the cloned repository(local), and copies the
        model related files to the cloned local repository path.
    step 1-4
        write model card.
    step 1-5.
        push the updated repository to the given branch of remote Model Hub.

    step 2. push application to the Space Hub
    step 2-1.
        create a repository on the HuggingFace Hub. if there is an existing r
        epository with the given repo_name, that rpository will be overwritten.
    step 2-2.
        copies directory where the application related files are stored to a
        temporary directory. Since the files could be hosted in GCS bucket, t
        his process ensures every necessary files are located in the local fil
        e system.
    step 2-3.
        replace speical tokens in every files under the given directory.
    step 2-4.
        clone the created or existing remote repository to the local path.
    step 2-5.
        remove every files under the cloned repository(local), and copies the
        application related files to the cloned local repository path.
    step 2-6.
        push the updated repository to the remote Space Hub. note that the br
        anch is always set to ""main"", so that HuggingFace Space could build t
        he application automatically when pushed.
    
    Args:
        username (str): Hugging Face Hub username.
        access_token (str): Hugging Face Hub access token.
        repo_name (str): Name of the repository.
        model_path (str): Path to model file.
        model_version (str): Model version.
        space_config (dict): Configuration for Space.
        model_metadata (dict): Metadata for model card.

        

    """"""
    outputs = {}

    # step 1
    repo_url_prefix = ""https://huggingface.co""
    repo_id = f""{username}/{repo_name}""
    repo_url = f""{repo_url_prefix}/{repo_id}""

    # step 1-1
    _create_remote_repo(access_token=access_token, repo_id=repo_id)
    logging.info(f""remote repository at {repo_url} is prepared"")

    # step 1-2
    local_path = ""hf_model""

    repository = _clone_and_checkout(
        repo_url=repo_url,
        local_path=local_path,
        access_token=access_token,
        version=model_version,
    )
    logging.info(
        f""remote repository is cloned, and new branch {model_version} is created""
    )
    # step 1-3
    if not model_metadata:
        model_metadata = {}
    card = create_card(model_metadata = model_metadata, 
                        template_path = ""model_card_template.md"",
                        **{""model_id"":_MODEL_REPO_KEY, **template_kwargs})

    with open(Path(model_path) / ""README.md"", ""w+"") as fp:
        fp.write(str(card))

    # step 1-4
    _replace_files(model_path, local_path)

    logging.info(
        ""current version of the model is copied to the cloned local repository""
    )



    # step 1-5
    _push_to_remote_repo(
        repo=repository,
        commit_msg=f""upload new version({model_version})"",
        branch=model_version,
    )
    logging.info(""updates are pushed to the remote repository"")

    outputs[""repo_id""] = repo_id
    outputs[""branch""] = model_version
    outputs[""commit_id""] = f""{repository.git_head_hash()}""
    outputs[""repo_url""] = repo_url

    # step 2
    if space_config is not None:
        if ""app_path"" not in space_config:
            raise RuntimeError(
                f""the app_path is not provided. ""
                f""app_path is required when space_config is set.""
            )

        model_repo_id = repo_id
        model_repo_url = repo_url

        if ""repo_name"" in space_config:
            repo_id = f""{username}/{repo_name}""
            repo_url = f""{repo_url_prefix}/{repo_id}""
        else:
            repo_url = f""{repo_url_prefix}/spaces/{repo_id}""
        app_path = space_config[""app_path""]
        app_path = app_path.replace(""."", ""/"")

        # step 2-1
        _create_remote_repo(
            access_token=access_token,
            repo_id=repo_id,
            repo_type=""space"",
            space_sdk=space_config[""space_sdk""]
            if ""space_sdk"" in space_config
            else ""gradio"",
        )

        # step 2-2
        tmp_dir = tempfile.gettempdir()
        io_utils.copy_dir(app_path, tmp_dir)

        # step 2-3
        _replace_placeholders(
            target_dir=tmp_dir,
            placeholders=space_config[""placeholders""]
            if ""placeholders"" in space_config
            else None,
            model_repo_id=model_repo_id,
            model_repo_url=model_repo_url,
            model_version=model_version,
        )

        # step 2-4
        local_path = ""hf_space""
        repository = _clone_and_checkout(
            repo_url=repo_url,
            local_path=local_path,
            access_token=access_token,
        )

        # step 2-5
        _replace_files(tmp_dir, local_path)

        # step 2-6
        _push_to_remote_repo(
            repo=repository,
            commit_msg=f""upload {model_version} model"",
        )

        outputs[""space_url""] = repo_url

    return outputs
"
